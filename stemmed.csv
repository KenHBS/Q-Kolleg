"";"id";"number";"title";"authors";"projectcode";"date";"jel";"abstracts"
"1";1;"2005-001";"Nonparametric Risk Management with Generalized Hyperbolic Distributions";"Ying Chen, Wolfgang Härdle and  Seok-Oh Jeong";"B1";"2004-10-27";"C14,  C16,  G15";" In this paper we propose the GHADA risk management model that is based on the generalized hyperbolic (GH) distribution and on a nonparametric adaptive methodology. Compared to the normal distribution, the GH distribution possesses semi-heavy tails and represents the financial risk factors more appropriately. The nonparametric adaptive methodology has the desirable property of estimating homogeneous volatility in a short time interval. For DEM/USD exchange rate data and a German bank portfolio data the proposed GHADA model provides more accurate value at risk calculation than the traditional model based on the normal distribution. All calculations and simulations are done with XploRe."
"2";2;"2005-002";"Selecting Comparables for the Valuation of European Firms";"Ingolf Dittmann and  Christian Weiner";"A1";"2005-02-10";"G19,  M41";" This paper investigates which comparables selection method generates the most precise forecasts when valuing European companies with the enterprise value to EBIT multiple. We also consider the USA as a reference point. It turns out that selecting comparable companies with similar return on assets clearly outperforms selections according to industry membership or total assets. Moreover, we investigate whether comparables should be selected from the same country, from the same region, or from all OECD members. For most European countries, choosing comparables from the 15 European Union member states yields the best forecasts. In contrast, for the UK and the US, comparables should be chosen from the same country only."
"3";3;"2005-003";"Competitive Risk Sharing Contracts with One-Sided Commitment";"Harald Uhlig and  Dirk Krueger";"C1";"2005-02-10";"G22,  E21,  D11,  D91";" This paper analyzes dynamic equilibrium risk sharing contracts between profit-maximizing intermediaries and a large pool of ex-ante identical agents that face idiosyncratic income uncertainty that makes them heterogeneous ex-post. In any given period, after having observed her income, the agent can walk away from the contract, while the intermediary cannot, i.e. there is one-sided commitment. We consider the extreme scenario that the agents face no costs to walking away, and can sign up with any competing intermediary without any reputational losses. We demonstrate that not only autarky, but also partial and full insurance can obtain, depending on the relative patience of agents and financial intermediaries. Insurance can be provided because in an equilibrium contract an up-front payment effectively locks in the agent with an intermediary. We then show that our contract economy is equivalent to a consumption-savings economy with one-period Arrow securities and a short-sale constraint, similar to Bulow and Rogoff (1989). From this equivalence and our characterization of dynamic contracts it immediately follows that without cost of switching financial intermediaries debt contracts are not sustainable, even though a risk allocation superior to autarky can be achieved."
"4";4;"2005-004";"Value-at-Risk Calculations with Time Varying Copulae";"Enzo Giacomini and  Wolfgang Härdle";"B1";"2005-02-10";"C14";" Value-at-Risk (VaR) of a portfolio is determined by the multivariate distribution of the risk factors increments. This distribution can be modelled through copulae, where the copulae parameters are not necessarily constant over time. For an exchange rate portfolio, copulae with time varying parameters are estimated and the VaR simulated accordingly. Backtesting underlines the improved performance of time varying copulae."
"5";5;"2005-005";"An optimal stopping problem in a diffusion-type model with delay";"Markus Reiß and  Pavel V. Gapeev";"C4";"2005-02-10";"C61";" We present an explicit solution to an optimal stopping problem in a model described by a stochastic delay differential equation with an exponential delay measure. The method of proof is based on reducing the initial problem to a free-boundary problem and solving the latter by means of the smooth-fit condition. The problem can be interpreted as pricing special perpetual average American put options in a diffusion-type model with delay."
"6";6;"2005-006";"Conditional and dynamic convex risk measures";"Kai Detlefsen and  Giacomo Scandolo";"B1";"2005-02-10";"D81";" We extend the definition of a convex risk measure to a conditional framework where additional information is available. We characterize these risk measures through the associated acceptance sets and prove a representation result in terms of conditional expectations. As an example we consider the class of conditional entropic risk measures. A new regularity property of conditional risk measures is defined and discussed. Finally we introduce the concept of a dynamic convex risk measure as a family of successive conditional convex risk measures and characterize those satisfying some natural time consistency properties."
"7";7;"2005-007";"Implied Trinomial Trees";"Pavel Cizek and  Karel Komorad";"B1";"2005-03-01";"C51,  G12";" Implied trinomial trees (ITTs) present an analogous extension of trinomial trees proposed by Derman, Kani, and Chriss (1996). Like their binomial counterparts, they can fit the market volatility smile and actually converge to the same continuous limit as binomial trees. In addition, they allow for a free choice of the underlying prices at each node of a tree, the so-called state space. This feature of ITTs allows to improve the fit of the volatility smile under some circumstances such as inconsistent, arbitrage-violating, or other market prices leading to implausible or degenerated probability distributions in binomial trees. We introduce ITTs in several steps. We first review main concepts regarding option pricing (Section 1) and implied models (Section 2). Later, we discuss the construction of ITTs (Section 3) and provide some illustrative examples (Section 4)."
"8";8;"2005-008";"Stable Distributions";"Szymon Borak, Wolfgang Härdle and  Rafal Weron";"B1";"2005-03-01";"C16";""
"9";9;"2005-009";"Predicting Bankruptcy with Support Vector Machines";"Wolfgang Härdle, Rouslan A. Moro and  Dorothea Schäfer";"B1";"2005-03-01";"C40,  G10";" The purpose of this work is to introduce one of the most promising among recently developed statistical techniques Â– the support vector machine (SVM) Â– to corporate bankruptcy analysis. An SVM is implemented for analysing such predictors as financial ratios. A method of adapting it to default probability estimation is proposed. A survey of practically applied methods is given. This work shows that support vector machines are capable of extracting useful information from financial data, although extensive data sets are required in order to fully utilize their classification power."
"10";10;"2005-010";"Working with the XQC";"Wolfgang Härdle and  Heiko Lehmann";"B1";"2005-03-01";"C87,  C88";""
"11";11;"2005-011";"FFT Based Option Pricing";"Szymon Borak, Kai Detlefsen and  Wolfgang Härdle";"B1";"2005-03-01";"G12";""
"12";12;"2005-012";"Common Functional Implied Volatility Analysis";"Michal Benko and  Wolfgang Härdle";"B1";"2005-03-01";"C13,  G19";" Trading, hedging and risk analysis of complex option portfolios depend on accurate pricing models. The modelling of implied volatilities (IV) plays an important role, since volatility is the crucial parameter in the Black-Scholes (BS) pricing formula. It is well known from empirical studies that the volatilities implied by observed market prices exhibit patterns known as volatility smiles or smirks that contradict the assumption of constant volatility in the BS pricing model. On the other hand, the IV is a function of two parameters: the strike price and the time to maturity and it is desirable in practice to reduce the dimension of this object and characterize the IV surface through a small number of factors. Clearly, a dimension reduced pricing-model that should reflect the dynamics of the IV surface needs to contain factors and factor loadings that characterize the IV surface itself and their movements across time."
"13";13;"2005-013";"Nonparametric Productivity Analysis";"Wolfgang Härdle and  Seok-Oh Jeong";"B1";"2005-03-01";"C14,  D20";" How can we measure and compare the relative performance of production units? If input and output variables are one dimensional, then the simplest way is to compute efficiency by calculating and comparing the ratio of output and input for each production unit. This idea is inappropriate though, when multiple inputs or multiple outputs are observed. Consider a bank, for example, with three branches A, B, and C. The branches take the number of staff as the input, and measures outputs such as the number of transactions on personal and business accounts. Assume that the following statistics are observed:"
"14";14;"2005-014";"Are Eastern European Countries Catching Up? Time Series Evidence for Czech Republic, Hungary, and Poland";"Ralf Brüggemann and  Carsten Trenkler";"C2";"2005-03-04";"C32,  E24";" The catching up process in Czech Republic, Hungary, and Poland is analyzed by investigating the integration properties of log-differences in per-capita GDP versus the EU15 and a Mediterranean country group. We account for structural changes by using unit root tests that allow for two endogenous breaks in the level and the trend. We find that Czech Republic and Hungary are stochastically converging towards the Mediterranean group, while only Czech Republic is stochastically converging towards EU15. Remaining per capita GDP differences are only reduced by deterministic trends. Extrapolating these trends we find that catching up will take about 20 years. Keywords: Stochastic convergence, Catching up, Unit root tests, EU accession JEL classification: C32, E24"
"15";15;"2005-015";"Robust estimation of dimension reduction space";"Wolfgang Härdle and  Pavel Cizek";"B1";"2005-03-24";"C14,  C40";" Most dimension reduction methods based on nonparametric smoothing are highly sensitive to outliers and to data coming from heavy-tailed distributions. We show that the recently proposed methods by Xia et al. (2002) can be made robust in such a way that preserves all advantages of the original approach. Their extension based on the local one-step M-estimators is suÂ±ciently robust to outliers and data from heavy tailed distributions, it is relatively easy to implement, and surprisingly, it performs as well as the original methods when applied to normally distributed data. Key words: Dimension reduction, Nonparametric regression, M-estimation"
"16";16;"2005-016";"Common functional component modelling";"Michal Benko and  Alois Kneip";"B1";"2005-03-24";"C13,  G19";" Functional data analysis (FDA) has become a popular technique in applied statistics. In particular, this methodology has received considerable attention in recent studies in empirical finance. In this talk we discuss selected topics of functional principal components analysis that are motivated by financial data."
"17";17;"2005-017";"A two state model for noise-induced resonance in bistable systems with delay";"Markus Fischer and  Peter Imkeller";"B6C4";"2005-03-24";"C13,  C51,  C87,  M31";" The subject of the present paper is a simplified model for a symmetric bistable system with memory or delay, the reference model, which in the presence of noise exhibits a phenomenon similar to what is known as stochastic resonance. The reference model is given by a one dimensional parametrized stochastic differential equation with point delay, basic properties whereof we check. With a view to capturing the effective dynamics and, in particular, the resonance-like behaviour of the reference model we construct a simplified or reduced model, the two state model, first in discrete time, then in the limit of discrete time tending to continuous time. The main advantage of the reduced model is that it enables us to explicitly calculate the distribution of residence times which in turn can be used to characterize the phenomenon of noise-induced resonance. Drawing on what has been proposed in the physics literature, we outline a heuristic method for establishing the link between the two state model and the reference model. The resonance characteristics developed for the reduced model can thus be applied to the original model."
"18";18;"2005-018";"Yxilon - a modular open-source statistical programming language";"Sigbert Klinke, Uwe Ziegenhagen and  Yuval Guri";"B1";"2005-04-01";"C80";" Statistical research has always been at the edge of available computing power. Huge datasets, e.g in DataMining or Quantitative Finance, and computationally intensive techniques, e.g. bootstrap methods, always require a little bit more computing power than is currently available. But the most popular statistical programming language R, as well as statistical programming languages like S or XploRe, are interpreted which makes them slow in computing intensive areas. The common solution is to implement these routines in low-level programming languages like C/C++ or Fortran and subsequently integrate them as dynamic linked libraries (DLL) or shared object libraries (SO) in the statistical programming language."
"19";19;"2005-019";"Arbitrage-free smoothing of the implied volatility surface";"Matthias R. Fengler";"B1";"2005-03-23";"C14,  C81,  G12";" The pricing accuracy and pricing performance of local volatility models crucially depends on absence of arbitrage in the implied volatility surface: an input implied volatility surface that is not arbitrage-free invariably results in negative transition probabilities and/ or negative local volatilities, and ultimately, into mispricings. The common smoothing algorithms of the implied volatility surface cannot guarantee the absence arbitrage. Here, we propose an approach for smoothing the implied volatility smile in an arbitrage-free way. Our methodology is simple to implement, computationally cheap and builds on the well-founded theory of natural smoothing splines under suitable shape constraints. Unlike other methods, our approach also works when input data are scarce and not arbitrage-free. Thus, it can easily be integrated into standard local volatility pricers."
"20";20;"2005-020";"A Dynamic Semiparametric Factor Model for Implied Volatility String Dynamics";"Wolfgang K. Härdle, Matthias R. Fengler and  Enno Mammen";"B1";"2005-03-06";"C14,  G12";" A primary goal in modelling the implied volatility surface (IVS) for pricing and hedging aims at reducing complexity. For this purpose one fits the IVS each day and applies a principal component analysis using a functional norm. This approach, however, neglects the degenerated string structure of the implied volatility data and may result in a modelling bias. We propose a dynamic semiparametric factor model (DSFM), which approximates the IVS in a finite dimensional function space. The key feature is that we only fit in the local neighborhood of the design points. Our approach is a combination of methods from functional principal component analysis and backfitting techniques for additive models. The model is found to have an approximate 10% better performance than a sticky moneyness model. Finally, based on the DSFM, we devise a generalized vega-hedging strategy for exotic options that are priced in the local volatility framework. The generalized vega-hedging extends the usual approaches employed in the local volatility framework."
"21";21;"2005-021";"Dynamics of State Price Densities";"Wolfgang Härdle and  Zdenek Hlávka";"B1";"2005-04-01";"C13,  C14,  G12";" State price densities (SPD) are an important element in applied quantitative finance. In a Black-Scholes model they are lognormal distributions with constant volatility parameter. In practice volatility changes and the distribution deviates from log-normality. We estimate SPDs using EUREX option data on the DAX index via a nonparametric estimator of the second derivative of the (European) call price function. The estimator is constrained so as to satisfy no-arbitrage constraints and it corrects for intraday covariance structure. Given a low dimensional representation of this SPD we study its dynamic for the years 1995Â–2003. We calculate a prediction corridor for the DAX for a 45 day forecast. The proposed algorithm is simple, it allows calculation of future volatility and can be applied to hedging exotic options."
"22";22;"2005-022";"DSFM fitting of Implied Volatility Surfaces";"Szymon Borak, Wolfgang Härdle and  Matthias R. Fengler";"B1";"2005-04-01";"C14";" The implied volatility became one of the key issues in modern quantitative finance, since the plain vanilla option prices contain vital information for pricing and hedging of exotic and illiquid options. European plain vanilla options are nowadays widely traded, which results in a great amount of high-dimensional data especially on an intra day level. The data reveal a degenerated string structure. Dynamic Semiparametric Factor Models (DSFM) are tailored to handle complex, degenerated data and yield low dimensional representation of the implied volatility surface (IVS). We discuss estimation issues of the model and apply it to DAX option prices."
"23";23;"2005-023";"Towards a Monthly Business Cycle Chronology for the Euro Area";"Emanuel Mönch and  Harald Uhlig";"C1";"2005-04-08";"B41,  C22,  C82,  E32,  E58";" This paper is an exercise in dating the Euro area business cycle on a monthly basis. Using a quite flexible interpolation routine, we construct several monthly series of Euro area real GDP, and then apply the Bry-Boschan (1971) procedure. To account for the asymmetry in growth regimes and duration across business cycle phases, we propose to extend this method with a combined amplitude/phase-length criterion ruling out expansionary phases that are short and flat. Applying the extended procedure to US and European data, we are able to replicate approximately the dating decisions of the NBER and the CEPR."
"24";24;"2005-024";"Modeling the FIBOR/EURIBOR Swap Term Structure: An Empirical Approach";"Oliver Blaskowitz, Helmut Herwartz and  Gonzalo de Cadenas Santiago";"B1C2";"2005-04-20";"C32,  C53,  E43,  G29";" In this study we forecast the term structure of FIBOR/EURIBOR swap rates by means of recursive vector autoregressive (VAR) models. In advance, a principal components analysis (PCA) is adopted to reduce the dimensionality of the term structure. To evaluate exÂ–ante forecasting performance for particular short, medium and long term rates and for the level, slope and curvature of the swap term structure, we rely on measures of both statistical and economic performance. Whereas the statistical performance is investigated by means of the HenrikksonÂ–Merton statistic, the economic performance is assessed in terms of cash flows implied by alternative trading strategies. Arguing in favor of local homogeneity of term structure dynamics, we propose a data driven, adaptive model selection strategy to Â’predict the best forecasting modelÂ’ out of a set of 100 alternative implementations of the PCA/VAR model. This approach is shown to outperform forecasting schemes relying on global homogeneity of the term structure."
"25";25;"2005-025";"Duality theory for optimal investments under model uncertainty";"Alexander Schied and  Ching-Tang Wu";"A3";"2005-04-27";"G11,  D81";" Robust utility functionals arise as numerical representations of investor preferences, when the investor is uncertain about the underlying probabilistic model and averse against both risk and model uncertainty. In this paper, we study the duality theory for the problem of maximizing the robust utility of the terminal wealth in a general incomplete market model. We also allow for very general sets of prior models. In particular, we do not assume that all prior models are equivalent to each other, which allows us to handle many economically meaningful robust utility functionals such as those defined by AVaR (lambda), concave distortions, or convex capacities. We also show that dropping the equivalence of prior models may lead to new effects such as the existence of arbitrage strategies under the least favorable model."
"26";26;"2005-026";"Projection Pursuit For Exploratory Supervised Classification";"Sigbert Klinke, Dianne Cook, Eun-Kyung Lee and  Thomas Lumley";"B1";"2005-05-10";"C19,  C49";" In high-dimensional data, one often seeks a few interesting low-dimensional projections that reveal important features of the data. Projection pursuit is a procedure for searching high-dimensional data for interesting low-dimensional projections via the optimization of a criterion function called the projection pursuit index. Very few projection pursuit indices incorporate class or group information in the calculation. Hence, they cannot be adequately applied in supervised classification problems to provide low-dimensional projections revealing class differences in the data . We introduce new indices derived from linear discriminant analysis that can be used for exploratory supervised classification."
"27";27;"2005-027";"Money Demand and Macroeconomic Stability Revisited";"Andreas Schabert and  Christian Stoltenberg";"C1";"2005-05-10";"E32,  E41,  E52";" This paper examines how money demand induced real balance effects contribute to the determination of the price level, as suggested by Patinkin (1949,1965), and if they affect conditions for local equilibrium uniqueness and stability. There exists a unique price level sequence that is consistent with an equilibrium under interest rate policy, only if beginning-of-period money enters the utility function. Real money can then serve as a state variable, implying that interest rate setting must be passive for unique, stable, and non-oscillatory equilibrium sequences. When end-ofperiod money provides utility, an equilibrium is consistent with infinitely many price level sequences, and equilibrium uniqueness requires an active interest rate setting. The stability results are, in general, independent of the magnitude of real balance effects, and apply also when prices are sticky. In contrast, under a constant money growth policy, equilibrium sequences are (likely to be) locally stable and unique for all model variants."
"28";28;"2005-028";"A Market Basket Analysis Conducted with a Multivariate Logit Model";"Yasemin Boztug and  Lutz Hildebrandt";"B2";"2005-05-01";"C31,  C33,  M31";" The following research is guided by the hypothesis that products chosen on a shopping trip in a supermarket can indicate the preference interdependencies between different products or brands. The bundle chosen on the trip can be regarded as the result of a global utility function. More specifically: the existence of such a function implies a cross-category dependence of brand choice behavior. It is hypothesized that the global utility function related to a product bundle results from the marketing-mix of the underlying brands. Several approaches exist to describe the choice of specific categories from a set of many alternatives. The models are discussed in brief; the multivariate logit approach is used to estimate a model with a German data set."
"29";29;"2005-029";"Utility Duality under Additional Information: Conditional Measures versus Filtration Enlargements";"Stefan Ankirchner";"B6";"2005-05-27";"C61,  D82";" The utility maximisation problem is considered for investors with anticipative additional information. We distinguish between models with conditional measures and models with enlarged filtrations. The dual functions of the maximal expected utility are determined with the help of f-divergences. We assume that our measures are absolutely continuous with respect to a local martingale measure (LMM), but not necessarily equivalent. Thus we do not exclude arbitrage."
"30";30;"2005-030";"The Shannon Information of Filtrations and the Additional Logarithmic Utility of Insiders";"Stefan Ankirchner, Peter Imkeller and  Steffen Dereich";"B6";"2005-05-27";"C61,  D82";" The background for the general mathematical link between utility and information theory investigated in this paper is a simple financial market model with two kinds of small traders: less informed traders and insiders, whose extra information is represented by an enlargement of the other agentsÂ’ filtration. The expected logarithmic utility increment, i.e. the difference of the insiderÂ’s and the less informed traderÂ’s expected logarithmic utility is described in terms of the information drift, i.e. the drift one has to eliminate in order to perceive the price dynamics as a martingale from the insiderÂ’s perspective. On the one hand, we describe the information drift in a very general setting by natural quantities expressing the probabilistic better informed view of the world. This on the other hand allows us to identify the additional utility by entropy related quantities known from information theory. In particular, in a complete market in which the insider has some fixed additional information during the entire trading interval, its utility increment can be represented by the Shannon information of his extra knowledge. For general markets, and in some particular examples, we provide estimates of maximal utility by information inequalities."
"31";31;"2005-031";"Does Temporary Agency Work Provide a Stepping Stone to Regular Employment?";"Michael Kvasnicka";"C7";"2005-05-31";"C14,  C41,  J41,  J64";" Based on administrative data from the federal employment office in Germany, we apply matching techniques to estimate the stepping-stone function of temporary agency work for the unemployed, i.e. its short-run and long-run effects on their future employment prospects. Our results show that unemployed workers who take up a job in the temporary work agency (TWA) industry are on average more likely than unemployed workers not joining TWA work to be in agency employment in the four year period these workers are tracked after entering TWA work. However, we find no discernable effects on the probabilities of being either in regular employment or registered unemployment. Our findings therefore do not lend support to the stepping-stone function of temporary agency work."
"32";32;"2005-032";"Working Time as an Investment? – The Effects of Unpaid Overtime on Wages, Promotions and Layoffs";"Silke Anger";"C7";"2005-06-13";"J2,  J3,  J4";" Whereas the number of paid overtime hours declined over the last decade, a different trend can be observed for unpaid overtime work in Germany. We look at the future consequences for overtime workers, and therefore investigate the investment character of working time. We examine whether unpaid extra hours induce a higher likelihood of promotion and pay rise, and whether they reduce the risk of losing the job. Using longitudinal micro data from the GSOEP for the years 1991 to 2002 we find significant positive effects of unpaid overtime work on future payoffs, but also a positive impact on the probability of job loss. Therefore, we find only partial evidence for the investment character of unpaid overtime."
"33";33;"2005-033";"Notes on an Endogenous Growth Model with two Capital Stocks II: The Stochastic Case";"Dirk Bethmann";"C7";"2005-06-13";"C61,  C62";" This paper extends the class of stochastic AK growth models with a closed-form solution to the case where there are two capital goods in the model. To be precise, we consider the Uzawa-Lucas model of endogenous growth with human and physical capital. The extension holds, even if an external effect in the use of human capital in goods production occurs. Using the Â“guess and verifyÂ” method, we determine the value function of the social planner in the centralized economy and the value function of the representative agent in the decentralized case. We show that the introduction of income taxes on wages and of a subsidy on physical capital earnings is able to help the decentralized economy in reaching the social optimum, while keeping the policy makerÂ’s budget balanced. Then the time series implications of the modelÂ’s solution are derived. In Appendix to the paper the uniqueness of the value functions is proved by using an alternative method."
"34";34;"2005-034";"Skill Mismatch in Equilibrium Unemployment";"Ronald Bachmann";"C7";"2005-06-22";"J64,  J65,  D33";" We analyse the effect of skill mismatch in a search model of equilibrium unemployment with risk-neutral agents, endogenous job destruction, and two-sided ex-ante heterogeneity. First, we examine the interaction of labour market institu- tions and skill mismatch. We find that skill mismatch changes the results obtained in a model with ex ante homogeneity. Second, we analyse the interaction of skill mismatch and labour market institutions for the diÂ®erence in the labour market experience of continental Europe on the one hand and the US on the other hand. We find that within-group skill mismatch cannot explain the rise in unemployment in Europe relative to the US. This result is due to the endogeneity of job destruction and stands at odds with previous findings in the literature. We can, however, confirm the fact that unemployment benefits potentially play a beneficial role by providing a subsidy to search. Generally, we argue that in search models with fixed match characteristics, job destruction should be endogenised in order to take account of heterogeneous decision rules."
"35";35;"2005-035";"Uncovered Interest Rate Parity and the Expectations Hypothesis of the Term Structure:Empirical Results for the  U.S. and Europe";"Ralf Brüggemann and  Helmut Lütkepohl";"C2";"2005-06-22";"C32";" A system of U.S. and euro area short- and long-term interest rates is analyzed. According to the expectations hypothesis of the term structure the interest rate spreads should be stationary and according to the uncovered interest rate parity the difference between the U.S. and euro area longterm interest rates should also be stationary. If all four interest rates are integrated of order one, one would expect to find three linearly independent cointegration relations in the system of four interest rate series. Combining German and European Monetary Union data to obtain the euro area interest rate series we find indeed the theoretically expected three cointegration relations, in contrast to previous studies based on different data sets."
"36";36;"2005-036";"Getting Used to Risk: Reference Dependence and Risk Inclusion";"Astrid Matthey";"A6";"2005-07-06";"D81,  I18";" Experimental and field evidence show that people perceive and evaluate new risks differently from risks that are common. In particular, people get used to the presence of certain risks and become less eager to avoid them. We explain this observation by including risks in the reference states of individuals, which requires a more general concept of the reference state than has previously been considered in the literature. We find two effects. First, the inclusion of the risk in the reference state changes its evaluation. A risk being present on the market induces a selfenforcing process of increasing acceptance of this risk without any new information becoming available or peoples tastes changing. We term this the risk inclusion effect. Second, a risk with lower inherent (reference-independent)utility may be preferred to a risk with higher inherent utility if individuals are more used to accepting the former than the latter. This leads to inefficient decisions if habituation is seen as contributing less to welfare than inherent utility. Both effects have implications for the optimal regulation of risks."
"37";37;"2005-037";"New Evidence on the Puzzles.  Results from Agnostic Identification on Monetary Policy and Exchange Rates";"Harald Uhlig and   Almuth Scholl";"C1";"2005-07-11";"C32,  E58,  F31,  F42";" Past empirical research on monetary policy in open economies has found evidence of the ""delayed overshooting"", the ""forward discount"" and the ""exchange rate"" puzzles. We revisit the effects of monetary policy on exchange rates by applying Uhligs (2005) identification procedure that involves sign restrictions on the impulse responses of selected variables. We impose no restrictions on the exchange rate to leave the key question as open as possible.   The sign restriction methodology avoids the ""price puzzles"" of the identification strategies used by Eichenbaum-Evans (1995) and by Grilli-Roubini (1995, 1996), which are particularly pronounced, when using an updated data set.  We find that the puzzles regarding the exchange rates are still there, but that the quantitative features are different.  In response to US monetary policy shocks, the peak appreciation happens during the first year after the shock  for the US-German and the US-UK pair, and during the first two years  for the US-Japan pair.  This is consirably quicker than the three-year horizon found by Eichenbaum-Evans. There is a robust forward discount puzzle implying a large risk premium.   We study this issue, introducing and calculating conditional Sharpe ratios for a Bayesian investor  investing in a hedged position following a US monetary policy shock.  For foreign monetary policy shocks, we find more robust results  than with the Grilli-Roubini recursive identification strategy: the posterior distribution regarding the exchange reaction looks rather similar across countries and VAR specifications.  In particular, we find that there seems to be considerable uncertainty regarding the initial reaction of the exchange rate.  Quantitatively, monetary policy shocks seem to have a minor impact on exchange rate fluctuations."
"38";38;"2005-038";"Discretisation of Stochastic Control Problems for Continuous Time Dynamics with Delay";"Markus Fischer and  Markus Reiss";"C4";"2005-08-02";"C63";" As a main step in the numerical solution of control problems in continuous time, the controlled process is approximated by sequences of controlled Markov chains, thus discretizing time and space. A new feature in this context is to allow for delay in the dynamics. The existence of an optimal strategy with respect to the cost functional can be guaranteed in the class of relaxed controls. Weak convergence of the approximating extended Markov chains to the original process together with convergence of the associated optimal strategies is established."
"39";39;"2005-039";"What are the Effects of Fiscal Policy Shocks?";"Harald Uhlig and  Andrew Mountford";"C1";"2005-08-03";"C32,  E60,  E62,  H20,  H50,  ";" We propose and apply a new approach for analyzing the effects of fiscal policy using vector autoregressions. Unlike most of the previous literature this approach does not require that the contemporaneous reaction of some variables to fiscal policy shocks be set to zero or need additional information, such as the timing of wars, in order to identify fiscal policy shocks. The paperÂ´s method is a purely vector autoregressive approach which can be universally applied. The approach also has the advantages that it is able to model the effects of announcements of future changes in fiscal policy and that it is able to distinguish between the changes in fiscal variables caused by fiscal policy shocks and those caused by business cycle and monetary policy shocks. We apply the method to US quarterly data from 1955-2000 and obtain interesting results. Our key finding is that the best fiscal policy to stimulate the economy is a deficit-financed tax cut and that the long term costs of fiscal expansion through government spending are probably greater than the short term gains."
"40";40;"2005-040";"Optimal Sticky Prices under Rational Inattention";"Bartosz Mackowiak and  Mirko Wiederholt";"C1C3";"2005-08-04";"E3,  E5,  D8";" In the data, individual prices change frequently and by large amounts. In standard sticky price models, frequent and large price changes imply a fast response of the aggregate price level to nominal shocks. This paper presents a model in which price setting firms optimally decide what to observe, subject to a constraint on information flow. When idiosyncratic conditions are more variable or more important than aggregate conditions, firms pay more attention to idiosyncratic conditions than to aggregate conditions. When we calibrate the model to match the large average absolute size of price changes observed in the data, prices react fast and by large amounts to idiosyncratic shocks, but prices react only slowly and by small amounts to nominal shocks. Nominal shocks have persistent real effects. We use the model to investigate how the optimal allocation of attention and the dynamics of prices depend on the firmsÂ´ environment."
"41";41;"2005-041";"Fixed-Prize Tournaments versus First-Price Auctions in Innovation Contests";"Anja Schöttner";"A4";"2005-08-09";"D44,  H57,  L15";" This paper analyzes a procurement setting with two identical firms and stochastic innovations. In contrast to the previous literature, I show that a procurer who cannot charge entry fees may prefer a fixed-prize tournament to a first-price auction since holding an auction may leave higher rents to firms when the innovation technology is subject to large random factors."
"42";42;"2005-042";"Bank finance versus bond finance: what explains the differences between US and Europe?";"Harald Uhlig and  Fiorella De Fiore";"C1";"2005-08-01";"E20,  E44,  C68";" We present a dynamic general equilibrium model with agency costs, where heterogeneous firms choose among two alternative instruments of external finance - corporate bonds and bank loans. We characterize the financing choice of firms and the endogenous financial structure of the economy. The calibrated model is used to address questions such as: What explains differences in the financial structure of the US and the euro area? What are the implications of these differences for allocations? We find that a higher share of bank finance in the euro area relative to the US is due to lower availability of public information about firmsÂ’ credit worthiness and to higher efficiency of banks in acquiring this information. We also quantify the effect of differences in the financial structure on per-capita GDP."
"43";43;"2005-043";"On Local Times of Ranked Continuous Semimartingales; Application to Portfolio Generating Functions";"Raouf Ghomrasni";"A3";"2005-06-13";"G11";" We derive the decomposition of the ranked continuous semimartingales i.e. orderstatistics processes. We apply it to portfolios generated by functions of the ranked market weights. Thus we generalize recent results of Fernholz."
"44";44;"2005-044";"A Software Framework for Data Based Analysis";"Markus Krätzig";"C2";"2005-08-29";"C63";" This paper presents the software framework JStatCom which is geared towards the development of rich GUI clients for numerical procedures. The concept is to solve all recurring tasks with the help of reusable Java components. Optionally, one can delegate the execution of special numerical algorithms to external programs, for example Gauss or Matlab. This way it is possible to reuse an already existing code base for numerical routines written in different programming languages and to link them with the Java world. A reference application for JStatCom is the econometric software package JMulTi, which will shortly be introduced."
"45";45;"2005-045";"Labour Market Dynamics in Germany: Hirings, Separations, and Job-to-Job Transitions over the Business Cycle";"Ronald Bachmann";"C7";"2005-09-02";"J63,  J64,  J21,  E24";" In this paper, we provide a comprehensive overview of labour market dynamics in Western Germany by looking at gross worker flows. To do so, we use a subsample of the registry data collected by the German social security system, the IAB employment sample, for the time period 1975-2001. The latter provides daily information on 2% of the German workforce covered by social security legislation. Using these data, we are able to exactly calculate the number of transitions between the different labour market states, and between different employers over time. We first provide an overview of the cross-section and time series properties of these flows. We then study the cyclical features of gross worker flows, accessions, and separations. We find that separations are relatively flat over the cycle, while accessions are markedly procyclical, and that the increased flow into unemployment in a recession is mainly due to reduced hirings, and hence lower job-to-job transitions, rather than increased match separations. Our findings have important implications both for the way we view recessions and for the role of the labour market as a propagation mechanism for productivity shocks."
"46";46;"2005-046";"Paternal Uncertainty and the Economics of Mating, Marriage, and Parental Investment in Children";"Dirk Bethmann and  Michael Kvasnicka";"C7";"2005-09-05";"D10,  J12,  J13,  D02";" We develop a theoretical model of mating behavior and parental investment in children under asymmetry in kin recognition between men and women that provides a microfoundation for the institution of marriage. In the model, men and women derive utility from consumption and reproductive success, which is a function of the number and quality of own offspring. Because of paternal uncertainty, men unlike women may err in investing resources in offspring that is not biologically theirs. As a socially sanctioned commitment device among partners, the institution of marriage reduces this risk by restraining promiscuity in society. Both women and men are shown to benefit from lower levels of paternal uncertainty, as does average child quality because of increased parental investments. As an analytical framework, the model is suitable to study a number of societal, economic, and technological changes in their effects on marriage patterns. A combination of factors is argued to underlie the demise of marriage."
"47";47;"2005-047";"Estimation and Testing for Varying Coefficients in Additive Models with Marginal Integration";"Wolfgang Härdle, Byeong Park, Lan Xue and  Lijian Yang";"B1";"2005-09-06";"C13,  C14,  C40";" We propose marginal integration estimation and testing methods for the coefficients of varying coefficient multivariate regression model. Asymptotic distribution theory is developed for the estimation method which enjoys the same rate of convergence as univariate function estimation. For the test statistic, asymptotic normal theory is established. These theoretical results are derived under the fairly general conditions of absolute regularity (beta-mixing). Application of the test procedure to the West German real GNP data reveals that a partially linear varying coefficient model is best parsimonious in fitting the data dynamics, a fact that is also confirmed with residual diagnostics."
"48";48;"2005-048";"Zeitarbeit in Deutschland: Trends und Perspektiven";"Michael C. Burda and  Michael Kvasnicka";"C7";"2005-09-14";"J30,  J40,  J60";""
"49";49;"2005-049";"Courtesy and Idleness: Gender Differences in Team Work and Team Competition";"Dorothea Kübler and   Radosveta Ivanova-Stenzel";"A6";"2005-09-21";"C72,  C73,  C91,  D82";" Does gender play a role in the context of team work? Our results based on a real-effort experiment suggest that performance depends on the composition of the team. We find that female and male performance differ most in mixed teams with revenue sharing between the team members, as men put in significantly more effort than women. The data also indicate that women perform best when competing in pure female teams against male teams whereas men perform best when women are present or in a competitive environment."
"50";50;"2005-050";"Do Factor Shares Reflect Technology?";"Dominique Demougin and  Benjamin Bental";"A4";"2005-09-26";"E23,  E25";" This note demonstrates that it is easily possible to compute technological parameters out of national income accounting data in the presence of bargaining in the labor market. Applying the method to US data, we obtain that the output elasticity with respect to capital exceed 0.5."
"51";51;"2005-051";"Optimal Investments for Risk- and Ambiguity-Averse Preferences: A Duality Approach";"Alexander Schied";"A3";"2006-09-11";"G11,  D81";" Ambiguity, also called Knightian or model uncertainty, is a key feature in financial  modeling. A recent paper by Maccheroni et al. (2004) characterizes investor preferences under  aversion against both risk and ambiguity. Their result shows that these preferences can be  numerically represented in terms of convex risk measures. In this paper we study the corresponding  problem of optimal investment over a given time horizon, using a duality approach and building  upon the results by Kramkov and Schachermayer (1999, 2001)."
"52";52;"2005-052";"Relational Contracts and Job Design";"Anja Schöttner";"A4";"2005-09-29";"M51,  M54";" This paper analyzes the problem of optimal job design when there is only one contractible and imperfect performance measure for all tasks whose contribution to firm value is non-veritable. I find that task splitting is optimal when relational contracts based on firm value are not feasible. By contrast, if an agent who performs a given set of tasks receives an implicit bonus, the principal always benefits from assigning an additional task to this agent."
"53";53;"2005-053";"Explicit characterization of the super-replication strategy in financial markets with partial transaction costs";"Imen Bentahar and  Bruno Bouchard";"A3";"2005-10-20";"G12,  G13,  C6";" We consider a continuous time multivariate financial market with proportional transaction costs and study the problem of finding the minimal initial capital needed to hedge, without risk, European-type contingent claims. The model is similar to the one considered in Bouchard and Touzi (2000) except that some of the assets can be exchanged freely, i.e. without paying transaction costs. This is the so-called non-effcient friction case. To our knowledge, this is the first time that such a model is considered in a continuous time setting. In this context, we generalize the result of the above paper and prove that the super-replication price is given by the cost of the cheapest hedging strategy in which the number of non-freely exchangeable assets is kept constant over time."
"54";54;"2005-054";"Aid Effectiveness and Limited Enforceable Conditionality";"Almuth Scholl";"C1";"2005-10-25";"E13,  F35,  O11,  O19";" This paper analyzes optimal foreign aid policy in a neoclassical framework with a conflict of interest between the donor and the recipient government. Aid conditionality is modelled as a limited enforceable contract. We define conditional aid policy to be self-enforcing if, at any point in time, the conditions imposed on aid funds are supportable by the threat of a permanent aid cutoff from then onward. Quantitative results show that the effectiveness of unconditional aid is low while self-enforcing conditional aid strongly stimulates the economy. However, increasing the welfare of the poor comes at high cost: to ensure aid effectiveness, less democratic political regimes receive permanently larger aid funds."
"55";55;"2005-055";"Limited Enforceable International Loans, International Risk Sharing and Trade";"Almuth Scholl";"C1";"2005-10-25";"E32,  D52,  F34,  F41";" This paper analyzes the impact of limited enforceable international loans on international risk sharing and trade fluctuations in a two-country two-good endowment economy. Our specification of the punishment threat allows the exclusion from trade to last only finitely many periods and distinguishes between financial autarky and full autarky. Quantitative results show that limited enforceability substantially alters cross-country consumption correlations and the dynamics of net exports. In contrast to existing studies, risk sharing is low for large elasticities of substitution between the domestic and foreign goods. However, it remains challenging to explain the high volatility of the terms of trade empirically observed."
"56";56;"2005-056";"Stock Markets and Business Cycle Comovement in Germany before World War I: Evidence from Spectral Analysis";"Albrecht Ritschl and  Martin Uebele";"C5";"2005-11-16";"E32,  E44,  N13";" This paper examines the comovement of the stock market and of real activity in Germany before World War I under the effcient market hypothesis. We employ multivariate spectral analysis to compare rivaling national product estimates to  stock market behavior in the frequency domain. Close comovement of one series with the stock market enables us to decide between various rivaling business cycle chronologies. We find that business cycle dates obtained from deflated national product series are severely distorted by interference with the implicit price deflator. Among the nominal series, the income estimate of Hoffmann (1965) correlates best with the stock market, while the tax based estimate of Hoffmann and MÃ¼ller (1959) is too smooth especially before 1890. We find impressive comovement between the stock market and nominal wages, a sub-series of HoffmannÂ´s income estimate. We can show that a substantial part of this nominal wage series is  driven by data on real investment activity. Our findings confirm the traditional business cycle chronology for Germany of Burns and Mitchell (1946) and Spiethoff (1955), and lead us to discard later, rivaling business cycle chronologies."
"57";57;"2005-057";"An empirical test of theories of price valuation using a semiparametric approach, reference prices, and accounting for heterogeneity";"Yasemin Boztug and  Lutz Hildebrandt";"B2";"2005-12-01";"C14,  C23,  C25,  D12,  M31";" In this paper we estimate and empirically test different behavioral theories of consumer reference price formation. Two major theories are proposed to model the reference price reaction: assimilation contrast theory and prospect theory. We assume that different consumer segments will use different reference prices. The study builds on earlier research by Kalyanaram and Little (1994); however, in contrast to their work, we use parametric and semiparametric approaches to detect the structure of the underlying data sets. The different models are tested using a program module in GAUSS that was able to account for heterogeneity. The model types were calibrated by a simulation study. The calibrated modules were then used to analyze real market data."
"58";58;"2005-058";"Integrable e-lements for Statistics Education";"Wolfgang Härdle, Sigbert Klinke and  Uwe Ziegenhagen";"B1";"2005-12-08";"I21,  C19";" Without doubt modern education in statistics must involve practical, computer-based data analysis but the question arises whether and how computational elements should be integrated into the canon of methodological education. Should the student see and study high-level programming code right at the beginning of his or her studies? Which technology can be presented during class and which computational elements can re-occur (at increasing level of complexity) during the different courses?"
"59";59;"2005-059";"What does the Bank of Japan do to East Asia?";"Bartosz Mackowiak";"C3";"2005-12-15";"F41,   E3,   E52";" In recent policy debates some have argued that expansionary monetary policy in Japan can increase real output in Japan and in JapanÂ´s neighbors, while others have warned that it is a beggar-thy-neighbor policy. In this paper we estimate structural vector autoregressions to assess the effects of Japanese monetary policy shocks. We find that the effects of Japanese monetary policy shocks on macroeconomic variation in East Asia have been modest and difficult to reconcile with the beggar-thy-neighbor view. We estimate that the Asian crisis was preceded by expansionary monetary policy shocks in Japan, but we fail to find support for the view that these shocks contributed to the crisis."
"60";60;"2005-060";"Portfolio Value at Risk Based on Independent Components Analysis";"Ying Chen, Wolfgang Härdle and  Vladimir Spokoiny";"B1B5";"2005-12-15";"C14,  C15,  C32,  C53,  G20";" Risk management technology applied to high dimensional portfolios needs simple and fast methods for calculation of Value-at-Risk (VaR). The multivariate normal framework provides a simple off-the-shelf methodology but lacks the heavy tailed distributional properties that are observed in data. A principle component based method (tied closely to the elliptical structure of the distribution) is therefore expected to be unsatisfactory. Here we propose and analyze a technology that is based on Independent Component Analysis (ICA). We study the proposed ICVaR methodology in an extensive simulation study and apply it to a high dimensional portfolio situation. Our analysis yields very accurate VaRs."
"61";61;"2005-061";"How much of the Macroeconomic Variation in Eastern Europe is Attributable to External Shocks?";"Bartosz Mackowiak";"C3";"2005-12-15";"F41,   E3,   O11,   P2";" We decompose by origin the sources of the variation in real aggregate output and aggregate price level in the Czech Republic, Hungary and Poland. We find that a sizable fraction of the variation is attributable to external shocks, especially so for aggregate price level. We show that euroarea interest rate shocks can account for a significant fraction of the external spillover effects. We conclude that theoretical models of advanced transition economies and policy rules for these economies should feature a prominent role for external shocks."
"62";62;"2005-062";"The Impact of Industry Classification Schemes on Financial Research";"Christian Weiner";"B4";"2005-12-20";"G12,  G14,  M40";" This paper investigates industry classification systems. During the last 50 years there has been a considerable discussion of problems regarding the classification of economic data by industries. From my perspective, the central point of each classification is to determine a balance between aggregation of similar firms and differentiation between industries. This paper examines the structure and content of industrial classification schemes and how they affect financial research. I use classification systems provided by the Worldscope and the Compustat database. First, this study gives a detailed description of the structure and methodology of industrial classification systems and the relevance in leading finance and accounting journals. Second, I construct a benchmark classification system to measure the performance of different systems and provide evidence that some systems a more homogeneous in terms of value drivers than others. Third, I examine how multiple valuation is influenced by industry classification and show that the results vary significantly for different systems."
"63";63;"2005-063";"The Conglomerate Discount in Germany and the Relationship to Corporate Governance";"Christian Weiner";"B4";"2005-12-14";"G31,  G32,  G34";" This paper documents the conglomerate discount for all available German firms and the DAX 30 firms in detail. It shows a moderate discount of about 0.06 based on German comparable firms and of about 0.20 for a combined sample of German and European peer groups. I further examine the relationship between the discount and industry concentration as well as uncertainty of valuation. Finally, I document that corporate governance behavior affects the conglomerate discount."
"64";64;"2005-064";"Worldscope meets Compustat: A Comparison of Financial Databases";"Niels Ulbricht and  Christian Weiner";"B4";"2005-12-14";"G14,  G29,  M41";" With this study we are the first to systematically compare todayÂ’s two major counterparts as a source of accounting and financial data for researchers: Compustat North America by Standard and PoorÂ’s and Worldscope by Thomson Financial. This investigation is conducted for U.S. and partly Canadian data over an extensive period from 1985 to 2003. We examine more than 650 data items available in both databases and address the question of whether or not the decision for one or the other source may have an impact on the outcome of research projects. It is probably commonly assumed that this impact is minor, but it also leaves room to question certain results. We show that the use of both databases should lead to comparable results, but also find that if, e.g. a size bias, is not treated with care the quality of results may differ considerable. Furthermore after 1998 the number of firms covered by Worldscope exceeds the one covered by Compustat by about one fourth."
"65";65;"2006-001";"Calibration Risk for Exotic Options";"Kai Detlefsen and  Wolfgang Härdle";"B1";"2006-01-06";"C13,  G12";" Option pricing models are calibrated to market data of plain vanillas by minimization of an error functional. From the economic viewpoint, there are several possibilities to measure the error between the market and the model. These different specifications of the error give rise to different sets of calibrated model parameters and the resulting prices of exotic options vary significantly. These price differences often exceed the usual profit margin of exotic options."
"66";66;"2006-002";"Calibration Design of Implied Volatility Surfaces";"Kai Detlefsen and  Wolfgang Härdle";"B1";"2006-01-09";"C80,  G13";" The calibration of option pricing models leads to the minimization of an error functional. We show that its usual specification as a root mean squared error implies fluctuating exotics prices and possibly wrong prices. We propose a simple and natural method to overcome these problems, illustrate drawbacks of the usual approach and show advantages of our method. To this end, we calibrate the Heston model to a time series of DAX implied volatility surfaces and then price cliquet options."
"67";67;"2006-003";"On the Appropriateness of Inappropriate VaR Models";"Wolfgang Härdle, Zdenek Hlávka and  Gerhard Stahl";"B1";"2006-01-10";"C51,  C52,  G20";" The Value-at-Risk calculation reduces the dimensionality of the risk factor space. The main reasons for such simplifications are, e.g., technical efficiency, the logic and statistical appropriateness of the model. In Chapter 2 we present three simple mappings: the mapping on the market index, the principal components model and the model with equally correlated risk factors. The comparison of these models in Chapter 3 is based on the literatere on the verification of weather forecasts (Murphy and Winkler 1992, Murphy 1997). Some considerations on the quantitative analysis are presented in the fourth chapter. In the last chapter, we present empirical analysis of the DAX data using XploRe."
"68";68;"2006-004";"Regional Labor Markets, Network Externalities and Migration: The Case of German Reunification";"Harald Uhlig";"C1";"2006-02-06";"E20,  E24,  J6,  J1";" Fifteen years after German reunification, the facts about slow regional convergence have born out the prediction of Barro (1991), except that migration out of East Germany has not slowed down. I document that in particular the 18-29 year old are leaving East Germany, and that the emigration has accelerated in recent years. To understand these patterns, I provide an extension of the standard labor search model by allowing for migration and network externalities. In that theory, two equilibria can result: one with a high networking rate, high average labor productivity, low unemployment and no emigration (Â“West GermanyÂ”) and one with a low networking rate, low average labor productivity, high unemployment and a constant rate of emigration (Â“East GermanyÂ”). The model does not imply any obviously sound policies to move from the weakly networked equilibrium to the highly networked equilibrium."
"69";69;"2006-005";"British Interest Rate Convergence between the US and Europe: A Recursive Cointegration Analysis";"Enzo Weber";"C6";"2006-01-18";"E43,  E44,  C32";" This paper addresses the question of the British state of convergence towards the Euro area, compared to the USA. Economically, the analysis is based on dependences in the money and capital markets, namely the uncovered interest parity (UIP) and the expectation hypothesis of the term structure (EHT). The econometric procedure consists of backward recursive calculations carried out in a cointegration framework. As the evidence for the single parities remains unconvincing, UIP and EHT are combined in a common model. Generally, the results are in favour of a growing British integration into the European Currency Union."
"70";70;"2006-006";"A Combined Approach for Segment-Specific Analysis of Market Basket Data";"Yasemin Boztug and   Thomas Reutterer";"B2";"2006-01-24";"C31,  C33,  C35,  C63,  M31";" There are two main research traditions for analyzing market basket data that exist more or less independently from each other, namely exploratory and explanatory model types. Exploratory approaches are restricted to the task of discovering cross-category interrelationships and provide marketing managers with only very limited recommendations regarding decision making. The latter type of models mainly focus on estimating the effects of category-level marketing mix variables on purchase incidences assuming cross-category dependencies. We propose a procedure that combines these two modeling approaches in a novel two-stage procedure for analyzing cross-category effects based on shopping basket data: In a data compression step we first derive a set of market basket prototypes and generate segments of households with internally more distinctive (complementary) cross-category interdependencies. Utilizing the information on categories that are most responsible for prototype construction, segment-specific multivariate logistic models are estimated in a second step. Based on the data-driven way of basket construction, we can show significant differences in cross-effects and related price elasticities both across segments and compared to the global (segment-unspecific) model."
"71";71;"2006-007";"Robust Utility Maximization in a Stochastic Factor Model";"Daniel Hernández–Hernández and   Alexander Schied";"A3";"2006-09-11";"G11,  D81";" We give an explicit PDE characterization for the solution of a robust utility  maximization problem in an incomplete market model, whose volatility, interest rate  process, and long-term trend are driven by an external stochastic factor process. The  robust utility functional is defined in terms of a HARA utility function with negative  risk aversion and a dynamically consistent coherent risk measure, which allows for model  uncertainty in the distributions of both the asset price dynamics and the factor process.  Our method combines two recent advances in the theory of optimal investments: the  general duality theory for robust utility maximization and the stochastic control approach  to the dual problem of determining optimal martingale measures."
"72";72;"2006-008";"Economic Growth of Agglomerations and Geographic Concentration of Industries – Evidence for Germany";"Kurt Geppert,  Martin Gornig and   Axel Werwatz";"B3";"2006-01-26";"C14,  C16,  R12,  R30";" The vast majority of regions in West Germany, and the EU, have become more similar in terms of per-capita income and productivity between 1980 and 2000. But a number of rich areas - generally large agglomerations - have succeeded in departing from this trend of convergence. They are continuing to rise above the average productivity level. We examine whether this development can also be seen as due to changes in the spatial distribution of economic sectors. Knowledge-intensive services in particular are identified as industries that combine employment growth and further geographical concentration. Logistical and nonparametric regressions confirm a positive relation between the regional weight of sectors that are continuing to concentrate geographically and the probability that this region will develop ahead of the general trend. We find that increasing localisation of fast growing industries is an important factor behind the changes in the spatial pattern of the economy."
"73";73;"2006-009";"Institutions, Bargaining Power and Labor Shares";"Benjamin Bental and   Dominique Demougin";"A4";"2006-01-26";"D02,  D24";" We use a static framework characterized by both moral hazard and holdup problems. In the model the optimal allocation of bargaining power balances these frictions. We examine the impact of improved monitoring on that optimal allocation and its impact upon effort, investment, profits and rents. The modelÂ’s predictions are consistent with the recent evolution of labor shares, wages per efficiency units and the ratio of labor in efficiency units to capital in several OECD countries. The model suggests further that improvement in monitoring may also play a key role in understanding opposition to institutional reforms in the labor market."
"74";74;"2006-010";"Common Functional Principal Components";"Michal Benko,  Wolfgang Härdle and   Alois Kneip";"B1";"2006-01-30";"C14,  G19";" Functional principal component analysis (FPCA) based on the Karhunen-LoÃ¨ve decomposition has been successfully applied in many applications, mainly for one sample problems. In this paper we consider common functional principal components for two sample problems. Our research is motivated not only by the theoretical challenge of this data situation but also by the actual question of dynamics of implied volatility (IV) functions. For different maturities the logreturns of IVs are samples of (smooth) random functions and the methods proposed here study the similarities of their stochastic behavior. Firstly we present a new method for estimation of functional principal components from discrete noisy data. Next we present the two sample inference for FPCA and develop two sample theory. We propose bootstrap tests for testing the equality of eigenvalues, eigenfunctions, and mean functions of two functional samples, illustrate the test-properties by simulation study and apply the method to the IV analysis."
"75";75;"2006-011";"VAR Modeling for Dynamic Semiparametric Factors of Volatility Strings";"Ralf Brüggemann, Wolfgang Härdle, Julius Mungo and  Carsten Trenkler";"B1C2";"2006-02-03";"C14,  C32";" The implied volatility of a European option as a function of strike price and time to maturity forms a volatility surface. Traders price according to the dynamics of this high dimensional surface. Recent developments that employ semiparametric models approximate the implied volatility surface (IVS) in a finite dimensional function space, allowing for a low dimensional factor representation of these dynamics. This paper presents an investigation into the stochastic properties of the factor loading times series using the vector autoregressive (VAR) framework and analyzes associated movements of these factors with movements in some macroeconomic variables of the Euro-economy."
"76";76;"2006-012";"Bootstrapping Systems Cointegration Tests with a Prior Adjustment for Deterministic Terms";"Carsten Trenkler";"C2";"2006-02-10";"C12,  C13,  C15,  C32";" In this paper we analyse bootstrap procedures for systems cointegration tests with a prior adjustment for deterministic terms suggested by Saikkonen and LÃ¼tkepohl (2000b), and Saikkonen, LÃ¼tkepohl and Trenkler (2006). The asymptotic properties of the bootstrap test procedures are derived and their small sample properties are studied. The simulation study also considers the standard asymptotic test versions and the Johansen cointegration test for comparison."
"77";77;"2006-013";"Penalties and Optimality in Financial Contracts: Taking Stock";"Michel A. Robe, Eva-Maria Steiger and  Pierre-Armand Michel";"A4";"2006-02-14";"G32,  D82";" A popular view of limited liability in financial contracting is that it is the result of societal preferences against excessive penalties. The view of most financial economists is instead that limited liability emerged as an optimal institution when, in the absence of a clear limit on economic agentsÂ´ liability, the development of some economic activities might have been thwarted. Viewing the institution from the perspective of optimal legal system design allows us to better understand the current debate on it."
"78";78;"2006-014";"Core Labour Standards and FDI: Friends or Foes? The Case of Child Labour";"Sebastian Braun";"C7";"2006-02-24";"C33,  F23,  J82";" We test the often-cited hypothesis that high levels of child labour attract foreign investors. Using panel data we show the overall effect, which child labour has on foreign direct investment (FDI), to be a (small) negative one. We find strong evidence for the theoretical prediction that child labour deters FDI by slowing down economic development. Weaker evidence is provided for our theoretical prediction that child labour can discourage FDI via its impact on the availability of a skilled labour force in an economy. The data do not indicate that high levels of child labour drive down the factor share of labour, thereby increasing the attractiveness of an economy for foreign investors."
"79";79;"2006-015";"Graphical Data Representation in Bankruptcy Analysis";"Wolfgang Härdle,  Rouslan  Moro and   Dorothea Schäfer";"B1";"2006-02-24";"C14,  G33,  C45";" Graphical data representation is an important tool for model selection in bankruptcy analysis since the problem is highly non-linear and its numerical representation is much less transparent. In classical rating models a convenient representation of ratings in a closed form is possible reducing the need for graphical tools. In contrast to that non-linear non-parametric models achieving better accuracy often rely on visualisation. We demonstrate an application of visualisation techniques at different stages of corporate default analysis based on Support Vector Machines (SVM). These stages are the selection of variables (predictors), probability of default (PD) estimation and the representation of PDs for two and higher dimensional models with colour coding. It is at this stage when the selection of a proper colour scheme becomes essential for a correct visualisation of PDs. The mapping of scores into PDs is done as a non-parametric regression with monotonisation. The SVM learns a non-parametric score function that is, in its turn, non-parametrically transformed into PDs. Since PDs cannot be represented in a closed form, some other ways of displaying them must be found. Graphical tools give this possibility."
"80";80;"2006-016";"Fiscal Policy Effects in the European Union";"Andreas Thams";"C6";"2006-02-27";"E30,  E31,  E42,  E62,  E63";" This paper analyzes empirically the impact of fiscal policy on the price level for the cases of Germany and Spain. We investigate whether the fiscal theory of the price level (FTPL) is able to deliver a reasonable explanation for the different performances of the price level in these two countries during recent years. We apply two different approaches. The first is a Bayesian VAR model using sign restrictions to assess the relation between surpluses and public debt. Afterwards, we use a Bayesian regime-switching model to uncover changes in monetary and fiscal policy behavior. The analysis basically shows that in each of the two countries fiscal shocks have a significant impact on the price level. Nonetheless, the FTPL does not deliver a reasonable explanation for the differences in the pattern of inflation between the two countries."
"81";81;"2006-017";"Estimation with the Nested Logit Model: Specifications and Software Particularities";"Nadja Silberhorn, Yasemin Boztug and  Lutz Hildebrandt";"B2";"2006-02-28";"C13,  C51,  C87,  M31";" Due to its ability to allow and account for similarities betweenpairs of alternatives, the nested logit model is increasingly used in practical applications. However the fact that there are two different specifications of the nested logit model has not received adequate attention. The utility maximization nested logit (UMNL) model and the non-normalized nested logit (NNNL) model have different properties, influencing the estimation results in a different manner. As the NNNL specification is not consistent with random utility theory (RUT), the UMNL form is preferred. This article introduces distinct specifications of the nested logit model and indicates particularities arising from model estimation. Additionally, it demonstrates the performance of simulation studies with the nested logit model. In simulation studies with the nested logit model using NNNL software (e. g. PROC MDC   in SAS(c) ), it must be pointed out that the simulation of the utility functionÂ´s error terms needs to assume RUT-conformity. But as the  NNNL specification is not consistent with RUT, the input parameters cannot be reproduced without imposing restrictions. The effects of using various software packages on the estimation results of a nested  logit model are shown on the basis of a simulation study."
"82";82;"2006-018";"The Bologna Process: How student mobility affects multi-cultural skills and educational quality";"Lydia Mechtenberg and   Roland Strausz";"A6";"2006-03-02";"D61,  H77,  I28";" We analyze the two goals behind the European Bologna Process of increasing student mobility: enabling graduates to develop multiÂ–cultural skills and increasing the quality of universities. We isolate three effects:"
"83";83;"2006-019";"Cheap Talk in the Classroom";"Lydia Mechtenberg";"A6";"2006-03-03";"D82,  I21,  J16";" In this paper, I offer a theoretical explanation of the robust gender differences in educational achievement distributions of school children. I consider a one shot cheap talk game with two different types of senders (biased teachers and fair teachers), two types of receivers (Â´Â´normalÂ´Â´ and Â´Â´specialÂ´Â´ pupils) and uncertainty about the sender type on the side of the receiver. I demonstrate that the group of pupils who, in expectation, get either too much or too little encouragement will have less top achievers and a lower average achievement than the group of pupils who get a more accurate feedback message, even if the prior talent distribution is the same for both groups of pupils."
"84";84;"2006-020";"Time Dependent Relative Risk Aversion";"Enzo Giacomini, Michael Handel and  Wolfgang Härdle";"B1";"2006-03-13";"C13,  C22,  G12";" Risk management and the thorough understanding of the relations between financial markets and the standard theory of macroeconomics have always been among the topics most addressed by researchers,  both financial mathematicians and economists. This work aims at explaining  investorsÂ’ behavior from a macroeconomic aspect (modeled by the investorsÂ’ pricing kernel and their relative risk aversion) using  stocks and options data. Daily estimates of investorsÂ’ pricing kernel and relative risk aversion are obtained and used to construct and analyze a three-year long time-series. The  first four moments of these time-series as well  as their values at the money are the starting point of a principal component analysis. The relation between changes in a major index level and implied volatility at the money and between the principal components of the changes in relative risk aversion is found to be linear. The relation of the same explanatory variables to the principal components of the changes in pricing kernels is found to be log-linear, although this relation is not significant for all of the examined maturities."
"85";85;"2006-021";"Finite Sample Properties of Impulse Response Intervals in SVECMs with Long-Run Identifying Restrictions";"Ralf Brüggemann";"C2";"2006-03-07";"C32,  C53,  C15";" This paper investigates the finite sample properties of confidence intervals for structural vector error correction models (SVECMs) with long-run identifying restrictions on the impulse response functions. The simulation study compares methods that are frequently used in applied SVECM studies including an interval based on the asymptotic distribution of impulse responses, a standard percentile (Efron) bootstrap interval, HallÂ’s percentile and  HallÂ’s studentized bootstrap interval. Data generating processes are based on empirical SVECM studies and evaluation criteria include  the empirical coverage, the average length and  the sign implied by the interval. Our Monte Carlo evidence suggests that applied researchers have little to choose between the asymptotic and the Hall bootstrap intervals in SVECMs. In contrast, the Efron bootstrap interval may be less suitable for applied work  as it is less informative about the sign of the underlying impulse response function and the computationally demanding studentized Hall interval is often outperformed by the other methods. Differences between methods are illustrated empirically by using a data set from  King, Plosser, Stock and Watson (1991). "
"86";86;"2006-022";"Barrier Option Hedging under Constraints: A Viscosity Approach";"Imen Bentahar and  Bruno Bouchard";"A3";"2006-03-30";"C60,  G13";" We study the problem of finding the minimal initial capital needed in order to hedge without risk a barrier option when the  vector of proportions of wealth invested in each risky asset is constraint to lie  in a closed convex domain. In the context of a Brownian diffusion model, we provide a PDE characterization  of the super-hedging price. This extends the result of Broadie, Cvitanic and  Soner (1998) and Cvitanic, Pham and Touzi (1999) which was obtained for plain  vanilla options, and provides a natural numerical procedure for computing the  corresponding super-hedging price. As a by-product, we obtain a comparison  theorem for a class of parabolic PDE with relaxed Dirichet conditions involving  a constraint on the gradient."
"87";87;"2006-023";"How Far Are We From The Slippery Slope? The Laffer Curve Revisited";"Mathias Trabandt and  Harald Uhlig";"C1";"2006-04-03";"E0,  E60,  H0";" The goal of this paper is to examine the  shape  of the Laffer curve quantitatively  in a simple neoclassical growth model calibrated to the US as well as  to the EU-15 economy. We show that the  US and the EU-15 area are located  on the left side of their labor and capital tax Laffer curves, but the EU-15 economy  being much closer to the slippery slopes  than the US. Our results indicate that since 1975 the EU-15 area has moved considerably closer to the peaks of their Laffer curves. We find that the slope of the Laffer curve in  the EU-15 economy is much flatter than in  the US which documents a much  higher degree of distortions in the EU-15 area. A dynamic scoring analysis shows that more than one half of a labor tax cut and more than four fifth of a capital tax cut are self-financing in the EU-15 economy."
"88";88;"2006-024";"e-Learning Statistics - A Selective Review";"Wolfgang Härdle, Sigbert Klinke and  Uwe Ziegenhagen";"B1";"2006-04-04";"I21,  C19";" Modern computing equipment is present at schools and universities at all levels  of education. In the statistical sciences computers offer great opportunities to  enrich the learning process by the means of e.g. animations, software integration  or on-the-fly computations."
"89";89;"2006-025";"Macroeconomic Regime Switches and Speculative Attacks";"Bartosz Mackowiak";"C3";"2006-04-05";"E52,  E61,  F33";" This paper explains a currency crisis as an outcome of a switch in how monetary  policy and fiscal policy are coordinated. The paper develops a model of an open economy  in which monetary policy starts active, fiscal policy starts passive and, in a particular  state of nature, monetary policy switches to passive and fiscal policy switches to active.  The probability of the regime switch is endogenous and changes over time together  with the state of the economy. The regime switch is preceded by a sharp increase in  interest rates and causes a jump in the exchange rate. The model predicts that currency composition of public debt affects dynamics of macroeconomic variables. Furthermore, the model is consistent with evidence from recent currency crises, in particular small seigniorage revenues."
"90";90;"2006-026";"External Shocks, U.S. Monetary Policy and Macroeconomic Fluctuations in Emerging Markets";"Bartosz Mackowiak";"C3";"2006-04-05";"F41,  E3,  O11";" Using structural VARs, I find that external shocks are an important source of macroeconomic fluctuations in emerging markets. Furthermore, U.S. monetary policy shocks affect quickly and strongly interest rates and the exchange rate in a typical emerging market. The price level and real output in a typical emerging market respond  to U.S. monetary policy shocks by more than the price level and real output in the U.S. itself. These findings are consistent with the idea that Â“when the U.S. sneezes, emerging markets catch a cold.Â” At the same time,  U.S. monetary policy shocks are not important for emerging markets relative to  other kinds of external shocks."
"91";91;"2006-027";"Institutional Competition, Political Process and Holdup";"Bruno Deffains and   Dominique Demougin";"A4";"2006-04-07";"D02,  D24";" We compare the effect of legal and institutional competition for the  design of labor institutions in an environment characterized by holdup  problems in human and physical capital. We compare autarky with  the two country case, assuming that capital  is perfectly mobile and  labor immobile. We distinguish two cases. In the first, the political  system is free from capture, while in the second, we examine the case  where labor captures the institutional design problem. We find that  in the former case, a competition of systems reduces welfare while in  the latter it improves the overall outcome."
"92";92;"2006-028";"Technological Choice under Organizational Diseconomies of Scale";"Dominique Demougin and   Anja Schöttner";"A4";"2006-04-20";"D82,  L23,  O33";" With adverse selection, diseconomies of  scale associated with hierarchies may  induce the implementation of a second-best technology. This occurs whenever rents to lower tiers of the hierarchy increase  faster than total surplus. This is more likely with longer hierarchies."
"93";93;"2006-029";"Tail Conditional Expectation for vector-valued Risks";"Imen Bentahar";"A3";"2006-04-21";"C60,  G13";" In his paper we introduce a quantile-based  risk measure for multivariate financial  positions: the vector-valued Tail-conditional-expectation (TCE). We adopt the framework proposed  by Jouini, Meddeb, and Touzi [9] to deal with multi-assets portfolios when one accounts for  frictions in the financial market. In this framework, the space of risks formed by essentially  bounded random vectors, is endowed with some partial vector preorder >= accounting for market frictions. In a first step we provide a definition for quantiles of vector-valued risks which is  compatible with the preorder >=.  The TCE is then introduced as a natural extension of the Â“classicalÂ” real-valued tail-conditional-expectation. Our main result states that for continuous  distributions TCE is equal to a coherent vector-valued risk measure. We also provide a numerical algorithm for computing vector-valued quantiles and TCE."
"94";94;"2006-030";"Approximate Solutions to Dynamic Models – Linear Methods";"Harald Uhlig";"C1";"2006-04-24";"C60,  C61,  C63,  E32";" Linear Methods are often used to compute approximate solutions to dynamic models, as these models often cannot be solved analytically. Linear methods are very popular, as they can easily be implemented. Also,  they provide a useful starting point for understanding more elaborate numerical methods. It shall be described here first for  the example of a simple real business cycle model, including how to easily generate the log-linearized equations needed before  solving the linear system. For a general framework, formulas are provided for calculating the recursive law of motion. The algorithm described here is implemented with  the ``toolkitÂ´Â´ programs available per http://www.wiwi.hu-berlin.de/wpol/html/toolkit.htm."
"95";95;"2006-031";"Exploratory Graphics of a Financial Dataset";"Antony Unwin,  Martin Theus and   Wolfgang Härdle";"B1";"2006-04-25";"C14,  G33,  C45";" Keywords: company rating, default probability, support vector machines, colour coding"
"96";96;"2006-032";"When did the 2001 recession really start?";"Jörg Polzehl, Vladimir Spokoiny and  Catalin Starica";"B5";"2006-04-26";"C14,  C16,  C32";" The paper develops a non-parametric, non-stationary framework for business-cycle dating based on an innovative statistical methodology known as Adaptive Weights Smoothing (AWS). The methodology is used both for the study of the individual macroeconomic time series relevant to the dating of the business cycle as well as for the estimation of their joint dynamic."
"97";97;"2006-033";"Varying coefficient GARCH versus local constant volatility modeling. Comparison of the predictive power";"Jörg Polzehl and  Vladimir Spokoiny";"B5";"2006-04-26";"C14,  C22,  C53";" GARCH models are widely used in financial econometrics. However, we show by mean of a simple simulation example that the GARCH approach may lead to a serious model misspecification if the assumption of stationarity is violated. In particular, the well  known integrated GARCH effect can be explained by nonstationarity of the time series."
"98";98;"2006-034";"Spectral calibration of exponential Lévy Models [1]";"Denis Belomestny and  Markus Reiß";"B5C4";"2006-04-27";"G13,  C14";" We investigate the problem of calibrating an exponential LÃ©vy model based on market prices of vanilla options. We show that this inverse problem is in general severely  ill-posed and we derive exact minimax rates  of convergence. The estimation procedure we propose is based on the explicit inversion of the option price formula in the spectral  domain and a cut-off scheme for high frequencies as regularisation."
"99";99;"2006-035";"Spectral calibration of exponential Lévy Models [2]";"Denis Belomestny and   Markus Reiß";"B5C4";"2006-04-28";"G13,  C14";" The calibration of financial models has become rather important topic in recent  years mainly because of the need to price increasingly complex options in a consistent way. The choice of the underlying model is crucial for the good performance of any calibration procedure. Recent empirical evidences suggest that more complex  models taking into account such phenomenons as jumps in the stock prices, smiles in implied volatilities and so on should be considered. Among most popular such models are Levy ones which are on the one hand able to produce complex behavior of the stock time series including jumps, heavy tails and on other hand remain tractable with respect to option pricing. The work on calibration methods for financial models  based on LÃ©vy processes has mainly focused on certain parametrisations of the underlying LÃ©vy process with the notable exception of Cont and Tankov (2004). Since the characteristic triplet of a LÃ©vy process is a  priori an infinite-dimensional object, the parametric approach is always exposed to  the problem of misspecification, in particular when there is no inherent economic  foundation of the parameters and they are  only used to generate different shapes of possible jump distributions. In this  work we propose and test a non-parametric calibration algorithm which is based on the inversion of the explicit pricing formula via Fourier transforms and a regularisation in the spectral domain.Using the Fast Fourier Transformation, the procedure is fast, easy to implement and yields good results in simulations in view of the severe ill-posedness of the underlying inverse problem."
"100";100;"2006-036";"Spatial aggregation of local likelihood estimates with applications to classification";"Denis Belomestny and  Vladimir Spokoiny";"B5";"2006-04-28";"C13,  C14";" This paper presents a new method for  spatially adaptive local likelihood estimation which applies to a broad class of nonparametric models, including  the Gaussian, Poisson and binary response models. The main idea of the method is given a sequence of local likelihood estimates (``weakÂ´Â´ estimates), to construct a new aggregated estimate whose pointwise risk is of order of the smallest risk among all ``weakÂ´Â´ estimates.  We also propose a new approach  towards selecting the parameters of the procedure by providing the prescribed  behavior of the resulting estimate in the  simple parametric situation. We establish a number of important theoretical results concerning the optimality of the aggregated estimate. In particular, our ``oracleÂ´Â´ results claims that its risk is up to some logarithmic multiplier equal to the smallest risk for the given family of estimates. The performance of the procedure is illustrated by application to the classification problem. A numerical study demonstrates its nice performance in simulated and real life examples."
"101";101;"2006-037";"A jump-diffusion Libor model and its robust calibration";"Denis Belomestny and  John Schoenmakers";"B5";"2006-04-26";"C15,  G12";" In this paper we propose a jump-diffusion  Libor model with jumps in a high-dimensional space (Rm) and test a stable non-parametric calibration algorithm which takes into account a given local covariance structure.  The algorithm returns smooth and simply structured LÃ©vy densities, and  penalizes the deviation from the Libor market model. In practice, the  procedure is FFT based, thus fast, easy to implement, and yields good  results, particularly in view of the severe ill-posedness of the underlying  inverse problem."
"102";102;"2006-038";"Adaptive Simulation Algorithms for Pricing American and Bermudan Options by Local Analysis of Financial Market";"Denis Belomestny and  Grigori N. Milstein";"B5";"2006-04-28";"C15,  G12";" Here we develop an approach for efficient pricing discrete-time American  and Bermudan options which employs the  fact that such options are equivalent to the  European ones with a consumption,  combined with analysis of the market model over a small number of steps ahead. This approach allows constructing both upper and low  bounds for the true price by Monte Carlo simulations. An adaptive choice of local low  bounds and use of the kernel interpolation technique enhance efficiency of the whole  procedure, which is supported by numerical experiments."
"103";103;"2006-039";"Macroeconomic Integration in Asia Pacific: Common Stochastic Trends and Business Cycle Coherence";"Enzo Weber";"C6";"2006-05-02";"E32,  F15,  C32";" This paper addresses the question of macroeconomic integration in the Asian Pacific region. Economically, the analysis is based on the notions of stochastic long-run convergence and business cycle coherence. The econometric procedure consists of tests for cointegration, the examination of vector error correction models, several variants of common cycle tests and forecast error variance decompositions. Results in favour of cyclical synchrony can be partly established, and are even exceeded by the broad evidence for equilibrium relations. In these domains, several leading countries are identified."
"104";104;"2006-040";"In Search of Non-Gaussian Components of a High-Dimensional Distribution";"Gilles Blanchard,  Motoaki Kawanabe,  Masashi Sugiyama,  Vladimir Spokoiny and   Klaus-Robert Müller";"B5";"2006-05-02";"C14,  C51";" Finding non-Gaussian components of high-dimensional data is an important preprocessing step for effcient information processing. This article proposes a new linear method to identify the ``non-Gaussian subspaceÂ´Â´ within a very general semi-parametric framework.  Our proposed method, called NGCA (Non-Gaussian Component Analysis), is essentially based on a linear operator which, to any arbitrary nonlinear (smooth) function, associates a vector which belongs to the low dimensional non-Gaussian target subspace  up to an estimation error. By applying this operator to a family of different nonlinear functions, one obtains a family of different vectors lying in a vicinity of the target space. As a final step, the target space itself is estimated by applying PCA to this family of vectors. We show that this procedure is consistent in the sense that the estimaton error tends to zero at a parametric rate, uniformly over the family, Numerical examples demonstrate the usefulness of our method."
"105";105;"2006-041";"Forward and reverse representations for Markov chains";"Grigori N. Milstein,  John Schoenmakers and   Vladimir Spokoiny";"B5";"2006-05-02";"C13,  C15";" In this paper we carry over the concept of reverse probabilistic representations  developed in Milstein, Schoenmakers, Spokoiny (2004) for diffusion processes, to discrete time Markov chains. We outline the construction of reverse chains in several situations and apply this to processes  which are connected with jump-diffusion models and finite state Markov chains. By combining forward and reverse  representations we then construct transition density estimators for chains which have root-N accuracy in any dimension and consider some applications."
"106";106;"2006-042";"Discussion of ""The Source of Historical Economic Fluctu- ations: An Analysis using Long-Run Restrictions"" by Neville Francis and Valerie A. Ramey";"Harald Uhlig";"C1";"2006-05-08";"E22,  E23,  E24,  E32,  C32";" This paper discusses the paper ``The Source of Historical Economic Fluctuations: An Analysis using Long-Run RestrictionsÂ´Â´ by Neville Francis and Valerie A. Ramey. It argues that these authors have made great progress both in the precise measurement of labor input as well as determining the effect of productivity shocks on labor, but a number of questions remain.  As for measurement, the issue of schooling needs further work. As for calculating the long-run impact of labor productivity shocks, unreasonable results emerge for the response of the capital stock,  if included in the VAR.  Using medium-term identification delivers more reasonable results."
"107";107;"2006-043";"An Iteration Procedure for Solving Integral Equations Related to Optimal Stopping Problems";"Denis Belomestny and  Pavel V. Gapeev";"B5";"2006-05-17";"C15,  G12";" A new algorithm for finding value functions of finite horizon optimal stopping problems in one-dimensional diffusion models is presented. It is based on a time discretization of the corresponding integral equation. The  proposed iterative procedure for solving the discretized integral equation converges in a finite number of steps and delivers in each step a lower or an upper bound for value of discretized problem on the whole time  interval. The remarks on the application of the method for solving integral equations related  to some optimal stopping problems are given."
"108";108;"2006-044";"East Germany’s Wage Gap: A non-parametric decomposition based on establishment characteristics";"Bernd Görzig,  Martin Gornig and   Axel Werwatz";"B3";"2006-05-31";"J31,  L16,  C14,  C31";" East German wages have been below the West German wage level since unification. Moreover, the East-West wage gap implied  by the contractual wages specified in collective wage agreements is drifting ever further apart from the wage gap in terms of effective wages. This paper looks at the role  of establishment-specific factors Â— such as sectoral affiliation and size of the labour  force Â— in this process. A non-parametric decomposition that has played a prominent role in the gender wage-gap literature is applied to breakdown the East- West wage gap into its constituent components. Using establishment data from the German employment statistics, the paper demonstrates that the divergence between  wage agreements and effective wages is probably not a consequence of a massive  escape from collective wage agreements, or the intense use of opt-out clauses in  such agreements in East Germany. Rather, the shift of East GermanyÂ’s economic  structure towards lower-paying types of  companies has caused the lagging behind in  the adjustment of wages."
"109";109;"2006-045";"Firm Specific Wage Spread in Germany - Decomposition of regional differences in inter firm wage dispersion";"Bernd Görzig,  Martin Gornig and   Axel Werwatz";"B3";"2006-05-31";"L16,  C14,  J30";" The purpose of this paper is to sort out firm-related differences from effects that result  from different economic structures. A non-parametric decomposition is used to  analyse firm level difference between the wage spread in the two major regions of  unified Germany. If firm-specific effects explain wage dispersion between firms, a  decomposition of the wage dispersion between firms is necessary. The decomposition  can help to find out, whether the economy-wide results for different regions are  due to the composition of the regional economies by industries and firm-size, or  whether the differences are due to firm-specific influences, like distinctions in market  power. For Germany, a considerable part of the difference in the wage spread between  the East and the West can be explained by different economic structures.  However, by far the greater part of the difference in the wage spread between firms  in the two parts of the country results from lower wages paid by firms of the same  type in East Germany compared with their counterparts in West Germany."
"110";110;"2006-046";"Produktdiversifizierung: Haben die ostdeutschen Unternehmen den Anschluss an den Westen geschafft? – Eine vergleichende Analyse mit Mikrodaten der amtlichen Statistik";"Bernd Görzig,  Martin Gornig and   Axel Werwatz";"B3";"2006-05-31";"P23,  L60,  C14";" Die Gestaltung der Produktpalette war ein zentrale Herausforderung fÃ¼r ostdeutsche Unternehmen  nach der Wende. Spezialisierung oder eine diffuse Generalistenstrategie war die  Frage. Welche Strategie sich durchgesetzt hat und ob der Anschluss an den Westen gelang,  wird in dieser Arbeit erstmals auf reprÃ¤sentativer Basis fÃ¼r das verarbeitende Gewerbe mit  den Mikrodaten der amtlichen Statistik untersucht. Mit einem nichtparametrischen Dekompositionsansatz  wird das West-Ost-DiversifizierungsgefÃ¤lle 1995 und 2001 in einen reinen Regionaleffekt  und in strukturelle Komponenten zerlegt. Dabei zeigt sich, dass bei vergleichbaren  Unternehmen heute keine signifikanten West-Ost-Unterschiede mehr im Produktdiversifizierungsverhalten  bestehen. PrÃ¤gnant sind aber weiterhin strukturelle Unterschiede Â– wie  das Defizit an hochdiversifizierten GroÃŸunternehmen."
"111";111;"2006-047";"The Division of Ownership in New Ventures";"Dominique Demougin and   Oliver Fabel";"A4";"2006-06-01";"M13,  M21";" The current study investigates a tripartite incentive contract between an innovator supplying an  intellectual asset, a professional assigned to productive tasks, and a consulting firm specializing  in matching ideas and professional skills. A rather simple pure tripartite partnership implements  the consultantÂ´s expected profit maximum and maximizes the project`s expected surplus. The  liquidity-constrained professional is compensated by receiving a share of one half in the new venture.  The consultantÂ´s and the innovatorÂ´s shares reflect the relative value of search. However, the  consultantÂ´s optimal search effort to find an appropriate production partner is inefficiently low."
"112";112;"2006-048";"The Anglo-German Industrial Productivity Paradox, 1895-1938: A Restatement and a Possible Resolution";"Albrecht Ritschl";"C5";"2006-06-22";"N10,  N60";" Recent research on international productivity comparisons with historical data has encountered  large discrepancies between benchmark comparisons and time series extrapolations from other benchmarks.  Broadberry and Burhop (2005) have recently argued that for HoffmannÂ’s (1965) widely accepted time series  for German industrial output, there is no such productivity paradox, while for a revision of that series  recently suggested by Ritschl (2004), the discrepancy between the Anglo-German benchmark and the time  series projection is considerable. Attempting to reconcile the time series evidence and the productivity  benchmarks, they discard the revised series in favor of the original, disregarding mounting evidence on  its lacking reliability. The present paper restates this productivity paradox and proposes a possible  resolution. We draw on recent archival discoveries by Fremdling and Staeglin (2003) and Fremdling (2005)  that confirm the revisions to the Hoffmann series. We also draw on recent advances in the reconstruction  of a German industry census of 1936, and argue that the productivity paradox is largely the consequence  of mismeasurement in all versions of the German series. Correcting for the omissions, much of the  Anglo-German productivity paradox disappears."
"113";113;"2006-049";"The Influence of Information Costs on the Integration of Financial Markets: Northern Europe, 1350-1560";"Oliver Volckart";"C5";"2006-06-22";"E44,  F31,  F36,  N24";" In this paper, the influence of information costs on the integration of Northern European  financial markets between ca. 1350 and 1560 is explored. The approach is based on splitting information  costs into their constitutive components and on measuring one of these, i.e. the costs of transmitting  information, which have particular importance for market integration. The analysis has two main results:  First, under pre-industrial conditions, when transmitting information was extremely labour intensive and  very little capital intensive, transmission costs can be largely identified with labour costs, and were  subject to the same influences. Next, the integration of financial markets depended crucially on the  level of transmission costs, high costs being strongly and significantly correlated with weak integration,  while lower costs favoured convergence."
"114";114;"2006-050";"Robust Econometrics";"Pavel Cizek and  Wolfgang Härdle";"B1";"2006-06-30";"";" Econometrics often deals with data under, from the statistical point of view, non-standard conditions such as heteroscedasticity or measurement errors and the estimation methods need thus be either adopted to such conditions or be at least insensitive to them. The methods insensitive to violation of certain assumptions, for example insensitive to the presence of heteroscedasticity, are in a broad sense referred to as robust (e.g., to heteroscedasticity). On the other hand, there is also a more specific meaning of the word `robust`, which stems from the field of robust statistics. This latter notion defines robustness rigorously in terms of behavior of an estimator both at the assumed (parametric) model and in its neighborhood in  the space of probability distributions. Even though the methods of robust  statistics have been used only in the simplest setting such as estimation of location, scale, or linear regression for a long time, they motivated a range of new econometric methods recently, which we focus on in this chapter."
"115";115;"2006-051";"Regression methods in pricing American and Bermudan options using consumption processes";"Denis Belomestny, Grigori N. Milstein and  Vladimir Spokoiny";"B5";"2006-07-06";"C15,  G12";" Here we develop methods for eÂ±cient pricing multidimensional discrete-time American and  Bermudan options by using regression based algorithms together with a new approach towards constructing  upper bounds for the price of the option. Applying the sample space with payoffs at the optimal stopping  times, we propose sequential estimates for continuation values, values of the consumption process, and  stopping times on the sample paths. The approach admits constructing both low and upper bounds for the  price by Monte Carlo simulations. The methods are illustrated by pricing Bermudan swaptions and snowballs  in the Libor market model."
"116";116;"2006-052";"Forecasting the Term Structure of Variance Swaps";"Kai Detlefsen and  Wolfgang Härdle";"B1";"2006-07-06";"G1,  D4,  C5";" Recently, Diebold and Li (2003) obtained  good forecasting results for yield curves in a  reparametrized Nelson-Siegel framework. We analyze similar modeling approaches for price curves of variance swaps that serve nowadays as hedging instruments for options on  realized variance."
"117";117;"2006-053";"Governance: Who Controls Matters";"Bruno Deffains and  Dominique Demougin";"A4";"2006-07-25";"G3";" In this paper, we provide an outlook for  further research on the topic of governance.  We review four different approaches on the  theory of the firm and discuss implications  for governance, namely; nexus of contracts / agency theory, property rights / incomplete contracts, adaptation, and nexus of specific investments."
"118";118;"2006-054";"On the Coexistence of Banks and Markets";"Hans Gersbach and  Harald Uhlig";"C1";"2006-08-14";"G24,  G28,  G32,  G38,  D80,  ";" We examine the coexistence of banks and financial markets, studying a credit market where the qualities of investment projects are not observable and the investment decisions  of entrepreneurs are not contractible.  Standard banks can alleviate moral-hazard problems by securing a portion of a  repayment in the case of non-investment. Financial markets operated by investment banks and rating agencies have screening know-how and can alleviate adverse-selection problems. In competition, standard banks are forced to increase repayments, since financial markets can attract the highest-quality borrowers. This, in turn, increases the share  of shirkers and may make lending unprofitable for standard banks. The coexistence of financial markets and standard banks is socially inefficient. The same inefficiency can happen with the entrance of sophisticated banks, operating with a combination of rating and ongoing monitoring technologies."
"119";119;"2006-055";"Reassessing Intergenerational Mobility in Germany and the United States: The Impact of Differences in Lifecycle Earnings Patterns";"Thorsten Vogel";"C7";"2006-09-11";"D31,  J31,  J62";" Using longitudinal data on fathers and their children, this study compares  the extent of intergenerational mobility in Germany and the United States  and introduces an estimation strategy that corrects estimates of intergenerational  earnings elasticities for a possible lifecycle bias. In contrast to previous  studies, we find that the extent of intergenerational mobility is more limited  in the US than in Germany. Furthermore,  while the errors-in-variables problems  have been dealt with extensively in the literature, the inconsistencies  in standard mobility measures due to lifecycle effects have attracted much  less attention. The present paper proposes an estimation method that corrects  for such inconsistencies. The extent of this lifecycle bias is found to be  strong in Germany but only modest in the US."
"120";120;"2006-056";"The Euro and the Transatlantic Capital Market Leadership: A Recursive Cointegration Analysis";"Enzo Weber";"C6";"2006-09-11";"E44,  F36,  C32";" In this paper, the capital market relations between the Euro area and the USA are subject to investigation. Formally based on  the uncovered interest rate parity (UIP), first  a longrun equilibrium between Euro and US government bond yields is established in backward recursively estimated vector error correction models (VECMs). Subsequently, the focus lies on interest rate leadership and adjustment as well as capital market integration. One major finding shows, that  the foundation of the European Monetary Union (EMU) strengthened its role relative to the USA. Furthermore, the transatlantic connections have become closer in the  course time."
"121";121;"2006-057";"Discounted Optimal Stopping for Maxima in Diffusion Models with Finite Horizon";"Pavel V. Gapeev";"B5";"2006-09-11";"G13";" We present a solution to some discounted optimal stopping problem for the maximum of a geometric Brownian motion on a finite time interval. The method of proof is based on reducing the initial optimal stopping  problem with the continuation region determined by an increasing continuous boundary surface to a parabolic free-boundary problem. Using the change-of-variable formula with local time on surfaces we show that the  optimal boundary can be characterized as a unique solution of a nonlinear integral equation. The result can be interpreted as pricing American fixed-strike lookback option in a diffusion model with finite time horizon."
"122";122;"2006-058";"Perpetual Barrier Options in Jump-Diffusion Models";"Pavel V. Gapeev";"B5";"2006-09-11";"G13";" We present a closed form solution to the perpetual American double barrier call  option problem in a model driven by a Brownian motion and a compound Poisson  process with exponential jumps. The method of proof is based on reducing the initial  irregular optimal stopping problem to an integro-differential free-boundary problem  and solving the latter by using continuous  and smooth fit. The obtained solution of  the nontrivial free-boundary problem gives the possibility to observe some special  analytic properties of the value function at the optimal stopping boundaries."
"123";123;"2006-059";"Discounted Optimal Stopping for Maxima of some Jump-Diffusion Processes";"Pavel V. Gapeev";"B5";"2006-09-11";"G13";" We present solutions to some discounted optimal stopping problems for the maximum process in a model driven by a Brownian motion and a compound Poisson process  with exponential jumps. The method of proof  is based on reducing the initial problems to integro-differential free-boundary  problems where the normal reflection and smooth fit may break down and  the latter then be replaced by the continuous fit. The results can be  interpreted as pricing perpetual American lookback options with fixed and  floating strikes in a jump-diffusion model."
"124";124;"2006-060";"On Maximal Inequalities for some Jump Processes";"Pavel V. Gapeev";"B5";"2006-09-11";"G13";" We present a solution to the considered in [5] and [22] optimal stopping  problem for some jump processes. The method of proof is based on  reducing the initial problem to an integro-differential free-boundary problem  where the normal reflection and smooth fit may break down and the  latter then be replaced by the continuous fit. The derived result is applied  for determining the best constants in maximal inequalities for a compound  Poisson process with linear drift and exponential jumps."
"125";125;"2006-061";"A Control Approach to Robust Utility Maximization with Logarithmic Utility and Time-Consistent Penalties";"Daniel Hernandez–Hernandez and  Alexander Schied";"A3";"2006-09-11";"G11,  D81";" We propose a stochastic control approach to the dynamic maximization of  robust utility functionals that are defined in terms of logarithmic utility and a dynamically  consistent convex risk measure. The underlying market is modeled by a diffusion process  whose coefficients are driven by an external stochastic factor process. In particular, the  market model is incomplete. Our main results give conditions on the minimal penalty  function of the robust utility functional under which the value function of our problem  can be identified with the unique classical solution of a quasilinear PDE within a class of  functions satisfying certain growth conditions. The fact that we obtain classical solutions  rather than viscosity solutions is important for the use of numerical algorithms, whose  applicability is demonstrated in examples."
"126";126;"2006-062";"On the Difficulty to Design Arabic E-learning System in Statistics";"Taleb Ahmad, Wolfgang Härdle and  Julius Mungo";"B1";"2006-09-11";"I21,  C19";" In this paper, we present a case study, which describe the development of the Statistic  e-learning-course in Arabic language Â–``Arabic MM*STATÂ´Â´. The basic frame for this E-book, the system MM*STAT was developed at the School for Business and Economics of Humboldt-UniversitÃ¤t zu Berlin. Arabic MM*STAT uses a HTML - based  filing card structure. We discuss the difficulties of the implementation of such a system  in to the standard WWW formats and present the solutions needed for Arab educational  institutions and the Arabic user. Those solutions are consistent with the Arabic language,  and include the modern trend in the e-learning environment."
"127";127;"2006-063";"Robust Optimization of Consumption with Random Endowment";"Wiebke Wittmüß";"A3";"2006-09-11";"D11,  D81";" We consider the problem of optimal consumption for an investor who is risk and uncertainty  avers. We model these preferences of the investor with the help of a convex risk-measure. Apart  from consumption the agent has the possibility to invest initial capital and random endowment  in a market where stock-prices are semimartingales. We formulate this as a maximin problem  that will be solved by duality methods."
"128";128;"2006-064";"Common and Uncommon Sources of Growth in Asia Pacific";"Enzo Weber";"C6";"2006-09-11";"O11,  F15,  C32";" This paper embarks to analyse the role of exports and investment supposed to be major  sources of economic growth in Asia Pacific. Therefore at first, the cointegration properties  of exports, capital formation and GDP are examined in vector error correction models  (VECMs). The results confirm the crucial role of exports and investment in the Asian  growth dynamics. In a second stage, the structural shocks are identified by short- and  long-run restrictions. These shocks, as well  as the corresponding dynamic responses, are  then correlated across all sample countries to provide insight into the depth of regional  coherence. At last, the identified trends are explained by various macroeconomic variables."
"129";129;"2006-065";"Forecasting Euro-Area Variables with German Pre-EMU Data";"Ralf Brüggemann, Helmut Lütkepohl and  Massimiliano Marcellino";"C2";"2006-09-11";"C22,  C53";" It is investigated whether Euro-area variables can be forecast better based on synthetic  time series for the pre-Euro period or by using just data from Germany for the pre-Euro period.  Our forecast comparison is based on  quarterly data for the period 1970Q1 - 2003Q4  for ten macroeconomic variables. The years 2000 - 2003 are used as forecasting period. A  range of different univariate forecasting methods is applied. Some of them are based on linear  autoregressive models and we also use some nonlinear or time-varying coefficient models. It  turns out that most variables which have a similar level for Germany and the Euro-area such  as prices can be better predicted based on German data while aggregated European data are  preferable for forecasting variables which need considerable adjustments in their levels when  joining German and EMU data. These results suggest that for variables which have a similar  level for Germany and the Euro-area it may be reasonable to consider the German pre-EMU  data for studying economic problems in the Euro-area."
"130";130;"2006-066";"Pension Sytems and the Allocation of Macroeconomic Risk";"Lans Bovenberg and  Harald Uhlig";"C1";"2006-09-11";"E21,  E61,  E62,  O40,  H21,  ";" This paper explores the optimal risk sharing arrangement between generations in an overlapping generations model with endogenous growth.  We allow for nonseparable preferences, paying particular attention to the risk aversion of the old as well as overall ``life-cycleÂ´Â´ risk aversion.  We provide a fairly tractable model, which can serve as a starting point to explore these issues in models with a larger number of periods of life, and show how it can be solved.  We provide a general risk sharing condition, and discuss its implications.  We explore the properties of the model quantitatively.  Among the key findings are the  following.  First and for reasonable parameters, the old typically bear a larger burden of the risk in productivity surprises, if old-age risk-aversion is smaller than life risk aversion, and vice versa.  Thus, it is not necessarily the case that the young ensure smooth consumption of the old.  Second, consumption of the young and the old always move in the same direction, even for population growth shocks.  This result is in contrast to the result of a fully-funded decentralized system without risk-sharing between generations. Third,  persistent increases in longevity will lead to lower total consumption of the old (and thus certainly lower per-period consumption of the old) as well as the young as well as higher work effort of the young.  The additional resources are instead used to increase  growth and future output, resulting in higher consumption of future generations."
"131";131;"2006-067";"Testing for the Cointegrating Rank of a VAR Process with Level Shift and Trend Break";"Carsten Trenkler, Pentti Saikkonen and  Helmut Lütkepohl";"C2";"2006-09-11";"C32";" A test for the cointegrating rank of a vector autoregressive (VAR) process with a possible  shift and broken linear trend is proposed. The break point is assumed to be known. The  setup is a VAR process for cointegrated variables. The tests are not likelihood ratio tests  but the deterministic terms including the broken trends are removed first by a GLS procedure and a likelihood ratio type test is applied to the adjusted series. The  asymptotic null distribution of the test is derived and it is shown by a Monte Carlo experiment that the test  has better small sample properties in many cases than a corresponding Gaussian likelihood ratio test for the cointegrating rank."
"132";132;"2006-068";"Integral Options in Models with Jumps";"Pavel V. Gapeev";"B5";"2006-09-11";"G13";" We present an explicit solution to the formulated in [17] optimal stopping  problem for a geometric compound Poisson process with exponential  jumps. The method of proof is based on reducing the initial problem to   an integro-differential free-boundary problem where the smooth fit may  break down and then be replaced by the continuous fit. The result can be  interpreted as pricing perpetual integral options in a model with jumps."
"133";133;"2006-069";"Constrained General Regression in Pseudo-Sobolev Spaces with Application to Option Pricing";"Zdenek Hlavka and  Michal Pesta";"B1";"2006-09-25";"C10,  C13,  C14,  C20,  C88,  ";" State price density (SPD) contains important information concerning market expectations. In  existing literature, a constrained estimator of the SPD is found by nonlinear least squares in a suitable  Sobolev space. We improve the behavior of this estimator by implementing a covariance structure taking  into account the time of the trade and by considering simultaneously both the observed Put and Call  option prices."
"134";134;"2006-070";"The Welfare Enhancing Effects of a Selfish Government in the Presence of Uninsurable, Idiosyncratic Risk";"R. Anton Braun and  Harald Uhlig";"C1";"2006-09-25";"H20,  H21,  H23,  D31,  D33,  ";" This paper poses the following question: Is it possible to improve welfare by increasing taxes  and throwing away the revenues? This paper demonstrates that the answer to this question is Â“yes.Â” We  show that there may be welfare gains from taxing capital income even when the additional capital income  tax revenues are wasted or consumed by a selfish government. Previous literature has assumed that  government expenditures are exogenous or productive, or allowed for redistribution of tax revenue either  via lump-sum transfers, unemployment compensation or other redistributive schemes. In our model a selfish  government taxes capital above a given threshold and then consumes the proceeds. This raises the  before-tax real return on capital and and thereby enhances the ability of agents to self-insure when they  are long-term unemployed and have low savings. Since all agents have positive probability of finding  themselves in that state there are cases where all agents prefer a selfish government to no government  at all."
"135";135;"2006-071";"Color Harmonization in Car Manufacturing Process";"Anton Andriyashin, Michal Benko, Wolfgang Härdle, Roman Timofeev and  Uwe Ziegenhagen";"B1";"2006-10-06";"C14,  C19,  C89";" One of the major cost factors in car manufacturing is the painting of body and other parts such as wing or bonnet. Surprisingly, the painting may be even more expensive than the body itself. From this point of view it is clear that car manufacturers need to observe the painting process carefully to avoid any deviations from the desired result. Especially for metallic colors where the shining is based on microscopic aluminium particles, customers tend to be very sensitive towards a difference in the light reflection of different parts of the car."
"136";136;"2006-072";"Optimal Interest Rate Stabilization in a Basic Sticky-Price Model";"Matthias Paustian and  Christian Stoltenberg";"C1";"2006-10-12";"E32,  E52,  E58";" This paper studies optimal monetary policy with the nominal interest rate as the single policy instrument in an economy, where firms set prices in a staggered way without indexation and real money balances contribute separately to householdsÂ´ utility. The optimal deterministic steady state under commitment is the Friedman rule - even if the importance assigned to the utility of money is small relative to consumption and leisure. We approximate the model around the optimal steady state as the long-run policy target. Optimal monetary policy is characterized by stabilization of the nominal interest rate instead of inflation stabilization as the predominant principle."
"137";137;"2006-073";"Real Balance Effects, Timing and Equilibrium Determination";"Christian Stoltenberg";"C1";"2006-10-12";"E32,  E41,  E52";" This paper examines whether the existence and the timing of real balance effects contribute to the determination of the absolute price level, as suggested by Patinkin (1949,1965), and if they affect conditions for local equilibrium uniqueness and stability. I show that there exists a unique price level sequence that is consistent with an equilibrium under interest rate policy, only if beginning-of-period money yields transaction services. Predetermined real money balances can then serve as a state variable, implying that interest rate setting must be passive - a violation of the Taylor-principle - for unique, stable, and non-oscillatory equilibrium sequences. On the contrary, when the end-of-period money stock facilitates transactions, the equilibrium displays nominal indeterminacy and equilibrium uniqueness requires an interest rate setting consistent with the Taylor-principle."
"138";138;"2006-074";"Multiple Disorder Problems for Wiener and Compound Poisson Processes With Exponential Jumps";"Pavel Gapeev";"B5";"2006-10-24";"G13";" The multiple disorder problem consists of finding a sequence of stopping times which are as close as possible to the (unknown) times of ``disorderÂ´Â´ when the distribution of an observed process changes its probability characteristics. We present a formulation and solution of the multiple disorder problem for a Wiener and a compound Poisson process with exponential jumps. The method of proof is based on reducing the initial optimal switching problems to the corresponding coupled optimal stopping problems and solving the equivalent coupled free-boundary problems by means of the smooth- and continuous-fit conditions."
"139";139;"2006-075";"Inhomogeneous Dependency Modelling with Time Varying Copulae";"Enzo Giacomini, Wolfgang Härdle, Ekaterina Ignatieva and  Vladimir Spokoiny";"B1B5";"2006-11-14";"C14";" Measuring dependence in a multivariate time series is tantamount to modelling its dynamic structure in space and time. In the context of a multivariate normally distributed time series, the evolution of the covariance (or correlation) matrix over time describes this dynamic. A wide variety of applications, though, requires a modelling framework different from the multivariate normal. In risk management the non-normal behaviour of most financial time series calls for nonlinear (i.e. non-gaussian) dependency. The correct modelling of non-gaussian dependencies is therefore a key issue in the analysis of multivariate time series. In this paper we use copulae functions with adaptively estimated time varying parameters for modelling the distribution of returns, free from the usual normality assumptions. Further, we apply copulae to estimation of Value-at-Risk (VaR) of a portfolio and show its better performance over the RiskMetrics approach, a widely used methodology for VaR estimation."
"140";140;"2006-076";"Convenience Yields for CO2 Emission Allowance Futures Contracts";"Szymon Borak, Wolfgang Härdle, Stefan Trück and  Rafal Weron";"B1";"2006-11-14";"Q28,  G13,  C19";" In January 2005 the EU-wide CO2 emissions trading system (EU-ETS) has formally entered into operation.Within the new trading system, the right to emit a particular amount of CO2 becomes a tradable commodity - called EU Allowances (EUAs) - and affected companies, traders and investors will face new strategic challenges. In this paper we investigate the nature of convenience yields for CO2 emission allowance futures. We conduct an empirical study on price behavior, volatility term structure and correlations in different CO2 EUA contracts. Our findings are that the market has changed from initial backwardation to contango with significant convenience yields in future contracts for the Kyoto commitment period starting in 2008. A high fraction of the yields can be explained by the price level and volatility of the spot prices. We conclude that the yields can be interpreted as market expectation on the price risk of CO2 emissions allowance prices and the uncertainty of EU allocation plans for the Kyoto period."
"141";141;"2006-077";"Estimation of Default Probabilities with Support Vector Machines";"Shiyi Chen, Wolfgang Härdle and  Rouslan Moro";"B1";"2006-11-16";"C14,  G33,  C45,  G32";" Predicting default probabilities is important for firms and banks to operate successfully and to estimate their specific risks. There are many reasons to use nonlinear techniques for predicting bankruptcy from financial ratios. Here we propose the so called Support Vector Machine (SVM) to estimate default probabilities of German firms. Our analysis is based on the Creditreform database. The results reveal that the most important eight predictors related to bankruptcy for these German firms belong to the ratios of activity, profitability, liquidity, leverage and the percentage of incremental inventories. Based on the performance measures, the SVM tool can predict a firms default risk and identify the insolvent firm more accurately than the benchmark logit model. The sensitivity investigation and a corresponding visualization tool reveal that the classifying ability of SVM appears to be superior over a wide range of the SVM parameters. Based on the nonparametric Nadaraya-Watson estimator, the expected returns predicted by the SVM for regression have a significant positive linear relationship with the risk scores obtained for classification. This evidence is stronger than empirical results for the CAPM based on a linear regression and confirms that higher risks need to be compensated by higher potential returns."
"142";142;"2006-078";"GHICA - Risk Analysis with GH Distributions and Independent Components";"Ying Chen, Wolfgang Härdle and  Vladimir Spokoiny";"B1B5";"2006-11-16";"C14,  C16,  C32,  C61,  G20";" Over recent years, study on risk management has been prompted by the Basel committee for regular banking supervisory. There are however limitations of some widely-used risk management methods that either calculate risk measures under the Gaussian distributional assumption or involve numerical difficulty. The primary aim of this paper is to present a realistic and fast method, GHICA, which overcomes the limitations in multivariate risk analysis. The idea is to first retrieve independent components (ICs) out of the observed high-dimensional time series and then individually and adaptively fit the resulting ICs in the generalized hyperbolic (GH) distributional framework. For the volatility estimation of each IC, the local exponential smoothing technique is used to achieve the best possible accuracy of estimation. Finally, the fast Fourier transformation technique is used to approximate the density of the portfolio returns. The proposed GHICA method is applicable to covariance estimation as well. It is compared with the dynamic conditional correlation (DCC) method based on the simulated data with d = 50 GH distributed components. We further implement the GHICA method to calculate risk measures given 20-dimensional German DAX portfolios and a dynamic exchange rate portfolio. Several alternative methods are considered as well to compare the accuracy of calculation with the GHICA one."
"143";143;"2006-079";"Do Individuals Recognize Cascade Behavior of Others? - An Experimental Study";"Tim Grebe, Julia Schmid and  Andreas Stiehler";"A6";"2006-11-27";"C91,  D81,  D82";" In an information cascade experiment participants are confronted with artificial  predecessors predicting in line with the BHW model (Bikchandani et al., 1992). Using the BDM (Becker et al., 1964) mechanism we study participantsÂ´ probability perceptions based on maximum prices for participating in the prediction game. We find increasing maximum prices the more coinciding predictions of predecessors are observed, regardless of whether additional information is revealed by these predictions. Individual price patterns of more than two thirds of the participants indicate that cascade behavior of predecessors is not recognized."
"144";144;"2006-080";"The Uniqueness of Extremum Estimation";"Volker Krätschmer";"A3";"2006-12-11";"C13,  C16";" Let W denote a family of probability distributions with parameter space "
"145";145;"2006-081";"Compactness in Spaces of Inner Regular Measures and a General Portmanteau Lemma";"Volker Krätschmer";"A3";"2006-12-11";"C65";" This paper may be understood as a continuation of TopsÃ¸eÂ’s seminal paper ([16]) to characterize, within an abstract setting, compact subsets of finite inner regular measures w.r.t. the weak topology. The new aspect is that neither assumptions on compactness of the inner approximating lattices nor nonsequential continuity properties for the measures will be imposed. As a providing step also a generalization of the classical Portmanteau lemma will be established. The obtained characterizations of compact subsets w.r.t. the weak topology encompass several known ones from literature. The investigations rely basically on the inner extension theory for measures which has been systemized recently by KÃ¶nig ([8], [10],[12])."
"146";146;"2006-082";"Probleme der Validierung mit Strukturgleichungsmodellen";"Lutz Hildebrandt and  Dirk Temme";"B2";"2006-12-11";"C31,  C51,  C52,  M31";" Dieser Beitrag setzt sich mit der LeistungsfÃ¤higkeit von Strukturgleichungsmodellen bei der ValiditÃ¤tsprÃ¼fung von Messmodellen fÃ¼r hypothetische Konstrukte auseinander und geht auf ausgewÃ¤hlte Problembereiche bei der gÃ¤ngigen Anwendung dieser Methodik fÃ¼r die Skalenkonstruktion ein. Insbesondere werden mit der Kontrolle verschiedener Arten von Methodeneffekten Alternativen zur Elimination von Indikatoren ausschlieÃŸlich auf Basis statistischer Kriterien (z. B. interne Konsistenz) aufgezeigt."
"147";147;"2006-083";"Formative Measurement Models in Covariance Structure Analysis: Specification and Identification";"Dirk Temme and  Lutz Hildebrandt";"B2";"2006-12-11";"C31,  C51,  C52,  M31";"  Many researchers seem to be unsure about how to specify formative measurement models in software programs like LISREL or AMOS and to establish identification of the corresponding structural equation model. In order to make identification easier, a new, mainly graphically oriented approach is presented for a specific class of recursive models with formative indicators. Using this procedure it is shown that some models have erroneously been considered underidentified. Furthermore, it is shown that specifying formative indicators as exogenous variables rises serious conceptual and substantial issues in the case that the formative construct is truly endogenous (i. e. influenced by more remote causes). An empirical study on the effects and causes of brand competence illustrates this point."
"148";148;"2006-084";"PLS Path Modeling – A Software Review";"Dirk Temme, Henning Kreis and  Lutz Hildebrandt";"B2";"2006-12-18";"C31,  C87,  M31";" After years of stagnancy, PLS path modeling has recently attracted renewed interest from applied researchers in marketing. At the same time, the availability of software alternatives to LohmÃ¶llerÂ´s LVPLS package has considerably increased (PLS-Graph, PLS-GUI, SPAD-PLS, SmartPLS). To help the user to make an informed decision, the existing programs are reviewed; their strengths and weaknesses are identified. Furthermore, analyzing simulated data reveals that the signs of weights/factor loadings and path coefficients can vary considerably across the different programs. Thus, applied researchers should treat the interpretation of their results with caution. Compared to programs for analysis of covariance structure models (LISREL approach), PLS path modeling software is on equal footing regarding ease of use, but clearly lags behind in terms of methodological capabilities."
"149";149;"2006-085";"Relational Contracts and Inequity Aversion";"Jenny Kragl and  Julia Schmid";"A4A6";"2006-12-18";"D63,  D82,  M52,  M54";" We study the effects of envy on the feasibility of relational  contracts in a standard moral hazard setup with two agents. Performance  is evaluated via an observable, but non-contractible signal which reflects  the agentÂ´s individual contribution to firm value. Both agents exhibit  disadvantageous inequity aversion. In  contrast to the literature, we  find that inequity aversion may be  beneficial: In the presence of envy,  for a certain range of interest rates relational contracts may be more  profitable. Furthermore, for some interest rates reputational equilibria  exist only with inequity averse  agents."
"150";150;"2006-086";"Overreaction and Multiple Tail Dependence at the High-frequency Level — The Copula Rose";"Wing Lon Ng";"D";"2006-12-18";"C14,  C22,  G14";" This paper applies a non- and a semiparametric copula-based approach to analyze the first-order autocorrelation of returns in high frequency financial time series. Using the EUREX D3047 tick data from the German stock index, it can be shown that the temporal dependence structure of price movements is not always negatively correlated as assumed in the stylized facts in the finance literature. Depending on the sampling frequency, the estimated copulas exhibit some kind of overreaction phenomena and multiple tail dependence, revealing patterns similar to the compass rose. "
"151";151;"2006-087";"What kind of shock was it? Regional Integration and Structural Change in Germany after Unification";"Michael C. Burda";"C7";"2006-12-21";"F2,  J61,  P23";" Eastern GermanyÂ´s recovery from the ""unification shock"" has been characterized by deep structural change Â– with apparent repercussions for the West as well Â– and an integration process involving both capital deepening (extensive and intensive investment) and labor thinning (net out-migration). I propose a constant-returns neoclassical model of economic integration which can account for these facts. Adjustment costs determine dynamics and steady state regional distribution of production factors. The model also explains persistent wage and capital rate-of-return differentials along the equilibrium path. Under competitive conditions, observed factor price differentials contain information on those adjustment costs."
"152";152;"2006-088";"Imitation with Intention and Memory: an Experiment";"Astrid Matthey";"A6";"2006-12-21";"D01,  D83";" Three results emerge from a simple experiment on imitation. First, I find behavior which strongly suggests an intention to imitate. Second, players imitate successful other players rather than repeating successful actions. Third, to find imitation examples, players use several periods of memory. This lends support to learning models with a non-trivial role of memory. The experiment analyzes imitation in an individual learning context. It supplements the results obtained for imitation in evolutionary processes."
"153";153;"2006-089";"Biases in Estimates of the Smoking Wage Penalty";"Silke Anger and  Michael Kvasnicka";"C7";"2010-05-15";"J31,  I19,  C51";" Empirical studies on the earnings effects of tobacco use have found significant wage penalties attached to smoking. We produce evidence that suggests that these estimates are significantly upward biased. The bias arises from a general failure in the literature to control for the past smoking behavior of individuals. 2SLS earnings estimates show that the smoking wage penalty is reduced by as much as a third, if past smoking of individuals is controlled for. Our results also point to significant wage gains for individuals that quit smoking, a finding that is of substantial interest, given the lack of evidence on the earnings effects of smoking cessation."
"154";154;"2007-001";"Trade Liberalisation, Process and Product Innovation, and Relative Skill Demand";"Sebastian Braun";"C7";"2007-01-05";"F12,  F15,  F16,  O32";" The interaction between trade liberalisation, product and process innovation, and relative skill demand is analysed in a model of international oligopoly. Lower trading barriers increase the degree of foreign competition. The competing enterprises respond by investing more aggressively  in lowering marginal costs of production. Moreover, firms reduce the substitutability of their products through additional investment in product innovation. The paper also shows that the relative demand for skilled workers may increase as a result."
"155";155;"2007-002";"Robust Risk Management. Accounting for Nonstationarity and Heavy Tails";"Ying Chen and  Vladimir Spokoiny";"B1B5";"2007-01-16";"C14,  C53";" In the ideal Black-Scholes world, financial time series are assumed 1) stationary (time homogeneous) and 2) having conditionally normal distribution given the past.  These two assumptions have been widely-used in many methods such as the RiskMetrics, one risk management method considered as industry standard. However these assumptions are unrealistic.  The primary aim of the paper is to account for nonstationarity and heavy tails in time series by presenting a local exponential smoothing approach,  by which the smoothing parameter is adaptively selected at every time point and the heavy-tailedness of the process is considered. A complete theory addresses both issues. In our study, we demonstrate the implementation of the proposed method in volatility estimation and risk management given simulated and real data. Numerical results show the proposed method delivers accurate and sensitive estimates."
"156";156;"2007-003";"Explaining Asset Prices with External Habits and Wage Rigidities in a DSGE Model.";"Harald Uhlig";"C1";"2007-01-23";"E24,  E30,  G12";" In this paper, I investigate the scope of a  model with exogenous habit formation -  or `catching up with the Joneses`, see Abel (1990) - to generate the observed equity premium as  well as other key macroeconomic facts. Along the way, I derive restrictions for four out of eight  parameters for a rather general preference specification of habit formation by imposing consistency  with long-run growth, the leisure share, the aggregate Frisch elasticity of labor supply, the  observed risk-free rate, and the observed Sharpe ratio. I show that a DSGE model with  (exogenous and lagged) habits in both leisure and consumption, but not necessarily with additional  persistence, is well capable of matching the observed asset market facts as well as macro facts,  provided one allows for moderate real wage stickiness and provided one allows for sufficient curvature  on preferences, as dictated by the asset market observations. Without wage stickiness, delivery on both  the asset pricing implications as well as the macroeconomic implications seems to be much harder."
"157";157;"2007-003a";"Explaining Asset Prices with External Habits and Wage Rigidities in a DSGE Model.";"Harald Uhlig";"C1";"2007-05-25";"E24,  E30,  G12";" In this paper, I investigate the scope of a  model with exogenous habit formation -  or `catching up with the Joneses`, see Abel (1990) - to generate the observed equity premium as  well as other key macroeconomic facts. Along the way, I derive restrictions for four out of eight  parameters for a rather general preference specification of habit formation by imposing consistency  with long-run growth, the leisure share, the aggregate Frisch elasticity of labor supply, the  observed risk-free rate, and the observed Sharpe ratio. I show that a DSGE model with  (exogenous and lagged) habits in both leisure and consumption, but not necessarily with additional  persistence, is well capable of matching the observed asset market facts as well as macro facts,  provided one allows for moderate real wage stickiness and provided one allows for sufficient curvature  on preferences, as dictated by the asset market observations. Without wage stickiness, delivery on both  the asset pricing implications as well as the macroeconomic implications seems to be much harder."
"158";158;"2007-004";"Volatility and Causality in Asia Pacific Financial Markets";"Enzo Weber";"C6";"2007-01-30";"C32,  G15";" The present paper analyses interactions between the foreign exchange, money and stock markets in Asian Pacific countries from 1999 till 2006. Considering influences on financial market volatility, the estimations are carried out in multivariate EGARCH models using structural residuals. This approach consequently allows the identification of the contemporaneous effects between the variables. Structural VARs or VECMs can therefore give answers to questions of exchange rate stabilisation, monetary policy behaviour or equity market reagibility. Additionally, a correlation analysis of the identified innovations reveals the degree of coherence in the Asian Pacific region."
"159";159;"2007-005";"Quantile Sieve Estimates For Time Series";"Jürgen Franke, Jean-Pierre Stockis and  Joseph Tadjuidje";"D";"2007-02-05";"C14,  C45";" We consider the problem of estimating the conditional quantile of a time series at time t given observations of the same and perhaps other time series available at time t âˆ’ 1. We discuss sieve estimates which are a nonparametric versions of the Koenker-Bassett regression quantiles and do not require the specification of the innovation law. We prove consistency of those estimates and illustrate their good performance for light- and heavy-tailed distributions of the innovations with a small simulation study. As an economic application, we use the estimates for calculating the value at risk of some stock price series."
"160";160;"2007-006";"Real Origins of the Great Depression: Monopolistic Competition, Union Power, and the American Business Cycle in the 1920s";"Monique Ebell and  Albrecht Ritschl";"C5";"2007-02-05";"E24,  E27,  J51,  J64,  N12,  ";" Most treatments of the Great Depression have focused on its onset and its aftermath. In contrast, we take a unified view of the interwar period. We look at the slide into and the emergence from the 1920-21 recession and the roaring 1920s boom, as well as the slide into the Great Depression after 1929, and attempt to explain these phenomena in a unified framework. The model framework combines monopolistic product market competition with search frictions and bargaining in the labor market, allowing for both individual and collective (unionized) wage bargaining. We attribute the extraordinary macroeconomic and financial volatility of this period to two factors: Shifts in the wage bargaining regime and in the degree of monopoly power in the economy. The pro-union provisions of the Clayton Act of 1914 contributed to the slide in asset prices and the depression of 1920-21, while a series of tough anti-union Supreme Court decisions in late 1921 and 1922 coupled with the lax anti-trust enforcement of the Coolidge and Hoover administrations enabled a major rise in corporate profits and stock market valuations throughout the 1920s. Landmark court decisions in favor of trade unions in the late 1920s, as well as political pressure on firms to adopt the welfare capitalism model of high wages, made the economy increasingly susceptible to collapsing profit expectations. We model the onset of the great depression as an equilibrium switch from individual wage bargaining to (actual or mimicked) collective wage bargaining. The general equilibrium effects of this regime change are consistent with large decreases in output, employment, and stock prices."
"161";161;"2007-007";"Rules, Discretion or Reputation? Monetary Policies and the Efficiency of Financial Markets in Germany, 14th to 16th Centuries";"Oliver Volckart";"C5";"2007-02-08";"G15,  N13,  N23,  N43";" This paper examines the questions of whether and how feudal rulers were able to credibly commit to preserving monetary stability, and of which consequences their decisions had for the efficiency of financial markets. The study reveals that princes were usually only able to commit to issuing a stable coinage in gold, but not in silver. As for silver currencies, the hypothesis is that transferring the right of coinage to an autonomous city was the functional equivalent to establishing an independent central bank. An analysis of market performance indicates that financial markets between cities that were autonomous with regard to their monetary policies were significantly better integrated and more efficient than markets between cities whose currencies were supplied by a feudal ruler."
"162";162;"2007-008";"Sectoral Transformation, Turbulence, and Labour Market Dynamics in Germany";"Ronald Bachmann and  Michael C. Burda";"C7";"2007-02-19";"J63,  J64,  J62";" The secular rise of European unemployment since the 1960s is hard to explain without reference to structural change. This is especially true in Germany, where industrial employment has declined by more than 30% and service sector employment has more than doubled over the past three decades. Using individual transition data on West German workers, we document a marked increase in structural change and turbulence, in particular since 1990. Net employment changes resulted partly from an increase in gross flows, but also from an increase in the net transition Â´yieldÂ´ at any given gross worker turnover. In growing sectors, net structural change was driven by accessions from nonparticipation  rather than unemployment; contracting sectors reduced their net employment primarily via lower accessions from nonparticipation. While gross turnover is cyclically sensitive and strongly procyclical, net reallocation is countercyclical, meaning that recessions are associated with increased intensity of sectoral reallocation. Beyond this cyclical component, German reunification and Eastern enlargement appear to have contributed significantly to this accelerated pace of structural change."
"163";163;"2007-009";"Union Wage Compression in a Right-to-Manage Model";"Thorsten Vogel";"C7";"2007-02-19";"J51,  J31,  J41,  J21";" Trade unions are consistently found to compress the wage distribution. Moreover, unemployment affects in particular low-skilled workers. The present paper argues that an extended Right-to-Manage model can account for both of these findings. In this model unions compress the wage distribution by raising wages of workers in low productivity industries (or low-skilled workers) above market clearing levels. Our analysis suggests that the most direct way to test this model would be via a test for stochastic dominance. We also allow for capital adjustments and compare union and non-union wage distributions in a general equilibrium framework."
"164";164;"2007-010";"On s-additive robust representation of convex risk measures for unbounded financial positions in the presence of uncertainty about the market model";"Volker Krätschmer";"A3";"2007-03-01";"G10";" Recently, Frittelli and Scandolo ([9]) extend the notion of risk measures,  originally introduced by Artzner, Delbaen, Eber and Heath ([1]), to the risk assessment  of abstract financial positions, including pay offs spread over different dates, where  liquid derivatives are admitted to serve as financial instruments. The paper deals with Ïƒâˆ’additive robust representations of convex risk measures in the extended sense, dropping the assumption of an existing market model, and allowing also unbounded financial positions. The results may be applied for the case that a market model is available, and they encompass as well as improve criteria obtained for robust representations of the original convex risk measures for bounded positions ([4], [7], [16])."
"165";165;"2007-011";"Media Coverage and Macroeconomic Information Processing";"Alexandra Niessen";"D";"2007-03-07";"G23,  E44,  G14";" This paper investigates how media coverage influences macroeconomic information processing at the bond market. I provide evidence that a high media coverage of an economic topic increases investor attention prior to the release of the corresponding economic indicator: High media coverage of the business cycle leads to a stronger market reaction to the release of gross domestic product, industrial production and IFO business index than low media coverage. High media coverage of the price level increases the market reaction to the release of producer and consumer price index than low media coverage. High media coverage of unemployment leads to a stronger market reaction to the release of the unemployment rate than low media coverage."
"166";166;"2007-012";"Are Correlations Constant Over Time? Application of the CC-TRIGt-test to Return Series from Different Asset Classes.";"Matthias Fischer";"D";"2007-03-15";"C22,  C32,  G12";" A new test for constant correlation is proposed. Based on the bivariate Student-t distribution, this test is derived as Lagrange multiplier (LM) test. Whereas most of the traditional tests (e.g. Jennrich, 1970, Tang, 1995 and Goetzmann, Li & Rouwenhorst, 2005) specify the unknown correlations as piecewise constant, our model-setup for the correlation coefficient is based on trigonometric functions. Applying this test to assets from different financial markets (stocks, exchange rates, metals) there is empirical evidence that many of the correlations vary over time."
"167";167;"2007-013";"Uncertain Paternity, Mating Market Failure, and the Institution of Marriage";"Dirk Bethmann and  Michael Kvasnicka";"C7";"2007-03-20";"D10,  J12,  J13,  D02";" This paper provides a first microeconomic foundation for the institution of  marriage. Based on a model of reproduction, mating, and parental investment in  children, we argue that marriage serves the purpose of attenuating the risk of mating market failure that arises from incomplete information on individual paternity. Raising  the costs of mating to individuals, marriage circumscribes female infidelity and mate poaching among men, which reduces average levels of paternal uncertainty in society. A direct gain in male utility, the latter induces men to invest more in their putative offspring, a fact that benefits women because of the public good nature of children. Able to realize Pareto improvements, marriage as an institution is hence explained as the result of a societal consensus on the need to organize and structure mating behavior and reproduction in society for the benefit of paternal certainty and biparental investment in offspring."
"168";168;"2007-014";"What Happened to the Transatlantic Capital Market Relations?";"Enzo Weber";"C6";"2007-03-26";"E44,  F31,  C32";" This paper investigates the capital market relations between Euroland and the USA from 1990 until 2006. Formally based on the uncovered interest rate parity (UIP), backward recursive estimations establish a long-run equilibrium between European and US government bond yields. Since the mid-1990s though, cointegration can only be achieved additionally considering the exchange rate. The reason proves a stochastic trend common to the European interest and the exchange rate, consistently explained by central bank reactions and unfinished learning processes on the role of the euro. Furthermore, the US capital market dominance is strongly reduced, leading to transatlantic interdependence at eye level."
"169";169;"2007-015";"Who Leads Financial Markets?";"Enzo Weber";"C6";"2007-04-04";"C32,  G15";" The present paper embarks on an analysis of interactions between the US and Euroland in the capital, foreign exchange, money and stock markets from 1994 until 2006. Considering influences on financial market volatility, the estimations are carried out in multivariate EGARCH models using structural residuals. This approach consequently allows identifying the contemporaneous effects between the daily variables. Structural VARs or VECMs can therefore give answers to the question of financial markets leadership: Generally speaking, the US effects on Europe still dominate, but the special econometric methodology is able to uncover otherwise neglected effects in the reverse direction."
"170";170;"2007-016";"Fiscal Policy Rules in Practice";"Andreas Thams";"C6";"2007-04-04";"E62,  E63,  E65";" This paper analyzes German and Spanish fiscal policy using simple policy rules. We choose Germany and Spain, as both are Member States in the European Monetary Union (EMU) and underwent considerable increases in public debt in the early 1990s. We focus on the question, how fiscal policy behaves under rising public debt ratios. It is found that both Germany and Spain generally exhibit a positive relationship between government revenues and debt. Using Markov-switching techniques, we show that both countries underwent a change in policy behavior in the light of rising debt/output ratios at the end of the 1990s. Interestingly, this change in policy behavior differs in its characteristics across the two countries and seems to be non-permanent in the case of Germany."
"171";171;"2007-017";"Empirical Pricing Kernels and Investor Preferences";"Kai Detlefsen, Wolfgang Härdle and  Rouslan Moro";"B1";"2007-04-04";"G12,  G13,  C50";" This paper analyzes empirical market utility functions and pricing kernels derived from the DAX and DAX option data for three market regimes. A consistent parametric framework of stochastic volatility is used. All empirical market utility functions show a region of risk proclivity that is reproduced by adopting the hypothesis of heterogeneous individual investors whose utility functions have a switching point between bullish and bearish attitudes. The inverse problem of finding the distribution of individual switching points is formulated in the space of stock returns by discretization as a quadratic optimization problem. The resulting distributions vary over time and correspond to different market regimes."
"172";172;"2007-018";"Simultaneous Causality in International Trade";"Enzo Weber";"C6";"2007-04-18";"F10,  C32";" This paper proposes estimating causalities in bilateral international trade in simultaneous systems, including domestic and foreign GDP as well as mutual trade flows. Conventional macroeconomic theory mainly follows partial approaches like import functions or export-led growth. Focusing on the US relations with Euroland and Canada, cointegration analyses however reveal, that the system dynamics, and so both im- and exports, are simply governed by US GDP shocks. In conclusion, exploring sources and effects of international trade should be seen as an inherently empirical task."
"173";173;"2007-019";"Regional and Outward Economic Integration in South-East Asia";"Enzo Weber";"C6";"2007-04-18";"E32,  F15,  C32";" The subject of this paper tackles questions of macroeconomic integration of the  South-East Asian countries South Korea, Singapore and Taiwan. Economically, the analysis is based on notions of stochastic long-run convergence and business cycle synchrony in the GDPs. According tests for cointegration and common serial correlation features reveal a high degree of coherence in long-run growth and medium-run fluctuations. This allows extracting a common stochastic growth trend and a common business cycle. Further analysis shows, both of these components are subject to stronger influences from the US than from Japan. Convergence towards these matured economies conspicuously appears since the 1990s."
"174";174;"2007-020";"Computational Statistics and Data Visualization";"Antony Unwin, Chun-houh Chen and  Wolfgang Härdle";"B1";"2007-04-19";"C00,  C19,  C40,  C81,  C87";" This book is the third volume of the Handbook of Computational Statistics and covers the field of Data Visualization. In line with the companion volumes, it contains a collection of chapters by experts in the field to present readers with an up-to-date and comprehensive overview of the state of the art. Data Visualization is an active area of application and research and this is a good time to gather together a summary of current knowledge. Graphic displays are often very effective at communicating information. They are also very often not effective at communicating information. Two important reasons for this state of affairs are that graphics can be produced with a few clicks of the mouse without any thought, and that the design of graphics is not taken seriously in many scientific textbooks. Some people seem to think that preparing good graphics is just a matter of common sense (in which case their common sense cannot be in good shape) and others believe that preparing graphics is a low-level task, not appropriate for scientific attention. This volume of the Handbook of Computational Statistics takes graphics for Data Visualization seriously."
"175";175;"2007-021";"Ideology Without Ideologists";"Lydia Mechtenberg";"A6";"2007-04-23";"D72,  D78,  D82";" Generally, Democrats do not increase military spending, and Republicans do not raise welfare payments. Mostly, ruling politicians stick to the manifesto of their party. The current paper provides a theoretical explanation for this phenomenon that does not assume politicians or voters to be ideologists. I explore an environment where both voters and politicians always prefer the policy that is adequate to the world state but contradicts the party manifesto over the policy that is in line with the manifesto but not adequate. I find that nevertheless, the inefficient manifesto-driven policy will often result from their interaction. Besides, I show that a high degree of agreement between the politician in office, his party basis and the voter makes efficient, informed policy rare or even impossible. But if homogeneity of convictions within parties is high, swing voter behavior can solve the problem."
"176";176;"2007-022";"A Generalized ARFIMA Process with Markov-Switching Fractional Differencing Parameter";"Wen-Jen Tsay and  Wolfgang Härdle";"B1";"2007-04-25";"C14,  C22,  C32,  C52,  C53,  ";" We propose a general class of Markov-switching-ARFIMA processes in order to  combine strands of long memory and Markov-switching literature. Although the coverage of this class of models is broad, we show that these models can be easily estimated with the DLV algorithm proposed. This algorithm combines the Durbin-Levinson and Viterbi procedures. A Monte Carlo experiment reveals that the finite sample performance  of the proposed algorithm for a simple mixture model of Markov-switching mean and ARFIMA(1, d, 1) process is satisfactory. We apply the Markov-switching-ARFIMA models to the U.S. real interest rates, the Nile river level, and the U.S. unemployment rates, respectively. The results are all highly consistent with the conjectures made or empirical results found in the literature. Particularly, we confirm the conjecture in  Beran and Terrin (1996) that the observations 1 to about 100 of the Nile river data seem to be more independent than the subsequent observations, and the value of differencing parameter is lower for the first 100 observations than for the subsequent data."
"177";177;"2007-023";"Time Series Modelling with Semiparametric Factor Dynamics";"Szymon Borak, Wolfgang Härdle, Enno Mammen and  Byeong U. Park";"B1";"2007-04-26";"C14,  C32,  G12";" High-dimensional regression problems which reveal dynamic behavior are typically analyzed by time propagation of a few number of factors. The inference on the whole system is then based on the low-dimensional time series analysis. Such highdimensional problems occur frequently in many different fields of science. In this paper we address the problem of inference when the factors and factor loadings are estimated by semiparametric methods. This more flexible modelling approach poses an important question: Is it justified, from inferential point of view, to base statistical inference on the estimated times series factors? We show that the difference of the inference based on the estimated time series and 'true' unobserved time series is asymptotically negligible. Our results justify fitting vector autoregressive processes to the estimated factors, which allows one to study the dynamics of the whole high-dimensional system with a low-dimensional representation. We illustrate the theory with a simulation study. Also, we apply the method to a study of the dynamic behavior of implied volatilities and discuss other possible applications in finance and economics."
"178";178;"2007-024";"From Animal Baits to Investors’ Preference: Estimating and Demixing of the Weight Function in Semiparametric Models for Biased Samples";"Ya'acov Ritov and  Wolfgang Härdle";"B1";"2007-05-03";"C10,  C14,  D01,  D81";" We consider two semiparametric models for the weight function in a biased sample model. The object of our interest parametrizes the weight function, and it is either Euclidean or non Euclidean. One of the models discussed in this paper is motivated by the estimation the mixing distribution of individual utility functions in the DAX market."
"179";179;"2007-025";"Statistics of Risk Aversion";"Enzo Giacomini and  Wolfgang Härdle";"B1";"2007-05-03";"C14,  G13";" Information about risk preferences from investors is essential for modelling a wide range of quantitative finance applications. Valuable information related to preferences can be extracted from option prices through pricing kernels. In this paper, pricing kernels and their term structure are estimated in a time varying approach from DAX and ODAX data using dynamic semiparametric factor model (DSFM). DSFM smooths in time and space simultaneously, approximating complex dynamic structures by basis functions and a time series of loading coefficients. Contradicting standard risk aversion assumptions, the estimated pricing kernels indicate risk proclivity in certain levels of return. The analysis of the time series of loading coefficients allows a better understanding of the dynamic behaviour from investors preferences towards risk."
"180";180;"2007-026";"Robust Optimal Control for a Consumption-investment Problem";"Alexander Schied";"A3";"2007-05-18";"G11,  D81";" We give an explicit PDE characterization for the solution of the problem of maximizing the utility of both terminal wealth and intertemporal consumption under model uncertainty. The underlying market model consists of a risky asset, whose volatility and long-term trend are driven by an external stochastic factor process. The robust utility functional is defined in terms of a HARA utility function with risk aversion parameter 0  Keywords: Optimal Consumption, Robust Control, Model Uncertainty, Incomplete Markets,  Stochastic Volatility, Coherent Risk Measures, Convex Duality."
"181";181;"2007-027";"Long Memory Persistence in the Factor of Implied Volatility Dynamics";"Wolfgang Härdle and  Julius Mungo";"B1";"2007-05-18";"C14,  C32,  C52,  C53,  G12";" The volatility implied by observed market prices as a function of the strike and time to maturity form an Implied Volatility Surface (IV S). Practical applications require reducing the dimension and characterize its dynamics through a small number of factors. Such dimension reduction is summarized by a Dynamic Semiparametric Factor Model (DSFM) that characterizes the IV S itself and their movements across time by a multivariate time series of factor loadings. This paper focuses on investigating long range dependence in the factor loadings series. Our result reveals that shocks to volatility persist for a very long time, affecting significantly stock prices. For appropriate representation of the series dynamics and the possibility of improved forecasting, we model the long memory in levels and absolute returns using the class of fractional integrated volatility models that provide flexible structure to capture the slow decaying autocorrelation function reasonably well."
"182";182;"2007-028";"Macroeconomic Policy in a Heterogeneous Monetary Union";"Oliver Grimm and  Stefan Ried";"C1";"2007-05-18";"E52,  E61,  F42";" We use a two-country model with a central bank maximizing union-wide welfare and two fiscal authorities minimizing comparable, but slightly different country-wide losses. We analyze the rivalry between the three authorities in seven static games. Comparing a homogeneous with a heterogeneous monetary union, we find welfare losses to be significantly larger in the heterogeneous union. The best-performing scenarios are cooperation between all authorities and monetary leadership. Cooperation between the fiscal authorities is harmful to both the whole unionÂ’s and the country-specific welfare."
"183";183;"2007-029";"Comparison of Panel Cointegration Tests";"Deniz Dilan Karaman Örsal";"C2";"2007-05-18";"C23,  C33,  C15";" The main aim of this paper is to compare the size and size-adjusted power  properties of four residual-based and one maximum-likelihood-based panel cointegration tests with the help of Monte Carlo simulations. In this study the panel-p, the group-p,  the panel-t, the group-t statistics of Pedroni (1999) and the standardized LR-bar statistic of Larsson et al. (2001) are considered. The simulation results indicate that the panel-t and standardized LR-bar statistic have the best size and power properties a mong the five panel cointegration test statistics evaluated. Finally, the Fisher Hypothesis is tested with two different data sets for OECD countries. The results point out the existence of the Fisher relation."
"184";184;"2007-030";"Robust Maximization of Consumption with Logarithmic Utility";"Daniel Hernández-Hernández and  Alexander Schied";"A3";"2007-05-25";"G11,  D81";" We analyze the stochastic control approach to the dynamic maximization of the robust utility of consumption and investment. The robust utility functionals are defined in terms of logarithmic utility and a dynamically consistent convex risk measure. The underlying market is modeled by a diffusion process whose coefficients are driven by an external stochastic factor process. Our main results give conditions on the minimal penalty function of the robust utility functional under which the value function of our problem can be identified with the unique classical solution of a quasilinear PDE within a class of functions satisfying certain growth conditions."
"185";185;"2007-031";"Using Wiki to Build an E-learning System in Statistics in Arabic Language";"Taleb Ahmad, Wolfgang Härdle and  Sigbert Klinke";"B1";"2007-05-25";"I21,  C19";" E-learning plays an important role in education as it supports online education via computer networks and provides educational services by utilising information technologies. We present a case study describing the development of an Arabic language elearning course in statistics. Discussed are issues concerning e-learning in Arab countries with special focus on problems of the application of e-learning in the Arab world and the difficulties concerning the design Arabic platforms such as language problems, cultural and technical problems, especially ArabTeX works difficulty with LaTeX format. Thus Wiki is offered as a solution to such problems. Wiki supports LaTeX and other statistical programs, for instance R, and^Wiki offers the solution to language problems. Details of this technology are discussed and a solution as to how this system can serve in building an Arab platform is presented."
"186";186;"2007-032";"Visualization of Competitive Market Structure by Means of Choice Data";"Werner Kunz";"B2";"2007-05-25";"D49";" This paper presents a method for visualizing competitive market structures based on scanner panel data where asymmetries are taken into account. For this, I combined consumer choice models based on mixed logit models with three-mode principal component analysis. This approach can be used to unfold a competitive market structure map. The methodology presented is able to quantify the clout and receptivity of various brands. The results can then be visualized over time. Using this approach, guidelines for promotional activities of new brands can be provided, and possible threats from the competition detected."
"187";187;"2007-033";"Does International Outsourcing Depress Union Wages?";"Sebastian Braun and  Juliane Scheffel";"C7";"2007-05-25";"F16,  J51,  L24";" In this paper, we provide first empirical evidence on the effect of outsourcing on union wages using linked employer-employee data for Germany. We find that low skilled workers experience a decline in the union wage premium when working in industries with high outsourcing intensities. The finding applies to both firm- and sector-level agreements. Hence, outsourcing appears to deteriorate the bargaining position of unions. Outsourcing is not found to have a negative effect on the wages of low skilled employees not covered by collective bargaining agreements. While wages of medium skilled workers are largely unaffected by outsourcing, high skilled workers see their wages rise in industries with a high level of outsourcing. There is no interaction between coverage and outsourcing for these skill groups."
"188";188;"2007-034";"A Note on the Effect of Outsourcing on Union Wages";"Sebastian Braun and  Juliane Scheffel";"C7";"2007-05-25";"";" We analyze the effect of outsourcing on union wages in a simple two-stage game between a firm and a union. In contrast to public perception the ease with which the firm can outsource parts of their production does not necessarily reduce the wage set by the union. Even in the simple model framework a surprisingly large number of conflicting effects is established."
"189";189;"2007-035";"Estimating Probabilities of Default With Support Vector Machines";"Wolfgang Härdle, Rouslan Moro and  Dorothea Schäfer";"B1";"2007-06-01";"C14,  G33,  C45";" This paper proposes a rating methodology that is based on a non-linear classification method, the support vector machine, and a non-parametric technique for mapping rating scores into probabilities of default. We give an introduction to underlying statistical models and represent the results of testing our approach on German Bundesbank data. In particular we discuss the selection of variables and give a comparison with more traditional approaches such as discriminant analysis and the logit regression. The results demonstrate that the SVM has clear advantages over these methods for all variables tested."
"190";190;"2007-036";"Yxilon – A Client/Server Based Statistical Environment";"Wolfgang Härdle, Sigbert Klinke and  Uwe Ziegenhagen";"B1";"2007-06-01";"C88";" Along with many others, we agree that a modern education in statistics needs to incorporate the practical analysis of real datasets, which are usually more complex than the common examples found in standard textbooks. The software used in the teaching of statistics includes standard spreadsheet environments such as OpenOffice and Excel and dedicated commercial and non-commercial packages such as R, Minitab or SPSS. With the freely available Yxilon environment we add another package and proliferate the statistical programming language XploRe, using a modern client/server based architecture. This architecture has the capabilities of serving statistical results in a variety of flavors for different groups of users. In this paper we describe the general setup of the Yxilon environment and present selected technical details."
"191";191;"2007-037";"Calibrating CAT bonds for Mexican earthquakes";"Wolfgang Härdle and  Brenda López Cabrera";"B1";"2007-06-22";"G19,  G29,  N26,  N56,  Q29,  ";" The study of natural catastrophe models plays an important role in the prevention and mitigation of disasters. After the occurrence of a natural disaster, the reconstruction can be financed with catastrophe bonds (CAT bonds) or reinsurance. This paper examines the calibration of a real parametric CAT bond for earthquakes that was sponsored by the Mexican government. The calibration of the CAT bond is based on the estimation of the intensity rate that describes the earthquake process from the two sides of the contract, the reinsurance and the capital markets, and from the historical data. The results demonstrate that, under specific conditions, the financial strategy of the government, a mix of reinsurance and CAT bond, is optimal in the sense that it provides coverage of USD 450 million for a lower cost than the reinsurance itself. Since other variables can affect the value of the losses caused by earthquakes, e.g. magnitude, depth, city impact, etc., we also derive the price of a hypothetical modeled-index (zero) coupon CAT bond for earthquakes, which is based on a compound doubly stochastic Poisson pricing methodology. In essence, this hybrid trigger combines modeled loss and index trigger types, trying to reduce basis risk borne by the sponsor while still preserving a non-indemnity trigger mechanism. Our results indicate that the (zero) coupon CAT bond price increases as the threshold level increases, but decreases as the expiration time increases. Due to the quality of the data, the results show that the expected loss is considerably more important for the valuation of the CAT bond than the entire distribution of losses."
"192";192;"2007-038";"Economic Integration and the Foreign Exchange";"Enzo Weber";"C6";"2007-07-06";"F31,  F41,  C32";" This paper demonstrates effects of economic convergence processes on the foreign exchange behaviour in a monetary modelling approach. Since the exchange rate represents the relative price of two currencies, commonness of stochastic trends between the fundamental determinants of supply and demand of the underlying monies restricts exchange rate movements to transitory fluctuations. In the spirit of optimal currency areas, this has the potential to serve as a criterion for an all-round integration of two economies. Empirically, such a constellation is found between Australia and New Zealand, whereas diverging trends in money and interest rates characterise the relation of Australia towards the US."
"193";193;"2007-039";"Tracking Down the Business Cycle: A Dynamic Factor Model For Germany 1820-1913";"Samad Sarferaz and  Martin Uebele";"C5";"2007-07-06";"E32,  C11,  C32,  N13";" We use a Bayesian dynamic factor model to measure GermanyÂ’s pre World War I economic activity. The procedure makes better use of existing time series data than historical national accounting. To investigate industrialization we propose to look at comovement between sectors. We find that GermanyÂ’s industrial sector developed earlier than stated in the literature, since after the 1860s agricultural time series do not comove with the business cycle anymore. Also, the bulk of comovement between 1820 and 1913 can be traced back to five out of 18 series representing industrial production, investment and demand for industrial inputs. Our factor is impressingly confirmed by a stock price index, leading the factor by 1-2 years. We also find evidence for early market integration in the 1820s and 1830s. Our business cycle dating aims to resolve the debate on German business cycle history. Given the often unsatisfactory quality of national accounting data for the 19th century we show the advantage of dynamic factor models in making efficient use of rare historical time series."
"194";194;"2007-040";"Optimal Policy Under Model Uncertainty: A Structural-Bayesian Estimation Approach";"Alexander Kriwoluzky and  Christian Stoltenberg";"C1";"2007-07-13";"E32,  C51,  E52.";" In this paper we propose a novel methodology to analyze optimal policies under model uncertainty in micro-founded macroeconomic models. As an application we assess the relevant sources of uncertainty for the optimal conduct of monetary policy within (parameter uncertainty) and across models (specification uncertainty) using EU 13 data. Parameter uncertainty matters only if the zero bound on interest rates is explicitly taken into account. In any case, optimal monetary policy is highly sensitive with respect to specification uncertainty implying substantial welfare gains of a robustly-optimal rule that incorporates this risk."
"195";195;"2007-041";"QuantNet – A Database-Driven Online Repository of Scientific Information";"Anton Andriyashin and  Wolfgang Härdle";"B1";"2007-07-13";"C88,  C89.";" In this study a framework for an online database-driven repository of information Â– QuantNet Â– is presented. QuantNet is aimed at easing the process of web publishing for those who are unfamiliar with technical details and markup languages. At the same time advanced users are provided with easy user style markup tools while flexible and trouble-free application administration is being a top priority. In this realm a special emphasis is put on the construction of a metalanguage containing only simplest possible structures. Different stages Â– from low-level text processing via Atox to the transformation of XML documents via XSLT, PHP and mySQL Â– are thoroughly described. The motivation for further possible application extensions like DTD or preliminary document check, based on analytic grammar form, is provided."
"196";196;"2007-042";"Exchange Rate Uncertainty and Trade Growth - A Comparison of Linear and Nonlinear (Forecasting) Models";"Helmut Herwartz and  Henning Weber";"C3";"2007-07-13";"F14,  F17";" A huge body of empirical and theoretical literature has emerged on the relationship between foreign exchange (FX) uncertainty and international trade. Empirical findings about the impact of FX uncertainty on trade figures are at best weak and often ambiguous with respect to its direction. Almost all empirical contributions assume and estimate a linear relationship. Possible nonlinearity or state dependence of causal links between FX uncertainty and trade has been mostly ignored yet. In addition, widely used regression models have not been evaluated in terms of ex-ante forecasting. In this paper we analyze the impact of FX uncertainty on sectoral categories of multilateral exports and imports for 15 industrialized economies. We particularly provide a comparison of linear and nonlinear models with respect to ex-ante forecasting. In terms of average ranks of absolute forecast errors nonlinear models outperform both, a common linear model and some specification building on the assumption that FX uncertainty and trade growth are uncorrelated. Our results support the view that the relationship of interest might be nonlinear and, moreover, lacks of homogeneity across countries, economic sectors and when contrasting imports vs. exports."
"197";197;"2007-043";"How do Rating Agencies Score in Predicting Firm Performance";"Gunter Löffler and  Peter N. Posch";"D";"2007-08-01";"C33,  G20,  G33";" We use dynamic panel analysis to examine whether credit rating agencies achieve what they claim to achieve, namely, look into the future when assigning their ratings. We find that Moody's ratings help predict individual financial ratios over a horizon of up to five years. Ratings also predict a multivariate credit score,  again over five years. The contribution of ratings appears to be economically significant and robust for different specifications."
"198";198;"2007-044";"Ein Vergleich des binären Logit-Modells mit künstlichen neuronalen Netzen zur Insolvenzprognose anhand relativer Bilanzkennzahlen";"Ronald Franken";"D";"2007-08-01";"C45,  G32,  G33";" Die Prognose der InsolvenzgefÃ¤hrdung von Unternehmen anhand statistischer  Methodik war und ist eine bedeutende Aufgabe empirischer Forschung. Eine MÃ¶glichkeit der Beurteilung der finanziellen bzw. wirtschaftlichen Verfassung von Unternehmen stellt die sog. externe Bilanzanalyse anhand verschiedener relativer Kennzahlen(-systeme) dar, welche aus den verÃ¶ffentlichten JahresabschlÃ¼ssen von Kapitalgesellschaften  abgeleitet werden kÃ¶nnen. In der aktuellen Praxis der empirischen Insolvenz- und Risikoforschung ist nach wie vor die klassische parametrische Methode  der binÃ¤ren logistischen Regression weit verbreitet. In der jÃ¼ngeren Vergangenheit haben jedoch neue Methoden der statistischen Lerntheorie, die aus den Methoden des maschinellen Lernens hervorgegangen  sind, zunehmend an Bedeutung erlangt. In der vorliegenden Arbeit wird ein kÃ¼nstliches neuronales Netz zur Insolvenzklassifikation anhand relativer Bilanzkennzahlen entwickelt und mit den Ergebnissen der logistischen Regressionsanalyse verglichen und evaluiert."
"199";199;"2007-045";"Promotion Tournaments and Individual Performance Pay";"Anja Schöttner and  Veikko Thiele";"A4";"2007-08-01";"D82,  D86,  M52";" We analyze the optimal combination of promotion tournaments and individual performance pay in an employment relationship. An agent's effort is non-observable and he has private information about his suitability for promotion. We find that the principal does not provide individual incentives if it is sufficiently important to promote the most suitable candidate. Thus, we give a possible explanation for why individual performance schemes are less often observed in practice than predicted by theory. Furthermore, optimally trading off incentive and selection issues causes a form of the Peter Principle: The less suitable agent has an inefficiently high probability of promotion."
"200";200;"2007-046";"Estimation with the Nested Logit Model: Specifications and Software Particularities";"Nadja Silberhorn, Yasemin Boztug and  Lutz Hildebrandt";"B2";"2007-08-01";"C13,  C31,  C87,  M31";" Estimation with the Nested Logit Model: Specifications and Software Particularities Abstract:  The paper discusses the nested logit model for choices between a set of mutually exclusive alternatives (e.g. brand choice, strategy decisions, modes of transportation, etc.). Due to the ability of the nested logit model to allow and account for similarities between pairs of alternatives, the model has become very popular for the empirical  analysis of choice decisions. However the fact that there are two different specifications of the nested logit model (with different outcomes)  has not received adequate attention. The utility maximization nested logit  (UMNL) model and the non-normalized nested logit (NNNL) model have different properties, influencing the estimation results in a different manner. This paper introduces distinct specifications  of the nested logit model and indicates particularities arising from model estimation. The effects of using various software packages on the estimation results of a nested logit model are shown using simulated  data sets for an artificial decision situation."
"201";201;"2007-047";"Risiken infolge von Technologie-Outsourcing?";"Michael Stephan";"D";"2007-08-01";"L23,  L24,  L25,  O32,  O33";" Das vorliegende Diskussionspapier umfasst eine empirische Untersuchung der Auswirkungen Fremdvergabe von WertschÃ¶pfungsleistungen an externe Zulieferer auf die Breite der technologischen Kompetenzbasis von Unternehmen. Die Untersuchung umfasst eine Stichprobe von insgesamt 50 multinationalen Unternehmen aus der Grundgesamtheit der Top-200 F&E betreibenden Unternehmen. Der Untersuchungszeitraum erstreckt sich auf insgesamt 20 Jahre im Zeitraum 1983-2002. Das Ergebnis zeigt, dass sich wÃ¤hrend der letzten 20 Jahre die Breite des Technologieportfolios verstÃ¤rkt von der Entwicklung der WertschÃ¶pfungstiefe entkoppelt hat. Obwohl im Stichprobendurchschnitt die Werte fÃ¼r beide KenngrÃ¶ÃŸen zurÃ¼ckgegangen sind, war der RÃ¼ckgang bei der WertschÃ¶pfungstiefe mit 15 Prozent deutlich stÃ¤rker als der RÃ¼ckgang des Technologiespektrums (-3,03 Prozent). Diese Entkopplung kann in der multiplen Regressionsanalyse bestÃ¤tigt werden: Der Grad der vertikalen Spezialisierung hat keinen signifikanten Einfluss auf die zu erklÃ¤rende Variable. Damit lÃ¤sst sich die im Beitrag aufgeworfene Fragestellung beantworten: Die Fremdvergabe von WertschÃ¶pfungsleistungen an externe Zulieferer geht nicht mit dem Abbau der technologischen Kompetenzbasis einher."
"202";202;"2007-048";"Sensitivities for Bermudan Options by Regression Methods";"Denis Belomestny, Grigori Milstein and  John Schoenmakers";"B7";"2007-08-01";"G12,  G13";" In this article we propose several pathwise and finite difference based methods for calculating sensitivities of Bermudan options using regression methods and Monte Carlo simulation. These methods rely on conditional probabilistic representations which allow, in combination with a regression approach, for efficient simultaneous computation of sensitivities at many initial positions. Assuming that the price of a Bermudan option can be evaluated sufficiently accurate, we develop a method for constructing deltas based on least squares. We finally propose a testing procedure for assessing the performance of the developed methods."
"203";203;"2007-049";"Occupational Choice and the Spirit of Capitalism";"Matthias Doepke and  Fabrizio Zilibotti";"D";"2007-08-02";"J24,  N2,  N3,  O11,  O15,  O4";" The British Industrial Revolution triggered a reversal in the social order whereby the landed elite was replaced by industrial capitalists rising from the middle classes as the economically dominant group. Many observers have linked this transformation to the contrast in values between a hard-working and thrifty middle class and an upper class imbued with disdain for work. We propose an economic theory of preference formation in which both the divergence of attitudes across social classes and the ensuing reversal of economic fortunes are equilibrium outcomes. In our theory, parents shape their childrenÂ’s preferences in response to economic incentives. If financial markets are imperfect, this results in the stratification of society along occupational lines. Middle-class families in occupations that require effort, skill, and experience develop patience and work ethic, whereas upper-class families relying on rental income cultivate a refined taste for leisure. These class-specific attitudes, which are rooted in the nature of pre-industrial professions, become key determinants of success once industrialization transforms the economic landscape."
"204";204;"2007-050";"On the Utility of E-Learning in Statistics";"Wolfgang Härdle, Sigbert Klinke and  Uwe Ziegenhagen";"B1";"2007-08-09";"I21,  C19";" Abstract:  Students of introductory courses consider statistics as particularly difficult, as the understanding of the underlying concepts may require more time and energy than for other disciplines. For decades statisticians have tried to enhance understanding with the help of technical solutions such as animation, video or interactive tools. However it is not clear if the added value generated by these e-learning tools justifies the work invested. In this paper the experience with various e-learning solutions in terms of utility and the impact on teaching is discussed."
"205";205;"2007-051";"Mergers & Acquisitions and Innovation Performance in the Telecommunications Equipment Industry";"Tseveen Gantumur and  Andreas Stephan";"D";"2007-08-22";"L63,  O30,  L10";" In response to global market forces such as deregulation and globalization, technological change and digital convergence, the telecommunications in the 1990s witnessed an enormous worldwide round of Mergers & Acquisitions (M&A). Given both M&A and Innovation a major means of todayÂ’s competitive strategy development, this paper examines the innovation determinants of M&A activity and the consequences of M&A transactions on the technological potential and the innovation performance. We examine the telecommunications equipment industry over the period 1988-2002 using a newly constructed data set with firm-level data on M&A and innovation activity as well as financial characteristics. By implementing a counterfactual technique based on a matching propensity score procedure, the analysis not only controls for merger endogeneity and ex-ante observable firms characteristics but also takes account of unobserved heterogeneity. The study provides evidence that M&A realize significantly positive changes to the firmÂ’s post-merger innovation performance. The effects of M&A on innovation performance are in turn driven by both the success in Research and Development (R&D) activity and the deterioration in internal technological capabilities at acquiring firms prior to a merger."
"206";206;"2007-052";"Capturing Common Components in High-Frequency Financial Time Series: A Multivariate Stochastic Multiplicative Error Model";"Nikolaus Hautsch";"B8";"2007-09-05";"C15,  C32,  C52";" We introduce a multivariate multiplicative error model which is driven by component- specific observation driven dynamics as well as a common latent autoregressive factor. The model is designed to explicitly account for (information driven) common factor dynamics as well as idiosyncratic effects in the processes of high-frequency return volatilities, trade sizes and trading intensities. The model is estimated by simulated maximum likelihood using efficient importance sampling. Analyzing five minutes data from four liquid stocks traded at the New York Stock Exchange, we find that volatilities, volumes and intensities are driven by idiosyncratic dynamics as well as a highly persistent common factor capturing most causal relations and cross-dependencies between the individual variables. This confirms economic theory and suggests more parsimonious specifications of high-dimensional trading processes. It turns out that common shocks affect the return volatility and the trading volume rather than the trading intensity."
"207";207;"2007-053";"World War II, Missing Men, and Out-of-wedlock Childbearing";"Michael Kvasnicka and   Dirk Bethmann";"C7";"2007-09-05";"J12,  J13,  N34";" Based on county-level census data for the German state of Bavaria in 1939 and 1946, we use World War II as a natural experiment to study the effects of sex ratio changes on out-of-wedlock fertility. Our findings show that war-induced shortfalls of men to women significantly increased the nonmarital fertility ratio at mid century, a result that proves robust to the use of alternative sex ratio definitions, post-war measures of fertility, and estimation samples. The magnitude of this increase furthermore appears to depend on the future marriage market prospects that women at the time could expect to face in the not-too-distant future. We find the positive effect on the nonmarital fertility ratio of a decline in the sex ratio to be strongly attenuated by the magnitude of county- level shares of prisoners of war. Unlike military casualties and soldiers missing in action, prisoners of war had a sizeable positive probability of returning home from the war. Both current marriage market conditions, therefore, and foreseeable improvements in the future marriage market prospects of women appear to have influenced fertility behavior in the immediate aftermath of World War II."
"208";208;"2007-054";"The Drivers and Implications of Business Divestiture – An Application and Extension of Prior Findings";"Carolin Decker and  Thomas Mellewigt";"D";"2007-09-05";"G34,  L11,  L25,  M10";" The purpose of this study is to extend the current understanding of business divestiture by investigating its potential for triggering strategic reorientation. A divestiture involving strategic reorientation is here denoted as a strategic business exit, otherwise it is a status quopreserving business exit. The motives for divestiture specified in prior studies are mainly associated with firm financial performance and corporate strategy. Most studies investigate their impact on divestiture separately though both may interact. This study contributes to research by, first, distinguishing divestiture types, and, second, empirically testing the influence of performance and strategy both separately and in conjunction on the choice between strategic and status quo-preserving business exit with secondary data on 213 divestitures during 1999-2004 which were undertaken by a cross-industry sample of 91 firms listed in the German CDAX. The findings mainly indicate that firm financial performance is a stronger predictor of strategic business exit than corporate strategy."
"209";209;"2007-055";"Why Managers Hold Shares of Their Firms: An Empirical Analysis";"Ulf von Lilienfeld-Toal and   Stefan Ruenzi";"D";"2007-09-19";"G12,  G30";" We examine the relationship between CEO ownership and stock market performance. Firms in which the CEO voluntarily holds a considerable share of outstanding stocks outperform the market by more than 10% p.a. after controlling for traditional risk factors. The effect is most pronounced in firms that are characterized by large managerial discretion of the CEO. The abnormal returns we document are one potential explanation why so many CEOs hold a large fraction of their own companyÂ’s stocks. We also examine several potential explanations why the existence of an owner CEO is not fully reflected in prices but leads to abnormal returns."
"210";210;"2007-056";"Auswirkungen der IFRS-Umstellung auf die Risikoprämie von Unternehmensanleihen - Eine empirische Studie für Deutschland, Österreich und die Schweiz";"Kerstin Kiefer and  Philipp Schorn";"D";"2007-09-26";"M41,  G32";" Reduziert eine IFRS-Umstellung die Informationsdefizite der Fremdkapitalgeber und somit auch die RisikoprÃ¤mie von Unternehmensanleihen? Entgegen bisherigen empirischen Untersuchungen betrachten wir den Zusammenhang zwischen Offenlegung und Kapitalkosten fÃ¼r Fremdfinanzierung. Folglich analysieren wir den Einfluss einer IFRS-Umstellung auf die RisikoprÃ¤mie von Anleihen deutscher, Ã¶sterreichischer und schweizer Unternehmen im Zeitraum von 1997 bis 2005. Unsere Ergebnisse zeigen, dass sich die VerÃ¤nderung der RisikoprÃ¤mie nach einer Umstellung auf IFRS um ca. 39% verringert. Allerdings tritt dieser Effekt mit einer zeitlichen VerzÃ¶gerung auf, da die Verringerung im zweiten Jahr nach der Umstellung stÃ¤rker ist als im ersten Jahr."
"211";211;"2007-057";"Conditional Complexity of Compression for Authorship Attribution";"Mikhail B. Malyutov, Chammi I. Wickramasinghe and  Sufeng Li";"D";"2007-09-26";"C12,  C15,  C63";" Conditional Complexity of Compression for Authorship Attribution Abstract:  We introduce new stylometry tools based on the sliced conditional compression complexity of literary texts which are inspired by the nearly optimal application of the incomputable Kolmogorov conditional complexity (and presumably approximates it). Whereas other stylometry tools can occasionally be very close for different authors, our statistic is apparently strictly minimal for the true author, if the query and training texts are sufficiently large, compressor is sufficiently good and sampling bias is avoided (as in the poll samplings). We tune it and test its performance on attributing the Federalist papers (Madison vs. Hamilton). Our results confirm the previous attribution of Federalist papers by Mosteller and Wallace (1964) to Madison using the Naive Bayes classifier and the same attribution based on alternative classifiers such as SVM, and the second order Markov model of language. Then we apply our method for studying the attribution of the early poems from the Shakespeare Canon and the continuation of MarloweÂ’s poem Â‘Hero and LeanderÂ’ ascribed to G. Chapman."
"212";212;"2007-058";"Total Work, Gender and Social Norms";"Michael Burda, Daniel S. Hamermesh and  Philippe Weil";"C7";"2007-09-26";"J22,  J16,  D13";" Using time-diary data from 25 countries, we demonstrate that there is a negative relationship between real GDP per capita and the female-male difference in total work time per dayÂ—the sum of work for pay and work at home. In rich northern countries on four continents there is no differenceÂ—men and women do the same amount of total work. This latter fact has been presented before by several sociologists for a few rich countries; but our survey results show that labor economists, macroeconomists, the general public and sociologists are unaware of it and instead believe that women perform more total work. The facts do not arise from gender differences in the price of time (as measured by market wages), as womenÂ’s total work is further below menÂ’s where their relative wages are lower. Additional tests using U.S. and German data show that they do not arise from differences in marital bargaining, as gender equality is not associated with marital status; nor do they stem from family norms, since most of the variance in the gender total work difference is due to within-couple differences. We offer a theory of social norms to explain the facts. The social-norm explanation is better able to account for withineducation group and within-region gender differences in total work being smaller than inter-group differences. It is consistent with evidence using the World Values Surveys that female total work is relatively greater than menÂ’s where both men and women believe that scarce jobs should be offered to men first."
"213";213;"2007-059";"Long-Term Orientation In Family And Non-Family Firms: A Bayesian Analysis";"Jörn Hendrich Block and  Andreas Thams";"C6";"2007-11-21";"C11,  D21,  G31,  G32,  L20,  ";" A stronger long-term orientation is considered a competitive advantage of family firms relative to non-family firms. In this study, we use panel data of U.S. firms and analyze this proposition. Our findings are surprising. Only in when the family is involved in the management of the firm is the firm found to invest more in long-term projects relative to a non-family firm. We also find that investment in long-term projects in family firms is determined less by cash flow variations than for non-family firms. Managerial implications of our findings are discussed. Our hypotheses are tested using Bayesian methods."
"214";214;"2007-060";"Kombinierte Liquiditäts- und Solvenzkennzahlen und ein darauf basierendes Insolvenzprognosemodell für deutsche GmbHs";"Volodymyr Perederiy";"D";"2007-11-21";"C13,  C25,  G32,  G33";" Eine groÃŸe Herausforderung der multivariablen Analyse mit bilanziellen Kennzahlen besteht in der Identifikation derjenigen Kennzahlen, die zur besten Modellperformance fÃ¼hren und dabei mÃ¶glichst leicht interpretierbar und intuitiv bleiben. Die Menge der in Frage kommenden Kennzahlen ist in der Regel groÃŸ; viele Kennzahlen weisen AbhÃ¤ngigkeiten und Korrelationen auf, was im multivariaten Kontext zu MultikollinearitÃ¤tsproblemen fÃ¼hrt. Diese Aussagen betreffen insbesondere auch die Insolvenzprognosemodellierung auf Grundlage von bilanziellen Informationen. In der vorliegenden Studie wird mittels einer einfachen Gewichtung und Division von geeigneten Bilanz- und GuV-Posten eine kombinierte Kennzahl gebildet, welche die Informationen aus den meisten traditionellen LiquiditÃ¤ts- und Solvenzkennzahlen komprimiert und somit eine diesbezÃ¼gliche Kennzahlenauswahl Ã¼berflÃ¼ssig macht. Die kombinierte Kennzahl wird anschlieÃŸend Â– neben einigen Kennzahlen aus anderen Kennzahlenkategorien (RentabilitÃ¤t, Effizienz, UnternehmensgrÃ¶ÃŸe) Â– zur Insolvenzprognose fÃ¼r deutsche GmbHs verwendet. Es wird demonstriert, dass die kombinierte Kennzahl die Insolvenzprognose verbessert und dabei leicht interpretierbar bleibt."
"215";215;"2007-061";"Embedding R in the Mediawiki";"Sigbert Klinke and  Olga Zlatkin-Troitschanskaia";"B1";"2007-11-21";"C63";" Teaching statistics to students in our area of economics and educational science often brings about the problem that students have either forgotten their statistical knowledge, or have taken different classes than the ones we offer in basic statistics. We therefore need some kind of statistical dictionary where we, as teachers, can refer to a common base and where students can look up specific terms. The Wikipedia - a general online encyclopaedia - compelled us to use a wiki for our dictionary. While the Wikipedia contains a large number of statistical terms, these are often too long and detailed to be visual displayed in lectures very well and some more specific terms are not included. "
"216";216;"2007-062";"Das Hybride Wahlmodell und seine Anwendung im Marketing";"Till Dannewald, Henning Kreis and  Nadja Silberhorn";"B2";"2007-11-21";"M30,  C51,  C10";" Traditional choice models assume that observable behavior results from an unspecified evaluation process of the observed individual. When it comes to the revelation of this process mere choice models rapidly meet their boundaries, as psychological factors (e.g., consumersÂ’ perception or attitudes towards products) are not directly measurable variables and therefore cannot offhand be integrated within the model structure. The causal-analytic approach offers the possibility to specify not directly measurable factors as latent variables, and can thus reasonably supplement choice models. So far, methodological approaches investigating latent variables, and traditional choice models are perceived and applied independently of one another. In this paper the possibilities of an integration of latent variables into traditional choice models is pointed out, and an introduction into the modeling of hybrid choice models is provided. Furthermore, potential areas of application in marketing research are outlined."
"217";217;"2007-063";"Determinants of the Acquisition of Smaller Firms by Larger Incumbents in High-Tech Industries: Are they related to Innovation and Technology Sourcing?";"Marcus Wagner";"D";"2007-11-29";"L10,  L86,  M20";" Innovation activities in high tech industries provide considerable challenges for technology and innovation management. In particular, firms frequently face significant technological challenges since these industries has a long history of radical innovations which are taking place through distinct industry cycles of higher and lower demand. The paper investigates these issues for three high-tech industries, namely semiconductor manufacturing, biotechnology and electronic design automation which is a specific sub-segment of the semiconductor industry. It analyses the association of firm characteristics with different aspects of acquisition behaviour. Particular focus is put on innovation-related firm characteristics. The paper finds that the determinants for  acquisitions are mostly related to firm size, financial conditions and geographical origin of the firm. Only for biotechnology, a substitutive relationship is identified between acquisitions and own research activities. "
"218";218;"2007-064";"Correlation vs. Causality in Stock Market Comovement";"Enzo Weber";"C6";"2007-12-07";"C32,  G10";" This paper seeks to disentangle the sources of correlations between high-, mid- and lowcap stock indexes from the German prime standard. In principle, such comovement can arise from direct spillover between the variables or due to common factors. By standard means, these different components are obviously not identifiable. As a solution, the underlying study proposes specifying ARCH-type models for both the idiosyncratic innovations and a common factor, so that the model structure can be identified through heteroscedasticity. The seemingly surprising result that smaller caps have higher influence than larger ones is explained by asymmetric information processing in financial markets. Broad macroeconomic information is shown to enter the common factor rather than the segment-specific shocks."
"219";219;"2007-065";"Integrating latent variables in discrete choice models – How higher-order values and attitudes determine consumer choice";"Dirk Temme, Marcel Paulssen and  Till Dannewald";"B2";"2007-12-07";"C25,  C51,  C87,  M31,  R41";" Integrated choice and latent variable (ICLV) models represent a promising new class of models which merge classic choice models with the structural equation approach (SEM) for latent variables. Despite their conceptual appeal, to date applications of ICLV models in marketing are still rare. The present study on travel mode choice clearly demonstrates the value of ICLV models to enhance understanding of choice processes. In addition to the usually studied directly observable variables such as travel time, we show how abstract motivations such as power and hedonisms as well as attitudes such as a desire for flexibility impact on travel mode choice. Further, we can show that it is possible to estimate ICLV models with the widely available structural equation modeling package Mplus. This finding is likely to encourage wider usage of this appealing model class in the marketing field."
"220";220;"2007-066";"Modelling Financial High Frequency Data Using Point Processes";"Luc Bauwens and  Nikolaus Hautsch";"B8";"2007-12-10";"C22,  C32,  C41";" In this paper, we give an overview of the state-of-the-art in the econometric literature on the modeling of so-called financial point processes. The latter are associated with the random arrival of specific financial trading events, such as transactions, quote updates, limit orders or price changes observable based on financial high-frequency data. After discussing fundamental statistical concepts of point process theory, we review durationbased and intensity-based models of financial point processes. Whereas duration-based approaches are mostly preferable for univariate time series, intensity-based models provide powerful frameworks to model multivariate point processes in continuous time. We illustrate the most important properties of the individual models and discuss major empirical applications. "
"221";221;"2007-067";"A stochastic volatility Libor model and its robust calibration";"Denis Belomestny, Stanley Matthew and  John Schoenmakers";"B7";"2007-12-12";"J31,  I19,  C51";" In this paper we propose a Libor model with a high-dimensional specially structured system of driving CIR volatility processes. A stable calibration procedure which takes into account a given local correlation structure is presented. The calibration algorithm is FFT based, so fast and easy to implement. "
"222";222;"2007-068";"Why Votes Have a Value";"Ingolf Dittmann, Dorothea Kübler, Ernst Maug and  Lydia Mechtenberg";"A6";"2007-12-17";"C92,  D72,  G32";" We perform an experiment where subjects pay for the right to participate in a shareholder vote. We find that experimental subjects are willing to pay a significant premium for the voting right even though there should be no such premium in our setup under full rationality. Private benefits from controlling the firm are absent from our setup and overconfidence cannot explain the size of the observed voting premium. The premium disappears in treatments where voting has no material consequences for the subjects. We conclude that individuals enjoy being part of a group that exercises power and are therefore willing to pay for the right to vote even when the impact of their own vote on their payoffs is negligible. "
"223";223;"2007-069";"Solving Linear Rational Expectations Models with Lagged Expectations Quickly and Easily";"Alexander Meyer-Gohde";"C10";"2007-12-17";"C32,  C63";" A solution method is derived in this paper for solving a system of linear rationalexpectations equation with lagged expectations (e.g., models incorporating sticky information) using the method of undetermined coefficients for the infinite MA representation. The method applies a combination of a Generalized Schur Decomposition familiar elsewhere in the literature and a simple system of linear equations when lagged expectations are present to the infinite MA representation. Execution is faster, applicability more general, and use more straightforward than with existing algorithms. Current methods  of truncating lagged expectations are shown to not generally be innocuous and the use of such methods are rendered obsolete by the tremendous gains in computational efficiency of the method here which allows for a solution to floating-point accuracy in a fraction of the time required by standard methods.  The associated computational application of the method provides impulse responses to anticipated and unanticipated innovations, simulations, and frequency-domain and simulated moments. "
"224";224;"2007-070";"Telling the Truth May Not Pay Off: An Empirical Study of Centralised University Admissions in Germany";"Sebastian Braun, Nadja Dwenger and  Dorothea Kübler";"A6";"2007-12-20";"C78,  D02,  D78,  I29";" We investigate the matching algorithm used by the German central clearinghouse for university admissions (ZVS) in medicine and related subjects. This mechanism consists of three procedures based on final grades from school (Â“AbiturbestenverfahrenÂ”, Â“Auswahlverfahren der HochschulenÂ”) and on waiting time (Â“WartezeitverfahrenÂ”). While these procedures differ in the criteria applied for admission they all make use of priority matching. In priority matching schemes, it is not a dominant strategy for students to submit their true preferences. Thus, strategic behaviour is expected. Using the full data set of applicants, we are able to detect some amount of strategic behaviour which can lead to inefficient matching. Alternative ways to organize the market are briefly discussed. "
"225";225;"2008-001";"Testing Monotonicity of Pricing Kernels";"Yuri Golubev,  Wolfgang K. Härdle and   Roman Timofeev";"B1";"2008-01-07";"G12,  C12";" The behaviour of market agents has always been extensively covered in the literature. Risk averse behaviour, described by von Neumann and Morgenstern (1944) via a concave utility function, is considered to be a cornerstone of classical economics. Agents prefer a fixed profit over uncertain choice with the same expected value, however lately there has been a lot of discussion about the reliability of this approach. Some authors have shown that there is a reference point where market utility functions are convex. In this paper we have constructed a test to verify uncertainty about the concavity of agentsÂ’ utility function by testing the monotonicity of empirical pricing kernels (EPKs). A monotone decreasing EPK corresponds to a concave utility function while non-monotone decreasing EPK means non-averse pattern on one or more intervals of the utility function. We investigated the EPK for German DAX data for years 2000, 2002 and 2004 and found the evidence of non-concave utility functions: H0 hypothesis of monotone decreasing pricing kernel was rejected at 5% and 10% significance level in 2002 and at 10% significance level in 2000. "
"226";226;"2008-002";"Adaptive pointwise estimation in time-inhomogeneous time-series models";"Pavel Cizek, Wolfgang Härdle and  Vladimir Spokoiny";"B1";"2008-01-07";"C13,  C14,  C22";" This paper offers a new method for estimation and forecasting of the linear and nonlinear time series when the stationarity assumption is violated. Our general local parametric approach particularly applies to general varying-coefficient parametric models, such as AR or GARCH, whose coefficients may arbitrarily vary with time. Global parametric, smooth transition, and changepoint models are special cases. The method is based on an adaptive pointwise selection of the largest interval of homogeneity with a given right-end point by a local change-point analysis. We construct locally adaptive estimates that can perform this task and investigate them both from the theoretical point of view and by Monte Carlo simulations. In the particular case of GARCH estimation, the proposed method is applied to stock-index series and is shown to outperform the standard parametric GARCH model. "
"227";227;"2008-003";"The Bayesian Additive Classification Tree Applied to Credit Risk Modelling";"Junni L. Zhang and  Wolfgang Härdle";"B1";"2008-01-07";"C14,  C11,  C45,  C01";" We propose a new nonlinear classification method based on a Bayesian ""sum-of-trees"" model, the Bayesian Additive Classification Tree (BACT), which extends the Bayesian Additive Regression Tree (BART) method into the classification context. Like BART, the BACT is a Bayesian nonparametric additive model specified by a prior and a likelihood in which the additive components are trees, and it is fitted by an iterative MCMC algorithm. Each of the trees learns a different part of the underlying function relating the dependent variable to the input variables, but the sum of the trees offers a flexible and robust model. Through several benchmark examples, we show that the BACT has excellent performance. We apply the BACT technique to classify whether firms would be insolvent. This practical example is very important for banks to construct their risk profile and operate successfully. We use the German Creditreform database and classify the solvency status of German firms based on financial statement information. We show that the BACT outperforms the logit model, CART and the Support Vector Machine in identifying insolvent firms. "
"228";228;"2008-004";"Independent Component Analysis Via Copula Techniques";"Ray-Bing Chen, Meihui Guo, Wolfgang Härdle and  Shih-Feng Huang";"B1";"2008-01-07";"C01,  C13,  C14,  C63";" Independent component analysis (ICA) is a modern factor analysis tool de- veloped in the last two decades. Given p-dimensional data, we search for that linear combination of data which creates (almost) independent components. Here copulae are used to model the p-dimensional data and then independent components are found by optimizing the copula parameters. Based on this idea, we propose the COPICA method for searching independent components. We illustrate this method using several blind source separation examples, which are mathematically equivalent to ICA problems. Finally performances of our method and FastICA are compared to explore the advantages of this method. "
"229";229;"2008-005";"The Default Risk of Firms Examined with Smooth Support Vector Machines";"Wolfgang Härdle, Yuh-Jye Lee, Dorothea Schäfer and  Yi-Ren Yeh";"B1";"2008-01-07";"G30;C14;G33;C45";" In the era of Basel II a powerful tool for bankruptcy prognosis is vital for banks. The tool must be precise but also easily adaptable to the bank's objections regarding the relation of false acceptances (Type I error) and false rejections (Type II error). We explore the suitability of Smooth Support Vector Machines (SSVM), and investigate how important factors such as selection of appropriate accounting ratios (predictors), length of training period and structure of the training sample influence the precision of prediction. Furthermore we show that oversampling can be employed to gear the tradeoff between error types. Finally, we illustrate graphically how different variants of SSVM can be used jointly to support the decision task of loan officers. "
"230";230;"2008-006";"Value-at-Risk and Expected Shortfall when there is long range dependence";"Wolfgang Härdle and  Julius Mungo";"B1";"2008-01-07";"C14,  C32,  C52,  C53,  G12";" Empirical studies have shown that a large number of financial asset returns exhibit fat tails and are often characterized by volatility clustering and asymmetry. Also revealed as a stylized fact is Long memory or long range dependence in market volatility, with significant impact on pricing and forecasting of market volatility. The implication is that models that accomodate long memory hold the promise of improved long-run volatility forecast as well as accurate pricing of long-term contracts. On the other hand, recent focus is on whether long memory can affect the measurement of market risk in the context of Value-at- Risk (V aR). In this paper, we evaluate the Value-at-Risk (V aR) and Expected Shortfall (ESF) in financial markets under such conditions. We examine one equity portfolio, the British FTSE100 and three stocks of the German DAX index portfolio (Bayer, Siemens and Volkswagen). Classical V aR estimation methodology such as exponential moving average (EMA) as well as extension to cases where long memory is an inherent characteristics of the system are investigated. In particular, we estimate two long memory models, the Fractional Integrated Asymmetric Power-ARCH and the Hyperbolic-GARCH with different error distribution assumptions. Our results show that models that account for asymmetries in the volatility specifications as well as fractional integrated parametrization of the volatility process, perform better in predicting the one-step as well as five-step ahead V aR and ESF for short and long positions than short memory models. This suggests that for proper risk valuation of options, the degree of persistence should be investigated and appropriate models that incorporate the existence of such characteristic be taken into account. "
"231";231;"2008-007";"A Consistent Nonparametric Test for Causality in Quantile";"Kiho Jeong and  Wolfgang Härdle";"B1";"2008-01-07";"C14,  C52";" This paper proposes a nonparametric test of causality in quantile. Zheng (1998) has proposed an idea to reduce the problem of testing a quantile restriction to a problem of testing a particular type of mean restriction in independent data. We extend ZhengÂ’s approach to the case of dependent data, particularly to the test of Granger causality in quantile. The proposed test statistic is shown to have a second-order degenerate U-statistic as a leading term under the null hypothesis. Using the result on the asymptotic normal distribution for a general second order degenerate U-statistics with weakly dependent data of Fan and Li (1996), we establish the asymptotic distribution of the test statistic for causality in quantile under Î²-mixing (absolutely regular) process. "
"232";232;"2008-008";"Do Legal Standards Affect Ethical Concerns of Consumers?";"Dirk Engelmann and  Dorothea Kübler";"A6";"2008-01-21";"C91,  J88,  K31";" In order to address the impact of regulation on ethical concerns of consumers, we study the effect of a minimum wage. In our experimental market, consumers have monopsony power, firms engage in Bertrand competition, and workers are passive recipients of a wage payment. Two treatments are employed, one with no minimum wage in the first part but with a minimum wage in the second part, and one treatment with a minimum wage at the outset that is abolished in the second part. In both treatments, wages decrease over time in the first part even though some consumers show an interest in fair wages. If a minimum wage is in place, wages decline even faster. Introducing a minimum wage in a mature market raises average wages, while abolishing it lowers them. We discuss the implications of our results, such as the crowding out of ethical behavior through legal regulation. "
"233";233;"2008-009";"Recursive Portfolio Selection with Decision Trees";"Anton Andriyashin, Wolfgang Härdle and  Roman Timofeev";"B1";"2008-01-21";"C14,  C49,  G11,  G12";" A great proportion of stock dynamics can be explained using publicly available information. The relationship between dynamics and public information may be of nonlinear character. In this paper we offer an approach to stock picking by employing so-called decision trees and applying them to XETRA DAX stocks. Using a set of fundamental and technical variables, stocks are classified into three groups according to the proposed position: long, short or neutral. More precisely, by assessing the current state of a company, which is represented by fundamental variables and current market situation, well reflected by technical variables, it is possible to suggest if the current market value of a company is underestimated, overestimated or the stock is fairly priced. The performance of the model over the observed period suggests that XETRA DAX stock returns can adequately be predicted by publicly available economic data. Another conclusion of this study is that the implied volatility variable, when included into the training sample, boosts the predictive power of the model significantly. "
"234";234;"2008-010";"Do Public Banks have a Competitive Advantage?";"Astrid Matthey";"A6";"2008-01-21";"G21";" Private banks often blame state guarantees to distort competition by giv- ing public banks the advantage of lower funding costs. In this paper I show that if borrowers perceive the public bank as supporting economic develop- ment, private banks may be able to separate firms by self selection, enter the market, and obtain profits in equilibrium despite their cost disadvantage. The public bank's competitive advantage may be offset, independently of what its true objective function is. Even perfect competition between private banks does not lead to zero profits. "
"235";235;"2008-011";"Don’t aim too high: the potential costs of high aspirations";"Astrid Matthey and  Nadja Dwenger";"A6";"2008-01-21";"D11,  D84,  C91";" The higher our aspirations, the higher the probability that we have to adjust them downwards when forming more realistic expectations later on. This paper shows that the costs induced by high aspirations are not trivial. We first develop a theoretical framework to identify the factors that determine the effect of aspirations on expected utility. Then we present evidence from a lab experiment on the factor found to be crucial: the adjustment of reference states to changes in expectations. The results suggest that the costs of high aspirations can be significant, since reference states do not adjust quickly. We use a novel, indirect approach that allows us to infer the determinants of the reference state from observed behavior, rather than to rely on cheap talk. "
"236";236;"2008-012";"Visualizing exploratory factor analysis models";"Sigbert Klinke and   Cornelia Wagner";"B1";"2008-01-24";"C39,  C45,  C63";" Exploratory factor analysis (EFA) is an important tool in data analyses, particularly in social science. Usually four steps are carried out which contain a large number of options. One important option is the number of factors and the association of variables with a factor. Our tools aim to visualize various models with different numbers in parallel of factors and to analyze which consequences a specific option has.We apply our method to data collected at the School of Business and Economics for evaluation of lectures by students. These data were analyzed by Zhou (2004) and Reichelt (2007). "
"237";237;"2008-013";"House Prices and Replacement Cost: A Micro-Level Analysis";"Rainer Schulz and  Axel Werwatz";"B3";"2008-01-31";"C52,  C53,  R31";" According to housing investment models, house prices and replacement cost should have an equilibrating relationship. Previous empirical work mainly based on aggregate-level data has found only little evidence of such a relationship. By using a unique data set, covering transactions of single-family houses over a 25 years period, we establish strong support for the relationship at the micro level. In the time series context, we find that new house prices and replacement cost align quickly after a shock. In the cross-sectional context, we find prices of old houses and replacement cost are closely related once building depreciation has been taken into account. As to be expected from these results, replacement cost information also proves to be useful for the prediction of future house prices. "
"238";238;"2008-014";"Support Vector Regression Based GARCH Model with Application to Forecasting Volatility of Financial Returns";"Shiyi Chen, Kiho Jeong and  Wolfgang Härdle";"B1";"2008-01-31";"C45,  C53,  G32";" In recent years, support vector regression (SVR), a novel neural network (NN) technique, has been successfully used for financial forecasting. This paper deals with the application of SVR in volatility forecasting. Based on a recurrent SVR, a GARCH method is proposed and is compared with a moving average (MA), a recurrent NN and a parametric GACH in terms of their ability to forecast financial markets volatility. The real data in this study uses British Pound-US Dollar (GBP) daily exchange rates from July 2, 2003 to June 30, 2005 and New York Stock Exchange (NYSE) daily composite index from July 3, 2003 to June 30, 2005. The experiment shows that, under both varying and fixed forecasting schemes, the SVR-based GARCH outperforms the MA, the recurrent NN and the parametric GARCH based on the criteria of mean absolute error (MAE) and directional accuracy (DA). No structured way being available to choose the free parameters of SVR, the sensitivity of performance is also examined to the free parameters. "
"239";239;"2008-015";"Structural Constant Conditional Correlation";"Enzo Weber";"C6";"2008-01-31";"C32,  G10";" A small strand of recent literature is occupied with identifying simultaneity in multiple equation systems through autoregressive conditional heteroscedasticity. Since this approach assumes that the structural innovations are uncorrelated, any contemporaneous connection of the endogenous variables needs to be exclusively explained by mutual spillover effects. In contrast, this paper allows for instantaneous covariances, which become identifiable by imposing the constraint of structural constant conditional correlation (SCCC). In this, common driving forces can be modelled in addition to simultaneous transmission effects. The new methodology is applied to the Dow Jones and Nasdaq Composite indexes in a small empirical example, illuminating scope and functioning of the SCCC model. "
"240";240;"2008-016";"Estimating Investment Equations in Imperfect Capital Markets";"Silke Hüttel, Oliver Mußhoff, Martin Odening and  Nataliya Zinych";"D";"2008-01-31";"D81,  D92,  C51";" Numerous studies have tried to provide a better understanding of firm-level investment behaviour using econometric models. The model specification of more recent studies has been based on two main approaches. The first, the real options approach, focuses on irreversibility and uncertainty in perfect capital markets; of particular interest is the range of inaction caused by sunk costs. The second, the neo-institutional finance theory, emphasises capital market imperfections and firmsÂ’ released liquidity constraints. Empirical applications of the latter theory often refer to linear econometric models to prove these imperfections and thus do not account for the range of inaction caused by irreversibility. In this study, a generalised Tobit model based on an augmented q model is developed with the intention of considering the coexistence of irreversibility and capital market imperfections. Simulation-based experiments allow investigating the properties of this model. It can be shown how disregarding irreversibility reduces effectiveness of simpler linear models. "
"241";241;"2008-017";"Adaptive Forecasting of the EURIBOR Swap Term Structure";"Oliver Blaskowitz and  Helmut Herwatz";"C2";"2008-01-31";"C32,  C53,  E43,  G29";" In this paper we adopt a principal components analysis (PCA) to reduce the dimensionality of the term structure and employ autoregressive models (AR) to forecast principal components which, in turn, are used to forecast swap rates. Arguing in favor of structural variation, we propose data driven, adaptive model selection strategies based on the PCA/AR model. To evaluate ex-ante forecasting performance for particular rates, different forecast features such as mean squared errors, directional accuracy and big hit ability are considered. It turns out that relative to benchmark models, the adaptive approach offers additional forecast accuracy in terms of directional accuracy and big hit ability. "
"242";242;"2008-018";"Solving, Estimating and Selecting Nonlinear Dynamic Models without the Curse of Dimensionality";"Viktor Winschel and  Markus Krätzig";"C2";"2008-02-07";"C11,  C13,  C15,  C32,  C52,  ";" We present a comprehensive framework for Bayesian estimation of structural nonlinear dynamic  economic models on sparse grids. TheSmolyak operator underlying the sparse grids approach frees global  approximation from the curse of dimensionality and we apply it to a Chebyshev approximation of the model solution. The operator also eliminates the curse from Gaussian quadrature and we use it for the integrals arising from rational expectations and in three new nonlinear state space filters. The filters substantially  decrease the computational burden compared to the sequential importance resampling particle filter.  The posterior of the structural parameters is estimated by a new Metropolis-Hastings algorithm with mixing parallel sequences. The parallel extension improves the global maximization property of the algorithm,  simplifies the choice of the innovation variances, allows for unbiased convergence diagnostics and for  a simple implementation of the estimation on parallel computers. Finally, we provide all algorithms in the open source software JBendge4 for the solution and estimation of a general class of models. "
"243";243;"2008-019";"The Accuracy of Long-term Real Estate Valuations";"Rainer Schulz, Markus Staiber, Martin Wersing and  Axel Werwatz";"B3";"2008-02-12";"C52,  C53";" By using a unique data set of single-family house transactions, we examine the accuracy of the cost and sales comparison approach over different forecast horizons. We find that sales comparison values provide better long-term forecasts than cost values if the economic loss function is symmetric. A weighted average of both sales comparison value and cost value can reduce this loss even further. If the economic loss function is asymmetric, however, cost values might provide better long-term forecasts. "
"244";244;"2008-020";"The Impact of International Outsourcing on Labour Market Dynamics in Germany";"Ronald Bachmann and  Sebastian Braun";"C7";"2008-02-26";"F16,  J63,  J23";" Using an administrative data set containing daily information on individual workers' employment histories, we investigate how workers' labour market transitions are affected by international outsourcing. In order to do so, we estimate hazard rate models for match separations, as well as for worker flows from employment to another job, to unemployment, and to out of the labour force. Outsourcing is found to have no significant impact on job stability in the manufacturing sector, but it is associated with increased job stability in the service sector. Furthermore, especially in the service sector the effect of outsourcing varies across skill levels. An analysis of the different labour market flows shows that labour market transitions are not affected symmetrically by international outsourcing. "
"245";245;"2008-021";"Preferences for Collective versus Individualised Wage Setting";"Tito Boeri and  Michael C. Burda";"C7";"2008-02-26";"J5,  J6,  D7";" Standard models of equilibrium unemployment assume exogenous labour market institutions and flexible wage determination. This paper models wage rigidity and collective bargaining endogenously, when workers differ by observable skill and may adopt either individualised or collective wage bargaining. In the calibrated model, a substantial fraction of workers and firms as well as the median voter prefer collective bargaining to the decentralised regime. A fundamental distortion of the separation decision represented by employment protection (a firing tax) is necessary for such preferences to emerge. Endogenizing collective bargaining can significantly modify comparative statics effects of policy arising in a single-regime setting. "
"246";246;"2008-022";"Lumpy Labor Adjustment as a Propagation Mechanism of Business Cycles";"Fang Yao";"C7";"2008-02-27";"E32,  E24,  E22";" I explore the implications of the lumpy labor adjustment as a propagation mechanism for aggregate dynamics. The model I use nests the basic RBC model with a staggeredjob- turnover in the spirit of Taylor (1980) and Calvo (1983). It extends this approach by introducing a Weibull-distributed labor adjustment process to capture increasing hazard rates and heterogeneous labor rigidity in the economy corroborated by the micro data. My principal findings are: uncertainty in the labor adjustment process induces firms to make precautionary labor adjustment (the front-loading effect), amplifying the volatility of labor demand, and that the heterogeneity in labor rigidity leads to aggregate persistence in labor and output. The key message conveyed by this model is that heterogeneity in labor rigidity matters for the aggregate dynamics, and hence includes the information of the distribution of agents enriching the propagation mechanism of the RBC model. "
"247";247;"2008-023";"Family Management, Family Ownership and Downsizing: Evidence from S&P 500 Firms";"Jörn Hendrich Block";"D";"2008-03-06";"G34,  L21,  M12,  M13,  M14,  ";" Little is known about the relationship between family firms and their employees. This paper aims to close this gap. We distinguish between family management and family ownership as two dimensions of family firms and analyze their respective influence on downsizing. Our findings show that family management decreases the likelihood of downsizing, whereas the extent of family ownership decreases the likelihood of downsizing only with regard to deep job cuts (above 6%). We conclude that family managers have a strong long-term perspective, which is in line with both agency and stewardship theory. Yet, the idea that reputation concerns lead family owners to shy away from downsizing is only partially supported. "
"248";248;"2008-024";"Skill Specific Unemployment with Imperfect Substitution of Skills";"Runli Xie";"C7";"2008-03-11";"E24,  E32,  J63";" A large body of literature explains the inferior position of unskilled workers by imposing a structural shift in the labor force skill composition. This paper takes a different approach by emphasizing the connection between cyclical variations in skilled and unskilled labor markets. Using a stylized business cycle model with search frictions in the respective sub-markets, I find that imperfect substitution between skilled and unskilled labor creates a channel for the variations in the sub-markets. Together with a general labor augment- ing technology shock, it can generate downward sloping Beveridge curves. Calibrating the model to US data yields higher volatilities in the unskilled labor markets and reproduces stylized business cycle facts."
"249";249;"2008-025";"Price Adjustment to News with Uncertain Precision";"Nikolaus Hautsch, Dieter Hess and  Christoph Müller";"B8";"2008-03-11";"E44,  G14";" Bayesian learning provides a core concept of information processing in financial markets. Typically it is assumed that market participants perfectly know the quality of released news.  However, in practice, newsÂ’ precision is rarely disclosed. Therefore, we extend standard Bayesian  learning allowing traders to infer newsÂ’ precision from two different sources. If information  is perceived to be imprecise, prices react stronger. Moreover, interactions of the different  precision signals affect price responses nonlinearly. Empirical tests based on intra-day T-bond  futures price reactions to employment releases confirm the modelÂ’s predictions and reveal statistically  and economically significant effects of newsÂ’ precision. "
"250";250;"2008-026";"Information and Beliefs in a Repeated Normal-form Game";"Dietmar Fehr, Dorothea Kübler and  David Danz";"A6";"2008-04-03";"C72,  C92,  D84";" We study beliefs and choices in a repeated normal-form game. In addition to a baseline treatment with common knowledge of the game structure and feedback about choices in the previous period, we run treatments (i) without feedback about previous play, (ii) with no information about the opponent?s payoÂ¤s and (iii) with random matching. Using Stahl and Wilson's (1995) model of limited strategic reasoning, we classify behavior with regard to its strategic sophistication and consider its development over time. We use belief statements to track the consistency of subjects' actions and beliefs as well as the accuracy of their beliefs (relative to the opponent's true choice) over time. In the baseline treatment we observe more sophisticated play as well as more consistent and more accurate beliefs over time. We isolate feedback as the main driving force of such learning. In contrast, information about the opponent's payoffs has almost no effect on the learning path. While it has an impact on the average choice and belief structure aggregated over all periods, it does not alter the choices and the belief accuracy in their development over time. "
"251";251;"2008-027";"The Stochastic Fluctuation of the Quantile Regression Curve";"Wolfgang Härdle and   Song Song";"B1";"2008-04-03";"C00,  C14,  J01,  J31";" Let (X1, Y1), . . ., (Xn, Yn) be i.i.d. rvs and let l(x) be the unknown p-quantile regression curve of Y on X. A quantile-smoother ln(x) is a localised, nonlinear estimator of l(x). The strong uniform consistency rate is established under general conditions. In many applications it is necessary to know the stochastic fluctuation of the process {ln(x) - l(x)}. Using strong approximations of the empirical process and extreme value theory allows us to consider the asymptotic maximal deviation sup06x61 |ln(x)-l(x)|. The derived result helps in the construction of a uniform confidence band for the quantile curve l(x). This confidence band can be applied as a model check, e.g. in econometrics. An application considers a labour market discrimination effect. "
"252";252;"2008-028";"Are stewardship and valuation usefulness compatible or alternative objectives of financial accounting?";"Joachim Gassen";"D";"2008-04-03";"D82,  G14,  G34,  M41";" In their joint framework project, the FASB and the IASB recently proposed dropping stewardship as a separate objective of financial accounting, because the Boards view stewardship and valuation usefulness as compatible sub-objectives ranking under an overall objective of decision usefulness. This paper puts this conjecture to an empirical test. As it is widely agreed that asymmetric timely earnings increase the contractual efficiency of accounting information, I first test whether firms with more asymmetric timely earnings produce more valuation-useful financial accounting information. Second, I test whether firms with more influential non-equity stakeholders provide more valuation-useful financial accounting information. As non-equity stakeholders in general face higher transaction costs when diversifying unsystematic risk compared to equity stakeholders and as stewardship-related risks should be at least in part unsystematic, I expect the demand for stewardship-related accounting information to increase with the influence of non-equity stakeholders. Using a broad sample of U.S. firms and a set of firm-specific metrics for valuation usefulness based on short-window capital market reactions to quarterly earnings announcements, I document that the valuation usefulness of financial accounting information is consistently negatively related to its stewardshiporientation. I conclude from these analyses that valuation usefulness and stewardship are alternative objectives of financial accounting. "
"253";253;"2008-029";"Genetic Codes of Mergers, Post Merger Technology Evolution and Why Mergers Fail";"Alexander Cuntz";"D";"2008-04-18";"O30,  L22,  L25,  C78,  L65";" This paper addresses the key determinants of merger failure, in par- ticular the role of innovation (post-merger performance) and technology (ex-ante selection) when firms decide to separate. After a brief review of the existing literature we introduce a model of process innovation where merged firms exibit intra-merger spillover of knowledge under different mar- ket regimes, depending on whether firms integrate vertically or horizontally. Secondly, we describe an ideal matching pattern for ex-ante selection cri- teria of technological partnering, abstracting from financial market power issues. In a final section we test the model implications for merger failure for M&A data from the US biotechnology industry in the 90s. We find that post-merger innovation performance, in particular with large spillovers, in- creases the probability of survival, while we have no evidence that market power effects do so in long run. Additionally, we find extensive technology sourcing activity by firms (already in the 90s) which contradicts the notion of failure and suits well the open innovation paradigm. "
"254";254;"2008-030";"Using R, LaTeX and Wiki for an Arabic e-learning platform";"Taleb Ahmad, Wolfgang Härdle, Sigbert Klinke and  Shafeeqah Al Awadhi";"B1";"2008-04-18";"I21,  C19";" E-learning plays an important role in education as it supports online education via computer networks and provides educational services by utilising information technologies. We present a case study describing the development of an Arabic language elearning course in statistics. Discussed are issues concerning e-learning in Arab countries with special focus on problems of the application of e-learning in the Arab world and the difficulties concerning the design Arabic platforms such as language problems, cultural and technical problems, especially ArabTeX works difficulty with LaTeX format. Thus Wiki is offered as a solution to such problems. Wiki supports LaTeX and other statistical programs, for instance R, andWiki offers the solution to language problems. Details of this technology are discussed and a solution as to how this system can serve in building an Arab platform is presented. "
"255";255;"2008-031";"Beyond the business cycle - factors driving aggregate mortality rates";"Katja Hanewald";"D";"2008-04-18";"I12,  J11,  C32";" This article provides a comprehensive econometric analysis of factors driving aggregate mortality rates over time. It differs from previous studies in this field by simultaneously considering an extensive set of macroeconomic, socio-economic and ecological factors as explanatory variables. Germany is chosen as an indicative example for other industrialized countries due to its advanced demographic transition process. Our regression analysis, which covers the time interval 1956-2004, indicates that sex- and age-specific mortality rates vary substantially in their response to external factors. Strongest associations are found with changes in real GDP, flu epidemics and the two life style variables alcohol and cigarette consumption in both univariate and multivariate setups. Further analysis shows that these effects are primarily contemporary, while other indicators such as weather conditions exert lagged effects. By combining variables in a multivariate model the share of explained data volatility can be substantially increased. "
"256";256;"2008-032";"Against All Odds? National Sentiment and Wagering on European Football";"Sebastian Braun and  Michael Kvasnicka";"C7";"2008-04-18";"L20,  L83";" This paper studies how national sentiment in the form of either a perception or a loyalty bias of bettors may affect pricing patterns on national wagering markets for international sport events. We show theoretically that both biases can be profitably exploited by bookmakers by way of price adjustment (odds shading). The former bias induces bookmakers to shade odds against the domestic team, the latter to adjust them in a way that depends on the demand elasticity of bettors for their national favorite. Analyzing empirically a unique data set of betting quotas from online bookmakers in twelve European countries for qualification games to the UEFA Euro 2008, we find evidence for systematic biases in the pricing of own national teams in the odds for win offered across countries. Variations in the sign and magnitude of these deviations can be explained by differences across countries in the respective strengths of the perception and loyalty biases among domestic bettors. "
"257";257;"2008-033";"Are CEOs in Family Firms Paid Like Bureaucrats? Evidence from Bayesian and Frequentist Analyses";"Jörn Hendrich Block";"D";"2008-04-25";"G30,  J30,  M52";" The relationship between CEO pay and performance has been much analyzed in the management and economics literature. This study analyzes the structure of executive compensation in family and non-family firms. In line with predictions of agency theory, it is found that the share of base salary is higher with family-member CEOs than it is with nonfamily member CEOs. Furthermore, family-member CEOs receive a lower share of option pay. The paperÂ’s findings have implications for family business research and the executive compensation literature. To make the findings robust, the statistical analysis is performed with both Bayesian and classical frequentist methods. "
"258";258;"2008-034";"JBendge: An Object-Oriented System for Solving, Estimating and Selecting Nonlinear Dynamic Models";"Viktor Winschel and  Markus Krätzig";"C2";"2008-04-25";"C11,  C13,  C15,  C32,  C52,  ";" We present an object-oriented software framework allowing to specify, solve, and estimate nonlinear dynamic general equilibrium (DSGE) models. The imple- mented solution methods for finding the unknown policy function are the standard linearization around the deterministic steady state, and a function iterator using a multivariate global Chebyshev polynomial approximation with the Smolyak op- erator to overcome the course of dimensionality. The operator is also useful for numerical integration and we use it for the integrals arising in rational expecta- tions and in nonlinear state space filters. The estimation step is done by a parallel Metropolis-Hastings (MH) algorithm, using a linear or nonlinear filter. Implemented are the Kalman, Extended Kalman, Particle, Smolyak Kalman, Smolyak Sum, and Smolyak Kalman Particle filters. The MH sampling step can be interactively moni- tored and controlled by sequence and statistics plots. The number of parallel threads can be adjusted to benefit from multiprocessor environments. JBendge is based on the framework JStatCom, which provides a standardized ap- plication interface. All tasks are supported by an elaborate multi-threaded graphical user interface (GUI) with project management and data handling facilities. "
"259";259;"2008-035";"Stock Picking via Nonsymmetrically Pruned Binary Decision Trees";"Anton Andriyashin";"B1";"2008-05-27";"C14,  C15,  C44,  C63,  G12";" Stock picking is the field of financial analysis that is of particular interest for many professional investors and researchers. In this study stock picking is implemented via binary classification trees. Optimal tree size is believed to be the crucial factor in forecasting performance of the trees. While there exists a standard method of tree pruning, which is based on the cost-complexity tradeoff and used in the majority of studies employing binary decision trees, this paper introduces a novel methodology of nonsymmetric tree pruning called Best Node Strategy (BNS). An important property of BNS is proven that provides an easy way to implement the search of the optimal tree size in practice. BNS is compared with the traditional pruning approach by composing two recursive portfolios out of XETRA DAX stocks. Performance forecasts for each of the stocks are provided by constructed decision trees. It is shown that BNS clearly outperforms the traditional approach according to the backtesting results and the Diebold-Mariano test for statistical significance of the performance difference between two forecasting methods. "
"260";260;"2008-036";"Expected Inflation, Expected Stock Returns, and Money Illusion: What can we learn from Survey Expectations?";"Maik Schmeling and  Andreas Schrimpf";"D";"2008-05-27";"G10,  G12,  E44";" We show empirically that survey-based measures of expected inflation are significant and strong predictors of future aggregate stock returns in several industrialized countries both in-sample and out-of-sample. By empirically discriminating between competing sources of this return predictability by virtue of a comprehensive set of expectations data, we find that money illusion seems to be the driving force behind our results. Another popular hypothesis - inflation as a proxy for aggregate risk aversion - is not supported by the data. "
"261";261;"2008-037";"The Impact of Individual Investment Behavior for Retirement Welfare: Evidence from the United States and Germany";"Thomas Post,  Helmut Gründl,  Joan T. Schmit and  Anja Zimmer";"D";"2008-05-27";"D14,  D91,  G11,  G28,  I31";" Much of the industrialized world is undergoing a significant demographic shift, placing strain on public pension systems. Policymakers are responding with pension system reforms that put more weight on privately managed retirement funds. One concern with these changes is the effect on individual welfare if individuals invest suboptimally. Using micro-level data from the United States and Germany, we compare the optimal expected lifetime utility computed using a realistically calibrated model with the actual utility as reflected in empirical asset allocation choices. Through this analysis, we are able to identify the population subgroups with relatively large welfare losses. Our results should be helpful to public policymakers in designing programs to improve the performance of privately organized retirement systems. "
"262";262;"2008-038";"Dynamic Semiparametric Factor Models in Risk Neutral Density Estimation";"Enzo Giacomini, Wolfgang Härdle and  Volker Krätschmer";"B1";"2008-05-27";"C14,  C32,  G12";" Dimension reduction techniques for functional data analysis model and approximate smooth random functions by lower dimensional objects. In many applications the focus of interest lies not only in dimension reduction but also in the dynamic behaviour of the lower dimensional objects. The most prominent dimension reduction technique - functional principal components analysis - however, does not model time dependences embedded in functional data. In this paper we use dynamic semiparametric factor models (DSFM) to reduce dimensionality and analyse the dynamic structure of unknown random functions by means of inference based on their lower dimensional representation. We apply DSFM to estimate the dynamic structure of risk neutral densities implied by prices of option on the DAX stock index. "
"263";263;"2008-039";"Can Education Save Europe From High Unemployment?";"Nicole Walter and  Runli Xie";"C7";"2008-06-17";"E24,  J24,  J52";" Empirical observations show that education helps to protect against labor market risks. This is twofold: The higher educated face a higher expected wage income and a lower probability of being unemployed. Although this relationship has been analyzed in the literature broadly, several questions remain to be tackled. This paper contributes to the existing literature by looking at the above mentioned phenomena from a purely theoretic perspective and in a European context. We set up a model with search-and-matching frictions, collective bargaining and monopolistic competition in the product market. Workers are heterogeneous in their human capital level. It is shown that higher human capital increases the wage rate and reduces unemployment risks, which is consistent with empirical observations for European countries. "
"264";264;"2008-040";"Solow Residuals without Capital Stocks";"Michael C. Burda and   Battista Severgnini";"C7";"2008-08-11";"D24,  E01,  E22,  O33,  O47";" For more than fifty years, the Solow decomposition (Solow 1957) has served as the standard measurement of total factor productivity (TFP) growth in economics and management, yet little is known about its precision, especially when the capital stock is poorly measured. Using synthetic data generated from a prototypical stochastic growth model, we explore the quantitative extent of capital measurement error when the initial condition is unknown to the analyst and when capacity utilization and depreciation are endogenous. We propose two alternative measurements which eliminate capital stocks from the decomposition and significantly outperform the conventional Solow residual, reducing the root mean squared error in simulated data by as much as two-thirds. This improvement is inversely related to the sample size as well as proximity to the steady state. As an application, we compute and compare TFP growth estimates using data from the new and old German federal states."
"265";265;"2008-041";"Unionization, Stochastic Dominance, and Compression";"Michael Burda,  Bernd Fitzenberger,  Alexander Lembcke and   Thorsten Vogel";"C7";"2008-06-16";"J31,  J51,  J52";" This paper establishes theoretical and empirical linkages between union wage setting and the structure of the wage distribution. Theoretically, we identify conditions under which a right-to-manage model implies compression of the wage distribution in the union sector relative to the nonunion sector as well as first-order stochastic dominance. These implications are investigated using quantile regressions on the 2001 GSES, a large German linked employerÂ–employee data set which contains explicit information on coverage by collective agreements. The empirical results confirm that, in case of industry-wide collective agreements, log union wage effects decline in quantiles, implying union wage compression. This finding, however, cannot be corroborated for wages determined at the firm level. Stochastic dominance is confirmed, as predicted by the theoretical model, for both types of collective agreements."
"266";266;"2008-042";"Gruppenvergleiche bei hypothetischen Konstrukten – Die Prüfung der Übereinstimmung von Messmodellen mit der Strukturgleichungsmethodik";"Dirk Temme and  Lutz Hildebrandt";"B2";"2008-06-18";"C31,  C51,  C81,  M31";" Comparing groups with respect to hypothetical constructs requires that the measurement models are equal across groups. Otherwise conclusions drawn from the observed indicators regarding differences at the latent level (mean differences, differences in the structural relations) might be severly distorted. This article provides a state of the art on how to apply multi-group confirmatory factor analysis to assess measurement invariance. The required steps in the analysis of the observed indicator means and variances/covariances are described, placing special emphasis on how to identify noninvariant indicators. The procedure is demonstrated considering the construct brand strength (Â“Brand Potential IndexÂ”, BPIÂ®) introduced by GfK Market Research as an example. "
"267";267;"2008-043";"Modeling Dependencies in Finance using Copulae";"Wolfgang Härdle, Ostap Okhrin and  Yarema Okhrin";"B1B10";"2008-06-19";"C00,  C14,  C51";" In this paper we provide a review of copula theory with applications to finance. We illustrate the idea on the bivariate framework and discuss the simple, elliptical and Archimedean classes of copulae. Since the cop- ulae model the dependency structure between random variables, next we explain the link between the copulae and common dependency measures, such as Kendall's tau and Spearman's rho. In the next section the copulae are generalized to the multivariate case. In this general setup we discuss and provide an intensive literature review of estimation and simulation techniques. Separate section is devoted to the goodness-of-fit tests. The importance of copulae in finance we illustrate on the example of asset allocation problems, Value-at-Risk and time series models. The paper is complemented with an extensive simulation study and an application to financial data. "
"268";268;"2008-044";"Numerics of Implied Binomial Trees";"Wolfgang Härdle and  Alena Mysickova";"B1";"2008-06-19";"G12,  G13,  C13";" Market option prices in last 20 years confirmed deviations from the Black and Scholes (BS) models assumptions, especially on the BS implied volatility. Implied binomial trees (IBT) models capture the variations of the implied volatility known as \volatility smile"". They provide a discrete approximation to the continuous risk neutral process for the underlying assets. In this paper, we describe the numerical construction of IBTs by Derman and Kani (DK) and an alternative method by Barle and Cakici (BC). After the formation of IBT we can estimate the implied local volatility and the state price density (SPD). We compare the SPD estimated by the IBT methods with a conditional density computed from a simulated diffusion process. In addition, we apply the IBT to EUREX option prices and compare the estimated SPDs. Both IBT methods coincide well with the estimation from the simulated process, though the BC method shows smaller deviations in case of high interest rate, particularly. "
"269";269;"2008-045";"Measuring and Modeling Risk Using High-Frequency Data";"Wolfgang Härdle, Nikolaus Hautsch and  Uta Pigorsch";"B1";"2008-06-26";"C13,  C14,  C22,  C52,  C53";" Measuring and modeling financial volatility is the key to derivative pricing, asset allocation and risk management. The recent availability of high-frequency data allows for refined methods in this field. In particular, more precise measures for the daily or lower frequency volatility can be obtained by summing over squared high-frequency returns. In turn, this so-called realized volatility can be used for more accurate model evaluation and description of the dynamic and distributional structure of volatility. Moreover, non-parametric measures of systematic risk are attainable, that can straightforwardly be used to model the commonly observed time-variation in the betas. The discussion of these new measures and methods is accompanied by an empirical illustration using high-frequency data of the IBM incorporation and of the DJIA index. "
"270";270;"2008-046";"Links between sustainability-related innovation and sustainability management";"Marcus Wagner";"D";"2008-06-26";"C30,  L73,  Q25";" This paper analyses the link between sustainability-related innovation and sustainability performance and the role that family firms play in this. This theme is particular relevant from a European point of view given the large number of firms that are family-owned. Governments often support environmentally and socially beneficial innovation with various policy instruments with the intention is to increase international competitiveness and simultaneously support sustainable development. In parallel, firms use corporate social responsibility (CSR) and environmental management systems partly in the hope that this will foster such innovation in their organisation. Hence the main research question of this paper is about the association of CSR and environmental management with environmentally and socially beneficial innovation and its determinants. Based on panel data, the paper analyses the link of corporate sustainability performance with sustainability innovation and the effect of being a family firm using panel estimation techniques. The paper discusses the results of the analysis, which point to a moderating role of family firms on the link of sustainability innovation and performance and assesses the policy implications of this insight. "
"271";271;"2008-047";"Modelling High-Frequency Volatility and Liquidity Using Multiplicative Error Models";"Nikolaus Hautsch and  Vahidin Jeleskovic";"B8";"2008-07-08";"C13,  C32,  C52";" In this paper, we study the dynamic interdependencies between high-frequency volatility, liquidity demand as well as trading costs in an electronic limit order book market. Using data from the Australian Stock Exchange we model 1-min squared mid-quote returns, average trade sizes, number of trades and average (excess) trading costs per time interval in terms of a four-dimensional multiplicative error model. The latter is augmented to account also for zero observations. We find evidence for significant contemporaneous relationships and dynamic interdependencies between the individual variables. Liquidity is causal for future volatility but not vice versa. Furthermore, trade sizes are negatively driven by past trading intensities and trading costs. Finally, excess trading costs mainly depend on their own history. "
"272";272;"2008-048";"Macro Wine in Financial Skins: The Oil-FX Interdependence";"Enzo Weber";"C6";"2008-07-08";"C32,  F31,  Q43";" This paper analyses mutual causalities between crude oil price and euro / US dollar exchange rate. Instead of focusing on long-run macroeconomic linkages like the bulk of the relevant literature, the present approach takes a financial markets perspective using daily data. The fast-running simultaneous impacts are identified through heteroscedasticity by specifying multivariate EGARCH processes for the structural variances. While for the decade after 1986 no significance is found, thereafter oil price changes cause inverse reactions of the dollar price and affect its volatility. Reversely, dollar appreciation asymmetrically increases the oil price. "
"273";273;"2008-049";"Simultaneous Stochastic Volatility Transmission Across American Equity Markets";"Enzo Weber";"C6";"2008-07-08";"C32,  G15";" Information flows across international financial markets typically occur within hours, making volatility spillover appear contemporaneous in daily data. Such simultaneous transmission of variances is featured by the stochastic volatility model developed in this paper, in contrast to usually employed multivariate ARCH processes. The identification problem is solved by considering heteroscedasticity of the structural volatility innovations, and estimation takes place in an appropriately specified state space setup. In the empirical application, unidirectional volatility spillovers from the US stock market to three American countries are revealed. The impact is strongest for Canada, followed by Mexico and Brazil, which are subject to idiosyncratic crisis effects. "
"274";274;"2008-050";"A semiparametric factor model for electricity forward";"Szymon Borak and  Rafal Weron";"B1";"2008-07-16";"C51,  G13,  Q40";" In this paper we introduce the dynamic semiparametric factor model (DSFM) for electricity forward curves. The biggest advantage of our approach is that it not only leads to smooth, seasonal forward curves extracted from exchange traded futures and forward electricity contracts, but also to a parsimonious factor representation of the curve. Using closing prices from the Nordic power market Nord Pool we provide empirical evidence that the DSFM is an efficient tool for approximating forward curve dynamics. "
"275";275;"2008-051";"Recurrent Support Vector Regression for a Nonlinear ARMA Model with Applications to Forecasting Financial Returns";"Shiyi Chen, Kiho Jeong and  Wolfgang K. Härdle";"B1";"2008-07-16";"C45,  F37,  F47";" Recurrent Support Vector Regression for a Nonlinear ARMA Model with Applications to Forecasting Financial Returns Abstract: Motivated by the recurrent Neural Networks, this paper proposes a recurrent Support Vector Regression (SVR) procedure to forecast nonlinear ARMA model based simulated data and real data of financial returns. The forecasting ability of the recurrent SVR is compared with three competing methods, MLE, recurrent MLP and feedforward SVR. Theoretically, MLE and MLP only focus on fit in-sample, but SVR considers both fit and forecast out-of-sample which endows SVR with an excellent forecasting ability. This is confirmed by the evidence from the simulated and real data based on two forecasting accuracy evaluation metrics (NSME and sign). That is, for one-step-ahead forecasting, the recurrent SVR is consistently better than the MLE and the recurrent MLP in forecasting both the magnitude and turning points, and really improves the forecasting performance as opposed to the usual feedforward SVR. "
"276";276;"2008-052";"Bayesian Demographic Modeling and Forecasting: An Application to U.S. Mortality";"Wolfgang H. Reichmuth and  Samad Sarferaz";"C5";"2008-07-31";"C11,  C32,  C53,  I10,  J11";"  We present a new way to model age-specific demographic variables with the example of age-specific mortality in the U.S., building on the Lee-Carter approach and extending it in several dimensions. We incorporate covariates and model their dynamics jointly with the latent variables underlying mortality of all age classes. In contrast to previous models, a similar development of adjacent age groups is assured allowing for consistent forecasts. We develop an appropriate Markov Chain Monte Carlo algorithm to estimate the parameters and the latent variables in an efficient one-step procedure. Via the Bayesian approach we are able to asses uncertainty intuitively by constructing error bands for the forecasts. We observe that in particular parameter uncertainty is important for long-run forecasts. This implies that hitherto existing forecasting methods, which ignore certain sources of uncertainty, may yield misleadingly sure predictions. To test the forecast ability of our model we perform in-sample and out-of-sample forecasts up to 2050, revealing that covariates can help to improve the forecasts for particular age classes. A structural analysis of the relationship between age-specific mortality and covariates is conducted in a companion paper. "
"277";277;"2008-052a";"Modeling and Forecasting Age-Specific Mortality: A Bayesian Approach";"Wolfgang H. Reichmuth and   Samad Sarferaz";"C5";"2008-10-21";"";" We present a new way to model age-specific demographic variables, using the example of age-specific mortality in the United States, building on the LeeÂ–Carter approach and 	  extending it in several dimensions. We incorporate covariates and model their dynamics 	  jointly with the latent variables underlying mortality of all age classes. In contrast to 	  previous models, a similar development of adjacent age groups is assured, allowing for 	  consistent forecasts. We develop an appropriate Markov chain Monte Carlo algorithm 	  to estimate the parameters and the latent variables in an efficient one-step procedure. 	  Via the Bayesian approach we are able to assess uncertainty intuitively by constructing 	  error bands for the forecasts. We observe that in particular parameter uncertainty is 	  important for long-run forecasts. This implies that existing forecasting methods, which 	  ignore certain sources of uncertainty, may yield misleadingly sure predictions. To test 	  the forecast ability of our model we perform in-sample and out-of-sample forecasts up to  2050, revealing that covariates can help improve the forecasts for particular age classes. 	  A structural analysis of the relationship between age-specific mortality and covariates is conducted in a companion paper."
"278";278;"2008-053";"Yield Curve Factors, Term Structure Volatility, and Bond Risk Premia";"Nikolaus Hautsch and  Yangguoyi Ou";"B8";"2008-07-31";"C5,  E4,  G1";" We introduce a Nelson-Siegel type interest rate term structure model with the underlying yield factors following autoregressive processes revealing time-varying stochastic volatility. The factor volatilities capture risk inherent to the term struc- ture and are associated with the time-varying uncertainty of the yield curveÂ’s level, slope and curvature. Estimating the model based on U.S. government bond yields applying Markov chain Monte Carlo techniques we find that the yield factors and factor volatilities follow highly persistent processes. Using the extracted factors to explain one-year-ahead bond excess returns we observe that the slope and cur- vature yield factors contain the same explanatory power as the return-forecasting factor recently proposed by Cochrane and Piazzesi (2005). Moreover, we identify  slope and curvature risk as important additional determinants of future excess returns. Finally, we illustrate that the yield and volatility factors are closely con- nected to variables reflecting macroeconomic activity, inflation, monetary policy and employment growth. It is shown that the extracted yield curve components have long-term prediction power for macroeconomic fundamentals. "
"279";279;"2008-054";"The Natural Rate Hypothesis and Real Determinacy";"Alexander Meyer-Gohde";"C10";"2008-08-05";"C62,  E31,  E52,  E58,  E61";" The uniqueness of bounded local equilibria under interest rate rules is analyzed in a model with sticky information `a la Mankiw and Reis (2002). The main results are tighter bounds on monetary policy than in sticky-price models, irrelevance of the degree of output-gap targeting for determinacy, independence of determinacy regions from parameters outside the interest-rate rule, and equivalence between real determinacy in models satisfying the natural rate hypothesis and nominal determinacy in the associated full-information, flex-price equivalent. The analysis follows from boundedness considerations on the nonautonomous recursion that describe the MA(\infty) representation of variablesÂ’ reaction to endogenous fluctuations."
"280";280;"2008-055";"Technology sourcing by large incumbents through acquisition of small firms";"Marcus Wagner";"D";"2008-09-11";"L10,  L86,  M20";" Innovation activities in high technology industries provide considerable challenges for technology and innovation management. In particular, since these industries have a long history of radical innovations taking place through distinct industry cycles of higher and lower demand, firms frequently consider the option to use acquisitions as a means for technology sourcing. The paper investigates this behaviour for three high technology industries, namely semiconductor manufacturing, biotechnology and electronic design automation which is a specific sub-segment of the semiconductor industry. It analyses the association of firm characteristics with different aspects of acquisition behaviour with a particular focus being put on innovation-related firm characteristics. The paper confirms a substitutive relationship between acquisitions and own research activities as well as between own and acquired firm patenting, but also finds that firm size, financial conditions and geographical origin of the firm matter for acquisition behaviour."
"281";281;"2008-056";"Lumpy Labor Adjustment as a Propagation Mechanism of Business Cycles";"Fang Yao";"C7";"2008-08-11";"E32,  E24,  C68";" I explore the aggregate effects of micro lumpy labor adjustment in a prototypical RBC model, which embeds a stochastic labor duration mechanism in the spirit of Calvo(1983), and it extends this approach by introducing a Weibull-distributed labor adjustment process to capture the increasing hazard function corroborated by the micro data. My principal findings are: The aggregate labor demand equation derived from the baseline Calvostyle model corresponds to the same reduced form as the quadratic-adjustment-cost model and deep parameters have a one-to-one mapping. However, this result does not hold in general. When introducing the Weibull labor adjustment, the aggregate dynamics vary with the extent of increasing hazard function, e.g., the volatility of aggregate labor is increasing, but the persistence is decreasing in degree of the increasing hazard of the labor adjustment."
"282";282;"2008-057";"Measuring changes in preferences and perception due to the entry of a new brand with choice data";"Lutz Hildebrandt and   Lea Kalweit";"B2";"2008-09-18";"M31,  C23,  C51";" Context effects can have a major influence on brand choice behavior after the introduction of a new product. Based on behavioral literature, several hypotheses about the effects of a new brand on perception, preferences and choice behavior can be derived, but studies with real choice data are still lacking. We employ an internal market structure analysis to measure context effects caused by a new product in scanner panel data, and to discriminate between alternative theoretical explanations. An empirical investigation reveals strong support for categorization effects and changes in perception, which affect customers in two out of five segments."
"283";283;"2008-058";"Statistics E-learning Platforms Evaluation: Case Study";"Taleb Ahmad and   Wolfgang Härdle";"B1";"2008-08-31";"I21,  C19";" With the increase of e-learning by universities and educational institutes in the world through more electronic platforms, come the questions to researchers, educators and designers of electronic platforms about feasibility and using this method of learning. Are we achieving the desired goals and improving the quality of education? Are we improving their performance and ability to self-study without the need for a teacher? Is e-learning an effective and successful method from the students views? In this paper, we consider evaluate e-learning systems in statistics. We make an evaluation study, we analyze a students sample of the methods: Factor analysis, Logit model. The common aim of this evaluation is to provide data to justify the results or evidence to support that the e-learning platforms are helping the students to learn more effectively. The questionnaire covers information about e-learning evaluation criterias. Some of these criterias are: Navigability, applicability, instructional structure and interactivity."
"284";284;"2008-059";"The Influence of the Business Cycle on Mortality";"Wolfgang H. Reichmuth and  Samad Sarferaz";"C5";"2008-09-19";"C11,  C32,  E32,  I10,  J10";" We analyze the impact of short-run economic fluctuations on age-specific mortality using 	  Bayesian time series econometrics and contribute to the debate on the procyclicality of mortality. For the first time, we examine the differing consequences of economic changes 	 for all individual age classes. We employ a recently developed model to set up structural   VARs of a latent mortality variable and of unemployment and GDP growth as main 	  business cycle indicators. We find that young adults noticeably differ from the rest of 	  the population. They exhibit increased mortality in a recession, whereas most of the 	 other age classes between childhood and old age react with lower mortality to increased 	  unemployment or decreased GDP growth. In order to avoid that opposed effects may 	  cancel each other, our findings suggest to differentiate closely between particular age 	  classes, especially in the age range of young adults. The results for the U.S. in the period 	  1956Â–2004 are confirmed by an international comparison with France and Japan. Long-	  term changes in the relationship between macroeconomic conditions and mortality are   investigated with data since 1933."
"285";285;"2008-060";"Matching Theory and Data:Bayesian Vector Autoregression and Dynamic Stochastic General Equilibrium Models";"Alexander Kriwoluzky";"C1";"2008-09-19";"C51";" This paper shows how to identify the structural shocks of a Vector Autore-gression (VAR) while at the same time estimating a dynamic stochastic general equilibrium (DSGE) model that is not assumed to replicate the data generating process. It proposes a framework to estimate the parameters of the VAR model 	 and the DSGE model jointly: the VAR model is identified by sign restrictions 	  derived from the DSGE model; the DSGE model is estimated by matching the 	  corresponding impulse response functions."
"286";286;"2008-061";"Eine Analyse der Dimensionen des Fortune-Reputationsindex";"Lutz Hildebrandt, Henning Kreis and  Joachim Schwalbach";"B2";"2008-09-26";"M10,  M14,  M30";" Der vorliegende Beitrag beschÃ¤ftigt sich mit der Dimensionsstruktur sowie der 	  Verwertbarkeit des Fortune-Reputationsrankings in der wissenschaftlichen Forschung und 	  liefert Erkenntnisse Ã¼ber seine AussagefÃ¤higkeit. Nach ErÃ¶rterung der theoretischen 	  Grundlagen von Unternehmensreputation als Konstrukt und Asset sowie einem kurzen 	  Ãœberblick zu MessansÃ¤tzen von Unternehmensreputation liegt der Schwerpunkt des Beitrags 	  auf der Untersuchung der ValiditÃ¤t des Fortune-Reputationsindex. Auf Basis von 	 Untersuchungen dieses Ansatzes mit konfirmatorischen Faktoranalysen werden Fragen zur 	  wissenschaftlichen Verwertbarkeit des Index diskutiert und empirisch Ã¼berprÃ¼ft. Als Ergebnis 	  ist insbesondere festzustellen, dass sich die Dimensionsstruktur des Index Ã¼ber die Zeit 	 verÃ¤ndert hat und eine zunehmende Bedeutung von nicht-finanziellen Faktoren, bei der 	  Beurteilung von Unternehmen zu verzeichnen ist."
"287";287;"2008-062";"Nonlinear Modeling of Target Leverage with Latent Determinant Variables - New Evidence on the Trade-off Theory";"Ralf Sabiwalsky";"D";"2008-09-26";"G32,  G33,  C61";" The trade-off theory on capital structure is tested by modelling the capital structure target as 	  the solution to a maximization problem. This solution maps asset volatility and loss given 	  default to optimal leverage. By applying nonlinear structural equation modelling, these unobservable variables are estimated based on observable indicator variables, and 	  simultaneously, the speed of adjustment towards this leverage target is estimated. Linear 	  specifications of the leverage target suffer from overlap between the predictions of various 	  theories on capital structure about the sign and significance of determinants. In contrast, the 	  framework applied here allows for a direct test: results confirm the trade-off theory for small 	  and medium-sized firms, but not for large firms."
"288";288;"2008-063";"Discrete-Time Stochastic Volatility Models and MCMC-Based Statistical Inference";"Nikolaus Hautsch and  Yangguoyi Ou";"B8";"2008-10-01";"C15,  C22,  G12";" In this paper, we review the most common specifications of discrete-time stochastic volatility (SV) models and illustrate the major principles of corresponding Markov 	  Chain Monte Carlo (MCMC) based statistical inference. We provide a hands-on ap	  proach which is easily implemented in empirical applications and financial practice and 	  can be straightforwardly extended in various directions. We illustrate empirical results 	  based on different SV specifications using returns on stock indices and foreign exchange	rates."
"289";289;"2008-064";"A note on the model selection risk for ANOVA based adaptive forecasting of the EURIBOR swap term structure";"Oliver Blaskowitz and  Helmut Herwartz";"C2";"2008-10-21";"C32,  C53,  E43,  G29";" The paper proposes a data driven adaptive model selection strategy. The selection crite-	  rion measures economic exÂ–ante forecasting content by means of trading implied cash flows. 	  Empirical evidence suggests that the proposed strategy is neither exposed to selection bias 	  nor to the risk of choosing excessively poor models from a parameterized class of candidate 	  specifications."
"290";290;"2008-065";"When, How Fast and by How Much do Trade Costs change in the Euro Area?";"Helmut Herwartz and   Henning Weber";"C3";"2008-11-05";"C31,  C33,  F13,  F15,  F33,  ";" Microfoundations of the euroÂ’s effect on euro area trade hinge on the timing, the 	  speed and the size of adjustment in trade costs. We estimate timing, speed and size 	of adjustment in trade costs for sectoral trade data. Our approach allows for sector 	  specific impacts of trade costs on sectoral trade while controlling for unobserved but 	  time-variant variables at the sector level. We find that, due to falling trade costs, 	  trade within the euro area increases between the years 2000 and 2003 by 10 to 20 	  percent compared with trade between European countries that are not members of 	the euro area. Adjustment of individual sectors is extremely fast whereas aggregate 	  adjustment spreads out because different sectors adjust at distinct times."
"291";291;"2008-066";"The U.S. Business Cycle, 1867-1995:Dynamic Factor Analysis vs. Reconstructed National Accounts";"Albrecht Ritschl, Samad Sarferaz and  Martin Uebele";"C5";"2008-11-25";"N11,  N12,  C43,  E32";" This paper presents insights on U.S. business cycle volatility since 1867 de-	  rived from diffusion indices. We employ a Bayesian dynamic factor model to 	  obtain aggregate and sectoral economic activity indices. We find a remarkable 	  increase in volatility across World War I, which is reversed after World War II. 	  While we can generate evidence of postwar moderation relative to pre-1914, 	  this evidence is not robust to structural change, implemented by time-varying 	  factor loadings. We do find evidence of moderation in the nominal series, 	  however, and reproduce the standard result of moderation since the 1980s. 	  Our estimates broadly confirm the NBER historical business cycle chronology 	  as well the National Income and Product Accounts, except for World War II 	  where they support alternative estimates of Kuznets (1952)."
"292";292;"2008-067";"Testing Multiplicative Error Models Using Conditional Moment Tests";"Nikolaus Hautsch";"B8";"2008-12-02";"C12,  C22,  C52";" We suggest a robust form of conditional moment test as a constructive test for functional misspecification in multiplicative error models. The proposed test has power 	  solely against violations of the conditional mean restriction but is not affected by any 	  other type of model misspecification. Monte-Carlo investigations show that an appro  priate choice of weighting function induces high power against various alternatives. We 	  illustrate how to adapt the framework to test also out-of-sample moment restrictions, 	  such as orthogonalities of prediction errors."
"293";293;"2008-068";"Understanding West German Economic Growth in the 1950s";"Barry Eichengreen and  Albrecht Ritschl";"C5";"2008-12-02";"N14,  N44,  O52";" We evaluate explanations for why Germany grew so quickly in the 1950s. The recent literature has emphasized convergence, structural change and institutional shake-up while minimizing the importance of the postwar shock. We show that this shock and its consequences were 	  more important than neoclassical convergence and structural change in explaining the rapid 	  growth of the West German economy in the 1950s. We find little support for the hypothesis of institutional shakeup. This suggests a different interpretation of post-World War II German 	  economic growth than features in much of the literature."
"294";294;"2008-069";"Structural Dynamic Conditional Correlation";"Enzo Weber";"C6";"2008-12-18";"C32,   G10";" In the literature of identifcation through autoregressive conditional heteroscedasticity, Weber (2008) developed the structural constant conditional correlation (SCCC) model. Besides determining linear simultaneous in uences between several variables, this model considers interaction in the structural innovations. Even though this allows for common fundamental driving forces, these cannot explain time variation in correlations of observed variables, which still have to rely on causal transmission eects. In this context, the present paper extends the analysis to structural dynamic conditional correlation (SDCC). The additional fexibility is shown to make an important contribution in the estimation of empirical real-data examples."
"295";295;"2008-070";"A Brand Specific Investigation of International Cost Shock Threats on Price and Margin with a Manufacturer-Wholesaler-Retailer Model";"Till Dannewald and  Lutz Hildebrandt";"B2";"2008-12-18";"M31,  F12,  L66,  F14,  L13";" In times of increasing oil prices and a weak dollar,  European companies that focus their business on the US market may find themselves in a weak position. While many businesses can hedge this kind of risk by relocating production to the US, or employing financial remedies,  these strategies may not work throughout the consumer goods industry.  Especially for brands whose consumption is strongly impacted by country of origin  (e.g. French whine, Swiss chocolate, German beer, etc.), there are only limited possibilities to bypass these  challenges. To react efficiently to these threats, managers need a precise picture of complete market mechanisms  before they can set up an appropriate marketing strategy to react. We aim to enhance the understanding of market  mechanisms that are caused by exogenous cost shocks for typical consumer goods. The contribution of our work is  twofold: To investigate the underlying process and to derive concrete managerial suggestions. We hereby propose a  combination of two different empirical frameworks to measure the effects of exchange rate variations in fast moving  consumer markets. Furthermore we extend existing work in being the first to model vertical interactions with a  Manufacturer-Wholesaler-Retailer Model. Within this framework we investigate how changes in local currency affect  the strategic management variables of price, margin and profit in a typical consumer goods market. While it is widely  known that exchange rate changes cause variations in export/import prices and numerous studies show that the effect  of currency fluctuations decreases within the distribution process, recent marketing research in this area has not  explicitly accounted for the mechanisms that occur within the distribution channel. Many empirical studies implicate  that exogenous cost shocks, which are caused by exchange rate changes, are passed through imperfectly to final  consumer prices. We therefore show that the margins of the players involved "
"296";296;"2008-071";"Winners and Losers of Early Elections: On the Welfare Implications of Political Blockades and Early Elections";"Felix Bierbrauer and  Lydia Mechtenberg";"A6";"2008-12-18";"D72,  D61,  D82";" We develop a dynamic model of political competition. Each party has a policymotivated ideological wing and an office-motivated opportunistic wing. A blockade arises if inner-party conflict stops policy implementation. We use this model to study whether early elections should be used to overcome a blockade. They have the advantage that urgent decisions are no longer delayed, and the disadvantage that unsuccessful governments gain additional time in office. This may give rise to a time inconsistency. Voters are in favour of a constitution without early elections. However, in the middle of a political crisis, they are willing to abandon it."
"297";297;"2008-072";"Common Influences, Spillover and Integration in Chinese Stock Markets";"Enzo Weber and  Yanqun Zhang";"C6";"2008-12-29";"C32,  G10";" The Chinese stock market features an interesting history of divided market segments: domestic (A), foreigners' (B) and overseas (H). This puts forth questions of market integration as well as cross-divisional information transmission. We address these issues in a structural DCC framework, an econometric technique capable of identifying common factor in uences from (bi-directional) spillovers as constituents of contemporaneous correlations. We find initial dominance of transmission from A to B and to a lesser extent from H to B and A to H. However, since the opening of the B-market for Chinese citizens in 2001, common factors have largely replaced direct spillovers."
"298";298;"2008-073";"Testing directional forecast value in the presence of serial correlation";"Oliver Blaskowitz and  Helmut Herwartz";"C2";"2008-12-29";"C32,  C52,  C53,  E17,  E27,  ";" Common approaches to test for the economic value of directional forecasts are based on the classical Chi-square test for independence, FisherÂ’s exact test or the Pesaran and Timmerman (1992) test for market timing. These tests are asymptotically valid for serially independent observations. Yet, in the presence of serial correlation they are markedly oversized as confirmed in a simulation study. We summarize serial correlation robust test procedures and propose a bootstrap approach. By means of a Monte Carlo study we illustrate the relative merits of the latter. Two empirical applications demonstrate the relevance to account for serial correlation in economic time series when testing for the value of directional forecasts."
"299";299;"2008-074";"A model of reciprocity: Explaining cooperation in groups";"Biele,  G.,  Rieskamp,  J.,  Czienskowski and   U";"A12";"2008-12-31";"";"  "
"300";300;"2009-001";"Implied Market Price of Weather Risk";"Wolfgang Härdle and  Brenda López Cabrera";"B1";"2009-01-01";"G19,   G29,   N26,   N56,   Q2";" Weather influences our daily lives and choices and has an enormous impact on cooperate revenues and earnings. Weather derivatives differ from most derivatives in that the underlying weather cannot be traded and their market is relatively illiquid. The weather derivative market is therefore incomplete. This paper  implements a pricing methodology for weather derivatives that can increase the precision of measuring weather risk. We applied continous autoregressive models (CAR) with seasonal variation to model the temperature  in Berlin and with that to get explicite nature of non-arbitrage prices for temperature derivatives. We infer the implied market price from Berlin cumulative monthly temperature futures that are traded at the Chicago Mercantile Exchange (CME), which is an important parameter of the associated equivalent martingale measures used to price and hedge weather future/options in the market. We propose to study the market price of risk, not only as a piecewise constant linear function, but also as a time dependent. In any of the previous cases, we found that the market price of weather risk is different from zero and shows a seasonal structure. With the extract information we price other exotic options, such as cooling/heating degree day temperatures and non standard contract with crazy maturities."
"301";301;"2009-002";"On the Systemic Nature of Weather Risk";"Guenther Filler, Martin Odening, Ostap Okhrin and  Wei Xu";"B10C11";"2009-01-02";"C14,  Q19";" Systemic weather risk is a major obstacle for the formation of private (non- subsidized) crop insurance. This paper explores the possibility of spatial diversification of insurance by estimating the joint occurrence of unfavorable weather conditions in different locations. For that purpose copula methods are employed that allow an adequate description  of stochastic dependencies between multivariate random variables. The estimation procedure is applied to weather data in Germany. Our results indicate that indemnity payments based on temperature as well as on cumulative rainfall show strong stochastic dependence even at a national scale. Thus the possibility to reduce risk exposure by increasing the trading area of the insurance is limited. Irrespective of their economic implications our results pinpoint the necessity of a proper statistical modeling of the dependence  structure of multivariate random variables. The usual approach of measuring stochastic dependence with linear correlation coefficients turned out to be questionable in the context of weather insurance as it may overestimate diversification effects considerably."
"302";302;"2009-003";"Localized Realized Volatility Modelling";"Ying Chen, Wolfgang Härdle and  Uta Pigorsch";"B1";"2009-01-22";"";" With the recent availability of high-frequency Financial data the long 	range dependence of volatility regained researchers' interest and has lead 	to the consideration of long memory models for realized volatility. The 	long range diagnosis of volatility, however, is usually stated for long sample 	periods, while for small sample sizes, such as e.g. one year, the volatility 	dynamics appears to be better described by short-memory processes. The 	ensemble of these seemingly contradictory phenomena point towards short 	memory models of volatility with nonstationarities, such as structural 	breaks or regime switches, that spuriously generate a long memory pattern 	(see e.g. Diebold and Inoue, 2001; Mikosch and Starica, 2004b). In this 	paper we adopt this view on the dependence structure of volatility and 	propose a localized procedure for modeling realized volatility. That is 	at each point in time we determine a past interval over which volatility 	is approximated by a local linear process. Using S&P500 data we find 	that our local approach outperforms long memory type models in terms 	of predictability. "
"303";303;"2009-004";"New recipes for estimating default intensities";"Alexander Baranovski, Carsten von Lieres and  André Wilch";"B7";"2009-01-22";"C13,  C20,  C22";" This paper presents a new approach to deriving default intensities from CDS or bond spreads that yields smooth intensity curves required e.g. for pricing or risk management purposes. Assuming continuous premium or coupon payments, the default intensity can be obtained by solving an integral equation (Volterra equation of 2nd kind). This integral equation is shown to be equivalent to an ordinary linear differential equation of 2nd order with time dependent coefficients, which is numerically much easier to handle. For the special case of Nelson Siegel CDS term structure models, the problem permits a fully analytical solution. A very good and at the same time simple approximation to this analytical solution is derived, which serves as a recipe for easy implementation. Finally, it is shown how the new approach can be employed to estimate stochastic term structure models like the CIR model. "
"304";304;"2009-005";"Panel Cointegration Testing in the Presence of a Time Trend";"Bernd Droge and  Deniz Dilan Karaman Örsal";"C2";"2009-01-22";"C33,  C12,  C15";" The purpose of this paper is to propose a new likelihood-based panel cointegration test in the presence of a linear time trend in the data generating process. This new test is an extension of the likelihood ratio (LR) test of Saikkonen & LÃ¼tkepohl (2000) for trend-adjusted data to the panel data framework, and is called the panel SL test. The idea is first to take the average of the individual LR (trace) statistics over the cross-sections and then to standardize the test statistic with the appropriate asymptotic moments. Under the null hypothesis, this standardized statistic has a limiting normal distribution as the number of time periods (T) and the number of cross-sections (N) tend to infinity sequentially. In addition to the approximation based on asymptotic moments, a second approximation approach involving the moments from a vector autoregressive process of order one is also introduced. By means of a Monte Carlo study the finite sample size and size-adjusted power properties of the test are investigated. The test presents reasonable size with the increase in T and N, and has high power in small samples. "
"305";305;"2009-006";"Regulatory Risk under Optimal Incentive Regulation";"Roland Strausz";"A8";"2009-01-22";"L51,  D82";" The paper provides a tractable, analytical framework to study regulatory risk under optimal incentive regulation. Regulatory risk is captured by uncertainty about the policy variables in the regulatorÂ’s objective function: weights attached to profits and costs of public funds. Results are as follows: 1) The regulatorÂ’s reaction to regulatory risk depends on the curvature of the aggregate demand function. 2) It yields a positive information rent effect exactly when demand is convex. 3) Firms benefit from regulatory risk exactly when demand is convex. 4) ConsumersÂ’ risk preferences tend to contradict the firmÂ’s. 5) Benevolent regulators always prefer regulatory risk and these preferences may contradict both the firmÂ’s and consumersÂ’ preferences."
"306";306;"2009-007";"Combination of multivariate volatility forecasts";"Alessandra Amendola and  Giuseppe Storti";"Z";"2009-01-28";"C52,  C53,  C32,  G11,  G17";" This paper proposes a novel approach to the combination of conditional covariance matrix forecasts based on the use of the Generalized Method of Moments (GMM). It is shown how the procedure can be generalized to deal with large dimensional systems by means of a two-step strategy. The finite sample properties of the GMM estimator of the combination weights are investigated by Monte Carlo simulations. Finally, in order to give an appraisal of the economic implications of the combined volatility predictor, the results of an application to tactical asset allocation are presented."
"307";307;"2009-008";"Mortality modeling: Lee-Carter and the macroeconomy";"Katja Hanewald";"B9";"2009-01-28";"C32,  E32,  I12,  J11";" Using data for six OECD countries, this paper studies the effect of macroeconomic  conditions on the mortality index kt in the well-known Lee-Carter model.  Significant correlations are found with real GDP growth rates in Australia, Canada, and the United States,  and with unemployment rate changes in Japan, for the period 1950Â–2005. In recent years, the relationship  between the state of the economy and mortality is found to change from procyclical to countercyclical   in all six countries. Based on these findings, variants of the Lee-Carter model are proposed that capture   a substantial fraction of the variation in the mortality index."
"308";308;"2009-009";"Stochastic Population Forecast for Germany and its Consequence for the German Pension System";"Wolfgang Härdle and  Alena Mysickova";"B1";"2009-02-17";"J11,  J13,  C53,  C22";" opulation forecasts are crucial for many social, political and economic decisions. Official population projections rely in general on deterministic models which use different scenarios for future vital rates to indicate uncertainty. However, this technique shows substantial weak points such as assuming absolute correlations between the demographic components. In this paper, we argue that a stochastic projection alternative, with no a priori assumptions provides point forecasts and probabilistic prediction intervals for demographic parameters in addition. Age-sex specific population forecast for Germany is derived through a stochastic population renewal process using forecasts of mortality, fertility and migration. Time series models with demographic restrictions are used to describe immigration, emigration and time varying indices of mortality and fertility rates. These models are then used in the simulation of future vital rates to obtain age-specific population forecast using the cohort-component method. The consequence for the German pension system is discussed. To maintain the actual average pension level the premium rate of the present system rises at least by 50% as the old-age ratio nearly doubles by 2040. "
"309";309;"2009-010";"A Microeconomic Explanation of the EPK Paradox";"Wolfgang Härdle, Volker Krätschmer and  Rouslan Moro";"B1";"2009-02-17";"D01,  D58,  C02,  G13";" Supported by several recent investigations the empirical pricing kernel paradox might be considered as a stylized fact. In Chabi-Yo et al. (2008) simulation studies have been presented which suggest that this paradox might be caused by regime switching of stock prices in financial markets. Alternatively, we want to emphasize a microeconomic view. Based on an economic model with state dependent utilities for the financial investors we succeed in explaining the paradox by changes of the risk attitudes. Theoretically, the change behaviour is compressed by the pricing kernels. As a starting point for empirical insights we shall develop and investigate inverse problems in terms of data fits for estimated basic values of the pricing kernel."
"310";310;"2009-011";"Defending Against Speculative Attacks";"Tijmen Daniëls, Henk Jager and  Franc Klaassen";"C10";"2009-02-17";"E58,  F31,  F33,  G15";" While virtually all currency crisismodels recognise that the fate of a currency peg depends on how tenaciously policy makers defend it, they seldom model how this is done. We incorporate themechanics of speculation and the interest rate defence against it in the model ofMorris and Shin (American Economic Review 88, 1998). Our model captures that the interest rate defence reduces speculatorsÂ’ profits and thus postpones the crisis. It predicts that well before the fall of a currency interest rates are increased to offset the buildup of exchange market pressure, and this then unravels in a sharp depreciation. This pattern is at odds with predictions of standard models, but we show that it fits well with reality."
"311";311;"2009-012";"On the Existence of the Moments of the Asymptotic Trace Statistic";"Deniz Dilan Karaman Örsal and  Bernd Droge";"C2";"2009-02-20";"C32,  C33,  C12";" In this note we establish the existence of the first two moments of the asymptotic trace statistic, which appears as weak limit of the likelihood ratio statistic for testing the cointe- gration rank in a vector autoregressive model and whose moments may be used to develop panel cointegration tests. Moreover, we justify the common practice to approximate these moments by simulating a certain statistic, which converges weakly to the asymptotic trace statistic. To accomplish this we show that the moments of the mentioned statistic converge to those of the asymptotic trace statistic as the time dimension tends to infinity. "
"312";312;"2009-013";"CDO Pricing with Copulae";"Barbara Choros, Wolfgang Härdle and  Ostap Okhrin";"B1B10";"2009-03-06";"C14,  G12,  G13";" Modeling the portfolio credit risk is one of the crucial issues of the last years in the financial problems. We propose the valuation model of Collateralized Debt Obligations based on a one- and two-parameter copula and default intensities estimated from market data. The presented method is used to reproduce the spreads of the iTraxx Europe tranches. The two-parameter model incorporates the fact that the risky assets of the CDO pool are chosen from six different industry sectors. The dependency among the assets from the same group is described with the higher value of the copula parameter, otherwise the lower value of the parameter is ascribed. Our approach outperforms the standard market pricing procedure based on the Gaussian distribution."
"313";313;"2009-014";"Properties of Hierarchical Archimedean Copulas";"Ostap Okhrin, Yarema Okhrin and  Wolfgang Schmid";"B10";"2009-03-06";"C16,  C46";" In this paper we analyse the properties of hierarchical Archimedean copulas. This class is a generalisation of the Archimedean opulas and allows for general non-exchangeable dependency structures. We show that the structure of the copula can be uniquely recovered from all bivariate margins. We derive the distribution of the copula value, which is particularly useful for tests and constructing confidence intervals. Furthermore, we analyse dependence orderings, multivariate dependence measures and extreme value copulas. Special attention we pay to the tail dependencies and derive several tail dependence indices for general hierarchical Archimedean copulas."
"314";314;"2009-015";"Stochastic Mortality, Macroeconomic Risks, and Life Insurer Solvency";"Katja Hanewald, Thomas Post and  Helmut Gründl";"B9";"2009-03-17";"G22,  G23,  G28,  G32,  E32,  ";" Motivated by a recent demographic study establishing a link between macroeconomic fluctuations and the mortality index kt in the Lee-Carter model, we assess the impact of macroeconomic fluctuations on the solvency of a life insurance company. Liabilities in our stochastic simulation framework are driven by a GDP-linked variant of the Lee-Carter mortality model. Furthermore, interest rates and stock prices are allowed to react to changes in GDP, which itself is modeled as a stochastic process. Our results show that insolvency probabilities are significantly higher when the reaction of mortality rates to changes in GDP is incorporated. "
"315";315;"2009-016";"Men, Women, and the Ballot Woman Suffrage in the United States";"Sebastian Braun and  Michael Kvasnicka";"C7";"2009-03-17";"D72,  J16,  K10,  N41,  N42";" Woman suffrage led to the greatest enfranchisement in the history of the United States. Before World War I, however, suffrage states remained almost exclusively confined to the American West. The reasons for this pioneering role of the West are still unclear. Studying the timing of woman suffrage adoption at state level, we find that states in which women were scarce (the West) enfranchised their women much earlier than states in which the sex ratio was more balanced (the rest of the country). High sex ratios in the West, that is high ratios of grantors to grantees, reduced the political costs and risks to male electorates and legislators of extending the franchise. They are also likely to have enhanced female bargaining power and may have made woman suffrage more attractive in the eyes of western legislators that sought to attract more women to their states. Our finding of a reduced-form inverse relationship between the relative size of a group and its success in securing the ballot may be of use also for the study of other franchise extensions and for inquieries into the dynamics of political power sharing more generally. "
"316";316;"2009-017";"The Importance of Two-Sided Heterogeneity for the Cyclicality of Labour Market Dynamics";"Ronald Bachmann and  Peggy David";"C7";"2009-03-17";"J63,  J64,  J21,  E24";" Using two data sets derived from German administrative data, including a linked employer-employee data set, we investigate the cyclicality of worker and job flows. The analysis stresses the importance of two-sided labour market heterogeneity in this context, taking into account both observed and unobserved characteristics. We find that small firms hire mainly unemployed workers, and that they do so at the beginning of an economic expansion. Later on in the expansion, hirings more frequently result from direct job-to-job transitions, with employed workers moving to larger firms. Contrary to our expectations, workers moving to larger firms do not experience significantly larger wage gains than workers moving to smaller establishments. Furthermore, our econometric analysis shows that the interaction of unobserved heterogeneities on the two sides of the labour market plays a more important role for employed job seekers than for the unemployed."
"317";317;"2009-018";"Transparency through Financial Claims with Fingerprints – A Free Market Mechanism for Preventing Mortgage Securitization Induced Financial Crises";"Helmut Gründl and  Thomas Post";"B9";"2009-03-17";"D53,  E44F34,  G14,  G18,  G21";" Lack of transparency in securitization transactions significantly contributed to the severe financial crisis of 2007Â–2009. To increase transparency we propose a new mechanism: financial claims with fingerprints. They would allow market participants at each stage of the securitization process to obtain easily full information about the underlying original risks and the superior claims that need to be satisfied before receiving their own payoffs. The fingerprint mechanism would considerably enhance transparency in securitization transactions at the expense of some transaction costs, while reducing the need for government involvement in securitization markets."
"318";318;"2009-019";"A Joint Analysis of the KOSPI 200 Option and ODAX Option Markets Dynamics";"Ji Cao, Wolfgang Härdle and  Julius Mungo";"B1";"2009-03-23";"C14,  G12";" As a function of strike and time to maturity the implied volatility estimation is a challenging task in financial econometrics. Dynamic Semiparametric Factor Models (DSFM) are a model class that allows for the estimation of the implied volatility surface (IVS) in a dynamic context, employing semiparametric factor functions and time-varying loadings. Because financial asset volatilities move over time, across assets and over markets, this paper analyses volatility interaction between German and Korean stock markets. As proxy for the volatility, factor loadings series derived from a DSFM application on option prices are employed. We examine volatility transmission between the markets under the vector autoregressive (VAR) model framework. Our results show that a shock in the volatility of one market may not translate directly into greater uncertainty in another market and it is unlikely that portfolio investors can benefit from diversification among these markets due to cointegration."
"319";319;"2009-020";"Putting Up a Good Fight: The Galí-Monacelli Model versus “The Six Major Puzzles in International Macroeconomics”";"Stefan Ried";"C1";"2009-04-15";"F41,  F42,  E52";" In this paper, the following question is posed: Can the New Keynesian Open Economy Model by GalÃ­ and Monacelli (2005b) explain Â“Six Major Puzzles in International MacroeconomicsÂ”, as documented in Obstfeld and Rogoff (2000b)? "
"320";320;"2009-021";"Spectral estimation of the fractional order of a Lévy process";"Denis Belomestny";"B7";"2009-04-15";"C12,  C13";" We consider the problem of estimating the fractional order of a LÂ´evy process from low frequency historical and options data. An estimation methodology is developed which allows us to treat both estimation and calibration problems in a unified way. The corresponding procedure consists of two steps: the estimation of a conditional characteristic function and the weighted least squares estimation of the fractional order in spectral domain. While the second step is identical for both calibration and estimation, the first one depends on the problem at hand. Minimax rates of convergence for the fractional order estimate are derived, the asymptotic normality is proved and a data-driven algorithm based on aggregation is proposed. The performance of the estimator in both estimation and calibration setups is illustrated by a simulation study."
"321";321;"2009-022";"Individual Welfare Gains from Deferred Life-Annuities under Stochastic Lee-Carter Mortality";"Thomas Post";"B9";"2009-04-21";"D14,  D81,  D91,  G11,  G22,  ";" A deferred annuity typically includes an option-like right for the policyholder. At the end of the deferment period, he may either choose to receive annuity payouts, calculated based on a mortality table agreed to at contract inception, or receive the accumulated capital as a lump sum. Considering stochastic mortality improvements, such an option could be of substantial value. Whenever mortality improves less than originally expected, the policyholder will choose the lump sum and buy an annuity on the market granting him a better price. If, however, mortality improves more than expected, the policyholder will choose to retain the deferred annuity. We use a realistically calibrated life-cycle consumption/saving/asset allocation model and calculate the welfare gains of deferred annuities under stochastic Lee- Carter mortality. Our results are relevant both for individual retirement planning and for policymakers, especially if legislation makes annuitization, at least in part, mandatory. Our results also indicate the maximal willingness to pay for the mortality option inherent in deferred annuities, which is of relevance to insurance pricing."
"322";322;"2009-023";"Pricing Bermudan options using regression: optimal rates of convergence for lower estimates";"Denis Belomestny";"B7";"2009-04-21";"G14,  C15";" The problem of pricing Bermudan options using Monte Carlo and a nonparametric regression is considered. We derive optimal nonasymptotic bounds for a lower biased estimate based on the suboptimal stopping rule constructed using some estimates of continuation values. These estimates may be of different nature, they may be local or global, with the only requirement being that the deviations of these estimates from the true continuation values can be uniformly bounded in probability."
"323";323;"2009-024";"Incorporating the Dynamics of Leverage into Default Prediction";"Gunter Löffler and  Alina Maurer";"Z";"2009-04-24";"G32,  G33";" A firmÂ’s current leverage ratio is one of the core characteristics of credit quality used in statistical default prediction models. Based on the capital structure literature, which shows that leverage is mean-reverting to a target leverage, we forecast future leverage ratios and include them in the set of default risk drivers. The analysis is done with a discrete duration model. Out-of-sample analysis of default events two to five years ahead reveals that the discriminating power of the duration model increases substantially when leverage forecasts are included. We further document that credit ratings contain information beyond the one contained in standard variables but that this information is unrelated to forecasts of leverage ratios. "
"324";324;"2009-025";"Measuring the effects of geographical distance on stock market correlation";"Stefanie Eckel, Gunter Löffler, Alina Maurer and  Volker Schmidt";"Z";"2009-04-24";"R12,  G11,  G14";" Recent studies suggest that the correlation of stock returns increases with decreasing geographical distance. However, there is some debate on the appropriate methodology for measuring the effects of distance on correlation. We modify a regression approach suggested in the literature and complement it with an approach from spatial statistics, the mark correlation function. For the stocks contained in the S&P 500 that we examine, both approaches lead to similar results: correlation increases with decreasing distance. Contrary to previous studies, however, we find that differences in distance do not matter much once the firmsÂ’ headquarters are more than 40 miles apart, or separated through a federal border. Finally, we show through simulations that distance can significantly affect portfolio risk. Investors wishing to exploit local information should be aware that local portfolios are relatively risky."
"325";325;"2009-026";"Regression methods for stochastic control problems and their convergence analysis";"Denis Belomestny, Anastasia Kolodko and  John Schoenmakers";"B7";"2009-05-05";"C15,  C61";" In this paper we develop several regression algorithms for solving general stochastic optimal control problems via Monte Carlo. This type of algorithms is particularly useful for problems with a highdimensional state space and complex dependence structure of the underlying Markov process with respect to some control. The main idea behind the algorithms is to simulate a set of trajectories under some reference measure and to use the Bellman principle combined with fast methods for approximating conditional expectations and functional optimization. Theoretical properties of the presented algorithms are investigated and the convergence to the optimal solution is proved under some assumptions. Finally, the presented methods are applied in a numerical example of a high-dimensional controlled Bermudan basket option in a financial market with a large investor. "
"326";326;"2009-027";"Genetic variation in dopaminergic neuromodulation influences the ability to rapidly and flexibly adapt decisions";"Krugel,  L. K.,  Biele,  G.,  Mohr,  P. N. C.,  Li,  S. C.,  & Heekeren and   H. R.";"A12";"2009-12-31";"";" his paper studies how different unionisation structures affect firm productivity, firm performance, and consumer welfare in a monopolistic competition model with heterogeneous firms and free entry. While centralised bargaining induces tougher selection among hetero- geneous producers and thus increases average productivity, firm-level bargaining allows less productive entrants to remain in the market. Centralised bargaining also results in higher average output and profit levels than either decentralised bargaining or a competitive labour market. From a welfare perspective, the choice between centralised and decentralised bar- gaining involves a potential trade-off between product variety and product prices. Extending the model to a two-country setup, I furthermore show that the positive effect of centralised bargaining on average productivity can be overturned when firms face international low-wage competition."
"327";327;"2009-027";"Unionisation Structures, Productivity, and Firm Performance";"Sebastian Braun";"C7";"2009-05-13";"J50,  D43,  F16";" his paper studies how different unionisation structures affect firm productivity, firm performance, and consumer welfare in a monopolistic competition model with heterogeneous firms and free entry. While centralised bargaining induces tougher selection among hetero- geneous producers and thus increases average productivity, firm-level bargaining allows less productive entrants to remain in the market. Centralised bargaining also results in higher average output and profit levels than either decentralised bargaining or a competitive labour market. From a welfare perspective, the choice between centralised and decentralised bar- gaining involves a potential trade-off between product variety and product prices. Extending the model to a two-country setup, I furthermore show that the positive effect of centralised bargaining on average productivity can be overturned when firms face international low-wage competition."
"328";328;"2009-028";"Optimal Smoothing for a Computationally and Statistically Efficient Single Index Estimator";"Yingcun Xia, Wolfgang Härdle and  Oliver Linton";"B1";"2009-05-13";"C00,  C13,  C14";" In semiparametric models it is a common approach to under-smooth the nonparametric functions in order that estimators of the finite dimensional parameters can achieve root-n consistency. The requirement of under-smoothing may result as we show from inefficient estimation methods or technical difficulties. Based on local linear kernel smoother, we propose an estimation method to estimate the single-index model without under-smoothing. Under some conditions, our estimator of the single-index is asymptotically normal and most efficient in the semi-parametric sense. Moreover, we derive higher expansions for our estimator and use them to define an optimal bandwidth for the purposes of index estimation. As a result we obtain a practically more relevant method and we show its superior performance in a variety of applications."
"329";329;"2009-029";"Controllability and Persistence of Money Market Rates along the Yield Curve: Evidence from the Euro Area";"Ulrike Busch and  Dieter Nautz";"C14";"2009-05-15";"C22,   E43,   E52";" Controllability of longer-term interest rates requires that the persistence  of their deviations from the central bank's policy rate (i.e. the policy spreads)  remains sufficiently low. This paper applies fractional integration techniques to assess  the persistence of policy spreads of euro area money market rates along the yield curve.  Independently from anticipated policy rate changes, there is strong evidence for all maturities that policy spreads exhibit long memory. We show that recent changes  in the operational framework and the communication strategy of the European Central Bank  have significantly decreased the persistence of euro area policy spreads and, thus,  have enhanced the central bank's influence on longer-term money market rates."
"330";330;"2009-030";"Non-constant Hazard Function and Inflation Dynamics";"Fang Yao";"C7";"2009-05-29";"E12,  E31";" This paper explores implications of nominal rigidity characterized by a non-constant hazard function for aggregate dynamics. I derive the NKPC under an arbitrary hazard function and parameterize it with the Weibull duration model. The resulting Phillips curve involves lagged inflation and lagged expectations. It nests the Calvo NKPC as a limiting case in the sense that the effects of both terms are canceled out under the constant-hazard assumption. Furthermore, I find lagged inflation always has negative coefficients, thereby making it impossible to interpret inflation persistence as intrinsic. The numerical evaluation shows that the increasing hazard function leads to hump-shaped impulse responses of inÂ‡ation to monetary shocks, and output leads inflation."
"331";331;"2009-031";"De copulis non est disputandum - Copulae: An Overview";"Wolfgang Härdle and  Ostap Okhrin";"B1B10";"2009-05-29";"C13,  C14,  C50";" Normal distribution of the residuals is the traditional assumption in the classical multivariate time series models. Nevertheless it is not very often consistent with the real data. Copulae allows for an extension of the classical time series models to nonelliptically distributed residuals. In this paper we apply different copulae to the calculation of the static and dynamic Value-at-Risk of portfolio returns and Profit-and-Loss function. In our findings copula based multivariate model provide better results than those based on the normal distribution."
"332";332;"2009-032";"Weather-based estimation of wildfire risk";"Joanne Ho and  Martin Odening";"C11";"2009-06-18";"C24,  C25,  Q23,  Q54";" Catastrophic wildfires in California have become more frequent in past decades, while insured losses per event have been rising substantially. On average, California ranks the highest among states in the U.S. in the number of fires as well as the number of acres burned each year. The study of catastrophic wildfire models plays an important role in the prevention and mitigation of such disasters. Accurate forecasts of potential large fires assist fire managers in preparing resources and strategic planning for fire suppression. Furthermore, fire forecasting can a priori inform insurers on potential financial losses due to large fires. This paper describes a probabilistic model for predicting wildland fire risks using the two-stage Heckman procedure. Using 37 years of spatial and temporal information on weather and fire records in Southern California, this model measures the probability of a fire occurring and estimates the expected size of the fire on a given day and location, offering a technique to predict and forecast wildfire occurrences based on weather information that is readily available at low cost."
"333";333;"2009-033";"TFP Growth in Old and New Europe";"Michael C. Burda and  Battista Severgnini";"C7";"2009-06-24";"D24,   O47,   P27";" Using Solow-TÃ¶rnqvist residuals as well as two alternative measurements, we present estimates of total factor productivity (TFP) growth in a sample of 30 European economies for the period 1994-2005. In most of Western Europe, we find a deceleration of TFP growth since 2000. However, the economies of New Europe exhibit a higher level of TFP growth overall and have slowed less than those of Old Europe. In the new market economies of Central and Eastern Europe, we find both high TFP growth as well as acceleration in the second half of the sample. Regression evidence from Western Europe suggests that product market regulation may adversely affect TFP growth and may thus impair convergence."
"334";334;"2009-034";"How does entry regulation influence entry into self-employment and occupational mobility?";"Susanne Prantl and  Alexandra Spitz-Oener";"A9";"2009-06-29";"J24,  J62,  K20,  L11,  L51,  ";" We analyze how an entry regulation that imposes a mandatory educational standard affects entry into self-employment and occupational mobility. We exploit the German reunification as a natural experiment and identify regulatory effects by comparing differences between regulated occupations and unregulated occupations in East Germany to the corresponding differences in West Germany after reunification. Consistent with our expectations, we find that entry regulation reduces entry into selfemployment and occupational mobility after reunification more in regulated occupations in East Germany than in West Germany. Our findings are relevant for transition or emerging economies as well as for mature market economies requiring large structural changes after unforeseen economic shocks."
"335";335;"2009-035";"Trade-Off Between Consumption Growth and Inequality: Theory and Evidence for Germany";"Runli Xie";"C7";"2009-06-29";"J24,  J62,  K20,  L11,  L51,  ";" This paper examines the structure and evolution of consumption and consumption growth inequality. Once heterogeneous agents relate their neighbors' consumption to their own, consumption volatility and inequality are affected. The relationship predicted between the group average consumption growth and within-group growth inequality was shown as only slightly positive yet significant using survey data from the German Socio- Economic Panel (GSOEP, 1984-2005). Age and household size are crucial for withingroup inequality, as young and/or small households are more sensitive to income and consumption shocks. Large and well-educated households with unskilled jobs have shown surprisingly inferior performance in consumption growth and variance. The data also shows increases of within-group inequality directly after the reunification and the introduction of the euro."
"336";336;"2009-036";"Inflation and Growth: New Evidence From a Dynamic Panel Threshold Analysis";"Stephanie Kremer, Alexander Bick and  Dieter Nautz";"C14";"2009-07-07";"E31,  C23,  O40";" We introduce a dynamic panel threshold model to shed new light on the impact of inflation on long-term economic growth. The empirical analysis is based on a large panel-data set including 124 countries during the period from 1950 to 2004. For industrialized countries, our results confirm the inflation targets of about 2% set by many central banks. For non-industrialized countries, we estimate that inflation hampers growth if it exceeds 17%. Below this threshold, however, the impact of inflation on growth remains insignificant. Therefore, our results do not support growth-enhancing effects of inflation in developing countries."
"337";337;"2009-037";"The Impact of the European Monetary Union on Inflation Persistence in the Euro Area";"Barbara Meller and  Dieter Nautz";"C14";"2009-07-07";"C22,  C23,  E31";" This paper uses the European Monetary Union (EMU) as a natural experiment to investigate whether more effective monetary policy reduces the persistence of inflation. Taking into account the fractional integration of inflation, we confirm that inflation dynamics differed considerably across Euro area countries before the start of EMU. Since 1999, however, results obtained from panel estimation indicate that the degree of long run inflation persistence has converged. In line with theoretical predictions, we find that the persistence of inflation has significantly  decreased in the Euro area probably as a result of the more effective monetary policy of the ECB. "
"338";338;"2009-038";"CDO and HAC";"Barbara Choros, Wolfgang Härdle and  Ostap Okhrin";"B1B10";"2009-07-30";"C13,  G12,  G13,  G21";" Modelling portfolio credit risk is one of the crucial challenges faced by financial services industry in the last few years. We propose the valuation model of collateralized debt obligations (CDO) based on copula functions with up to three parameters, with default intensities estimated from market data and with a random loss given default that is correlated with default times. The methods presented are used to reproduce the spreads of the iTraxx Europe tranches. We apply hierarchical Archimedean copulae (HAC) whose construction allows for the fact that the risky assets of the CDO pool are chosen from six different industry sectors. The dependence among the assets from the same group is specified with the higher value of the copula parameter, otherwise the lower value of the parameter is ascribed. The copula with two and three parameters models the relation between the loss given default and the default times. Our approach describes the market prices better than the standard pricing procedure based on the Gaussian distribution."
"339";339;"2009-039";"Regulation and Investment in Network Industries: Evidence from European Telecoms";"Michal Grajek and  Lars-Hendrik Röller";"Z";"2009-07-31";"C51,  L59,  L96";" We provide evidence of an inherent trade-off between access regulation and investment incentives in telecommunications by using a comprehensive data set covering 70+ fixed-line operators in 20 countries over 10 years. Our econometric model accommodates: different investment incentives for incumbents and entrants; a strategic interaction of entrantsÂ’ and incumbentsÂ’ investments; and endogenous regulation. We find access regulation to negatively affect both total industry and individual carrier investment. Thus promoting market entry by means of regulated access undermines incentives to invest in facilities-based competition. Moreover, we find evidence of a regulatory commitment problem: higher incumbentsÂ’ investments encourage provision of regulated access."
"340";340;"2009-040";"The Political Economy of Regulatory Risk";"Roland Strausz";"A8";"2009-08-14";"D82";" I investigate the argument that, in a twoÂ–party system with different regulatory objectives, political uncertainty generates regulatory risk. I show that this risk has a fluctuation effect that hurts both parties and an outputÂ–expansion effect that benefits one party. Consequently, at least one party dislikes regulatory risk. Moreover, both political parties gain from eliminating regulatory risk when political divergence is small or the winning probability of the regulatoryÂ–riskÂ–averse party is not too large. Because of a commitment problem, direct political bargaining is insufficient to eliminate regulatory risk. Politically independent regulatory agencies solve this commitment problem."
"341";341;"2009-041";"Shape invariant modelling pricing kernels and risk aversion";"Maria Grith, Wolfgang Härdle and  Juhyun Park";"B1";"2009-08-20";"C14,   C32,   G12";" Pricing kernels play a major role in quantifying risk aversion and investors' preferences. Several empirical studies reported that pricing kernels exhibit a common pattern across different markets. Mostly visual inspection and occasionally numerically summarise are used to make comparison. With increasing amount of information updated every day, the empirical pricing kernels can be viewed as an object evolving over time. We propose a systematic modelling approach to describing the evolution of the empirical pricing kernels. The approach is based on shape invariant models. It captures the common features contained in the shape of the functions and at the same time characterises the variability between the pricing kernels based on a few interpretable parameters. The method is demonstrated with the European options and returns values of DAX index."
"342";342;"2009-042";"The Cost of Tractability and the Calvo Pricing Assumption";"Fang Yao";"C7";"2009-09-09";"E12,  E31";" This paper demonstrates that tractability gained from the Calvo pricing assumption is costly in terms of aggregate dynamics. I derive a generalized New Keynesian Phillips curve featuring a generalized hazard function, non-zero steady state inflation and real rigidity. Analytically, I find that important dynamics in the NKPC are canceled out due to the restrictive Calvo assumption. I also present a general result, showing that, under certain conditions, this generalized Calvo pricing model generates the same aggregate dynamics as the gen- eralized Taylor model with heterogeneous price durations. The richer dynamic structure introduced by the non-constant hazards is also quantitatively important to the inflation dy- namics. Incorporation of real rigidity and trend inflation strengthen this effect even further. With reasonable parameter values, the model accounts for hump-shaped impulse responses of inflation to the monetary shock, and the real effects of monetary shocks are 2-3 times higher than those in the Calvo model."
"343";343;"2009-043";"Evidence on Unemployment, Market Work and Household Production";"Michael C. Burda and  Daniel S. Hamermesh";"C7";"2009-09-11";"E24,  J22,  D13";" Time-diary data from four countries suggest that differences in market time between the unemployed and employed represent additional leisure and personal maintenance rather than increased household production. U.S. data for 2003-2006 show that almost none of the reduction in market work in areas of long-term high unemployment is offset by additional work at home. In contrast, in those areas where unemployment has risen cyclically, reduced market work is largely substituted by additional time in household production."
"344";344;"2009-044";"Modelling and Forecasting Liquidity Supply Using Semiparametric Factor Dynamics";"Wolfgang Härdle, Nikolaus Hautsch and  Andrija Mihoci";"B1B8";"2009-09-16";"C14,  C32,  C53,  G11";" We model the dynamics of ask and bid curves in a limit order book market using a dynamic semiparametric factor model. The shape of the curves is captured by a factor structure which is estimated nonparametrically. Corresponding factor loadings are assumed to follow multivariate dynamics and are modelled using a vector autoregressive model. Applying the framework to four stocks traded at the Australian Stock Exchange (ASX) in 2002, we show that the suggested model captures the spatial and temporal dependencies of the limit order book. Relating the shape of the curves to variables reflecting the current state of the market, we show that the recent liquidity demand has the strongest impact. In an extensive forecasting analysis we show that the model is successful in forecasting the liquidity supply over various time horizons during a trading day. Moreover, it is shown that the modelÂ’s forecasting power can be used to improve optimal order execution strategies."
"345";345;"2009-045";"Quantifizierbarkeit von Risiken auf Finanzmärkten";"Wolfgang Karl Härdle and   Christian Wolfgang Friedrich Kirchner";"B1";"2009-10-09";"B23,  C14,  G32,  K22";" Die Krise der internationalen FinanzmÃ¤rkte hat die allgemeine Wahrnehmung fÃ¼r die in diesen MÃ¤rkten inhÃ¤renten Risiken merklich verÃ¤ndert. Glaubten manche Anleger in den Boomphasen der FinanzmÃ¤rkte, dass sich eine hohe Kapitalrendite mit geringem Risiko verbinden lieÃŸe, wenn man nur die Finanzprodukte entsprechend gestaltete, hat sich diese Wahnvorstellung zwischenzeitlich verflÃ¼chtigt. Will man vernÃ¼nftig mit diesen Risiken umgehen, ist es notwendig, diese quantifizieren zu kÃ¶nnen. Hier gilt es, eine Reihe methodischer Probleme zu bewÃ¤ltigen, da sich einfache statistische Methodiken als nicht adÃ¤quat fÃ¼r die vielschichtigen Finanzmarktrisiken erweisen. Die Vielschichtigkeit dieser Risiken hat in den letzten Jahrzehnten zugenommen, insbesondere seitdem hypothekengesicherte Darlehen in verbriefter und verpackter Form auf FinanzmÃ¤rkten abgesetzt wurden. Der Fokus der folgenden AusfÃ¼hrungen liegt bei der Quantifizierung der RisikoeinschÃ¤tzungen, und zwar unter Beachtung von Wahrnehmungsproblemen, wie sie in der modernen VerhaltensÃ¶konomik erÃ¶rtert werden. Daneben werden aber auch Probleme des demographischen Risikos angesprochen."
"346";346;"2009-046";"Pricing of Asian temperature risk";"Fred Benth,  Wolfgang Karl Härdle and   Brenda López Cabrera";"B1";"2009-10-09";"G19,  G29,  G22,  N23,  N53,  ";" Weather derivatives (WD) are dierent from most nancial derivatives because the underlying weather cannot be traded and therefore cannot be replicated by other nancial instruments. The market price of risk (MPR) is an important parameter of the associated equivalent martingale measures used to price and hedge weather futures/options in the market. The majority of papers so far have priced non-tradable assets assuming zero MPR, but this assumption underestimates WD prices. We study the MPR structure as a time dependent object with concentration on emerging markets in Asia. We nd that Asian Temperatures (Tokyo, Osaka, Beijing, Teipei) are normal in the sense that the driving stochastics are close to a Wiener Process. The regression residuals of the temperature show a clear seasonal variation and the volatility term structure of CAT temperature futures presents a modied Samuelson eect. In order to achieve normality in standardized residuals, the seasonal variation is calibrated with a combination of a fourier truncated series with a GARCH model and with a local linear regression. By calibrating model prices, we implied the MPR from Cumulative total of 24- hour average temperature futures (C24AT) for Japanese Cities, or by knowing the formal dependence of MPR on seasonal variation, we price derivatives for Kaohsiung, where weather derivative market does not exist. The ndings support theoretical results of reverse relation between MPR and seasonal variation of temperature process."
"347";347;"2009-047";"MM-Stat - MultiMedia-Statistik: Statistische Datenanalyse - webbasiert, interaktiv und multimedial";"Sigbert Klinke, Dina Kuhlee, Christian Theel, Cornelia Wagner and  Christian Westermeier";"B1";"2009-10-16";"I21";" In der Vergangenheit wurden viele (interaktive) Lehrmaterialien auf proprietÃ¤ren Plattformen entwickelt. Mit den Web 2.0 Technologien bieten sich neue interaktive und technische MÃ¶glichkeiten der Darstellung dieser Lehrinhalte auf einer standardisierten Plattform an. Existierende und neue Lehrinhalte im Bereich Statistik, sowohl aus dem Grund- als auch aus dem Hauptstudium, wurden in ein Wiki (http://www.mm-stat.org) Ã¼bertragen bzw. erstellt. In das frei editierbare Wiki wurden Videos, Aufgaben, Bewertungsfunktionen und Softwareprogramme eingebettet."
"348";348;"2009-048";"Migration of the Highly Skilled: Can Europe catch up with the US?";"Lydia Mechtenberg and  Roland Strausz";"A6A8";"2009-10-21";"D61,  H77,  I28";" We develop a model to analyze the determinants and effects of an endogenous imperfect transferability of human capital on natives and immigrants. The model reveals that high migration flows and high skill-transferability are mutually interdependent. Moreover, we show that high mobility within a Federation is necessary to attract highly skilled immigrants into the Federation. We study in how far and in what way the European public policy behind the Bologna and the Lisbon Process can contribute to higher mobility in Europe."
"349";349;"2009-049";"A blocking and regularization approach to high dimensional realized covariance estimation";"Nikolaus Hautsch, Lada M. Kyj and  Roel C.A. Oomen";"B8";"2009-10-21";"C14,  C22";" We introduce a regularization and blocking estimator for well-conditioned high-dimensional daily covariances using high-frequency data. Using the Barndorff-Nielsen, Hansen, Lunde, and Shephard (2008a) kernel estimator, we estimate the covariance matrix block-wise and regularize it. A data-driven grouping of assets of similar trading frequency ensures the reduction of data loss due to refresh time sampling. In an extensive simulation study mimicking the empirical features of the S&P 1500 universe we show that the Â’RnBÂ’ estimator yields efficiency gains and outperforms competing kernel estimators for varying liquidity settings, noise-to-signal ratios, and dimensions. An empirical application of forecasting daily covariances of the S&P 500 index confirms the simulation results."
"350";350;"2009-050";"Generalized single-index models: The EFM approach";"Xia Cui, Wolfgang Karl Härdle and  Lixing Zhu";"B1";"2009-10-28";"C02,  C13,  C14,  C21";" Generalized single-index models are natural extensions of linear models and circumvent the so-called curse of dimensionality. They are becoming increasingly popular in many scientific fields including biostatistics, medicine, economics and finan- cial econometrics. Estimating and testing the model index coefficients beta is one of the most important objectives in the statistical analysis. However, the commonly used assumption on the index coefficients, beta = 1, represents a non-regular problem: the true index is on the boundary of the unit ball. In this paper we introduce the EFM ap- proach, a method of estimating functions, to study the generalized single-index model. The procedure is to first relax the equality constraint to one with (d - 1) components of beta lying in an open unit ball, and then to construct the associated (d - 1) estimating functions by projecting the score function to the linear space spanned by the residuals with the unknown link being estimated by kernel estimating functions. The root-n consistency and asymptotic normality for the estimator obtained from solving the re- sulting estimating equations is achieved, and a Wilk's type theorem for testing the index is demonstrated. A noticeable result we obtain is that our estimator for beta has smaller or equal limiting variance than the estimator of Carroll et al. (1997). A fixed point iterative scheme for computing this estimator is proposed. This algorithm only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high model dimensionality. Numerical studies based on simulation and on applications suggest that this new estimating system is quite powerful and easy to implement."
"351";351;"2009-051";"The Market Impact of a Limit Order";"Nikolaus Hautsch and  Ruihong Huang";"B8";"2009-10-28";"G14,  C32,  G17";" Despite their importance in modern electronic trading, virtually no systematic empirical evidence on the market impact of incoming orders is existing. We quantify the short-run and long-run price effect of posting a limit order by proposing a high-frequency cointegrated VAR model for ask and bid quotes and several levels of order book depth. Price impacts are estimated by means of appropriate impulse response functions. Analyzing order book data of 30 stocks traded at Euronext Amsterdam, we show that limit orders have significant market impacts and cause a dynamic (and typically asymmetric) rebalancing of the book. The strength and  direction of quote and spread responses depend on the incoming ordersÂ’ aggressiveness, their size and the state of the book. We show that the effects are qualitatively quite stable across the market. Cross-sectional variations in the magnitudes of price impacts are well explained by the underlying trading frequency and relative tick size."
"352";352;"2009-052";"On economic evaluation of directional forecasts";"Oliver Blaskowitz and  Helmut Herwartz";"B8";"2009-10-29";"C52,  E17,  E27,  E37,  E47,  ";" It is commonly accepted that information is helpful if it can be exploited to improve a decision mak- ing process. In economics, decisions are often based on forecasts of up{ or downward movements of the variable of interest. We point out that directional forecasts can provide a useful framework to assess the economic forecast value when loss functions (or success measures) are properly formu- lated to account for realized signs and realized magnitudes of directional movements. We discuss a general approach to evaluate (directional) forecasts which is simple to implement, robust to outlying or unreasonable forecasts and which provides an economically interpretable loss/success functional framework. As such, the measure of directional forecast value is a readily available alternative to the commonly used squared error loss criterion."
"353";353;"2009-053";"Monetary Policy Implementation and Overnight Rate Persistence";"Dieter Nautz and  Jan Scheithauer";"C14";"2009-11-09";"E52,  C22";" Overnight money market rates are the predominant operational target of monetary policy. As a consequence, central banks have redesigned the implementation of monetary policy to keep the deviations of the overnight rate from the key policy rate small and short-lived. This paper uses fractional integration techniques to explore how the operational framework of four major central banks affects the persistence of overnight rates. Our results suggest that a well-communicated and transparent interest rate target of the central bank is a particularly important condition for a low degree of overnight rate persistence."
"354";354;"2009-054";"Depression Econometrics: A FAVAR Model of Monetary Policy During the Great Depression";"Pooyan Amir Ahmadi and  Albrecht Ritschl";"Z";"2009-11-09";"N12,  E37,  E47,  E52,  C11,  ";" The prominent role of monetary policy in the U.S. interwar depression has been conventional wisdom since Friedman and Schwartz [1963]. This paper presents evidence on both the surprise and the systematic components of monetary policy between 1929 and 1933. Doubts surrounding GDP estimates for the 1920s would call into question conventional VAR techniques. We therefore adopt the FAVAR methodology of Bernanke, Boivin, and Eliasz [2005], aggregating a large number of time series into a few factors and inserting these into a monetary policy VAR. We work in a Bayesian framework and apply MCMC methods to obtain the posteriors. Employing the generalized sign restriction approach toward identification of Amir Ahmadi and Uhlig [2008], we find the effects of monetary policy shocks to have been moderate. To analyze the systematic policy component, we back out the monetary policy reaction function and its response to aggregate supply and demand shocks. Results broadly confirm the Friedman/Schwartz view about restrictive monetary policy, but indicate only moderate effects. We further analyze systematic policy through conditional forecasts of key time series at critical junctures, taken with and without the policy instrument. Effects are again quite moderate. Our results caution against a predominantly monetary interpretation of the Great Depression."
"355";355;"2009-055";"Representations for optimal stopping under dynamic monetary utility functionals";"Volker Krätschmer and  John Schoenmakers";"B5";"2009-11-09";"C61,  C63,  G12,  G13";" In this paper we consider the optimal stopping problem for general dynamic monetary utility functionals. Sufficient conditions for the Bellman principle and the existence of optimal stopping times are provided. Particular attention is payed to representations which allow for a numerical treatment in real situations. To this aim, generalizations of standard evaluation methods like policy iteration, dual and consumption based approaches are developed in the context of general dynamic monetary utility functionals. As a result, it turns out that the possibility of a particular generalization depends on specific properties of the utility functional under consideration."
"356";356;"2009-056";"Product policy and the East-West productivity gap";"Bernd Görzig, Martin Gornig, Ramona Voshage and  Axel Werwatz";"B3";"2009-11-09";"L25,  D24,  P23,  C14";" After 20 years of transition from an economy integrated in an exchange scheme of planned economies towards an open market economy based on the ideas of competition, we ask whether East German firms succeeded in finding their place in the international division of labour. We concentrate on the question, to what extent they have caught up with the productivity level of their Western counterparts of similar size and sector and how this productivity difference is related to changes in their product policy. We analyse these questions with a unique data set provided by Statistics Germany that contains both product policy and productivity information for individual manufacturers from both parts of the country. Using a decomposition approach suggested by Nopo (2008) as a nonparametric extension of the widely-used Oaxaca-Blinder methodology (Blinder 1973; Oaxaca 1973) we find that the time span from 1995-2004 has two component periods: a period of adaptation from 1995 to 2001and a period of branding from 2002 to 2004. The initial period is characterized by a smaller share of Eastern firms that modify their product range and by a large productivity gap of Eastern non-modifiers if compared to Western non-modifiers of comparable size and sector. The evidence for the second period, however, points to a more active and established role of East German manufacturers: more of them alter their product range and step up their productivity performance."
"357";357;"2009-057";"Real and Nominal Rigidities in Price Setting: A Bayesian Analysis Using Aggregate Data";"Fang Yao";"C7";"2009-11-09";"E12,  E31";" This paper uses the Bayesian approach to solve and estimate a New Keynesian model augmented by a generalized Phillips curve, in which the shape of the price reset hazards can be identiÂ…ed using aggregate data. My empirical result shows that a constant hazard function is easily rejected by the data. The empirical hazard function for post-1983 periods in the U.S. is consistent with micro evidence obtained using data from similar periods. The hazard for pre-1983 periods, however, exhibits a remarkable increasing pattern, implying that pricing decisions are characterized by both time- and state-dependent aspects. Additionally, real rigidity plays an important role, but not as big a role as found in empirical studies using limited information methods."
"358";358;"2009-058";"Polar sets of anisotropic Gaussian random fields";"Jakob Söhl";"C12";"2009-11-18";"G13,  C14";" This paper studies polar sets of anisotropic Gaussian random elds, i.e. sets which a Gaussian random eld does not hit almost surely. The main assumptions are that the eigenvalues of the covariance matrix are bounded from below and that the canonical metric associated with the Gaussian random eld is dominated by an anisotropic metric. We deduce an upper bound for the hitting probabilities and conclude that sets with small Hausdor dimension are polar. Moreover, the results allow for a translation of the Gaussian random eld by a random eld, that is independent of the Gaussian random eld and whose sample functions are of bounded HÃ¶lder norm."
"359";359;"2009-059";"Der Einfluss von Exporten auf die betriebliche Entwicklung";"Stefan Mangelsdorf";"B3";"2009-11-25";"F10,   D21,   L60";" Exporte gelten als Wachstumsmotor der deutschen Wirtschaft und werden von der Wirtschaftspolitik auf vielfÃ¤ltige Weise gefÃ¶rdert. Doch fÃ¼hrt die FÃ¶rderung der Aufnahme von Handelsbeziehungen mit dem Ausland durch Betriebe, die bislang nicht exportierten, wirklich zu einem Wachstum der ProduktivitÃ¤t in der deutschen Wirtschaft? Oder werden besonders produktive Betriebe von sich aus zu Exporteuren und mÃ¼ssen nicht gefÃ¶rdert werden? Dies ist eine sehr interessante Frage fÃ¼r die Wirtschaftspolitik und ihr soll in dieser Arbeit nachgegangen werden."
"360";360;"2009-060";"Renting versus Owning and the Role of Income Risk: The Case of Germany";"Rainer Schulz, Martin Wersing and  Axel Werwatz";"B3";"2009-12-03";"R21,  G11,  J24,  C25";" In a world with complete markets and no transactions cost, the decision whether to rent or buy a home is separate from a household's professional income risk. If markets are incomplete and have frictions, however, profession- specific income risk, regional house price risk, and mobility needs will interact and should affect the tenure mode choice. Using panel data from West Germany, we establish homogeneous profession groups and estimate their regional net income risk and regional mobility. We then examine the impact of the risk and mobility variables on the tenure mode decision at the aggregate and the individual household level. We find that the diversification potential of renting affects the tenure mode choice as do mobility needs."
"361";361;"2009-061";"Is cross-category brand loyalty determined by risk aversion?";"Nadja Silberhorn and  Lutz Hildebrandt";"B2";"2009-12-03";"M31,  C51";" The need to understand and leverage consumer-brand bonds has become critical in a marketplace characterized by increasing unpredictability, diminishing product differentiation, and heightened competitive pressure. This is especially true for fast moving consumer goods (FMCG) manufacturers and retailers. Knowing why a customer stays loyal to a brand in multiple product categories is necessary for deriving suitable marketing strategies in the context of a brand extension, yet research on the motives, characteristics, life styles and attitudes of cross-category brand loyal customers has been investigated only in a limited number of studies. We will fill a gap in the literature on cross-category brand choice behavior by analyzing revealed preference data with respect to brand loyalty in several categories in which a brand competes. Provided with purchase and corresponding survey data we investigate the product portfolio of a leading nonfood FMCG brand. We segment consumers on the basis of their revealed brand preferences and, focusing on consumersÂ’ risk aversion, identify cross-category brand loyal customersÂ’ personality traits as determinants of their brand loyal purchase behavior."
"362";362;"2009-062";"Interest Rate Dynamics and Monetary Policy Implementation in Switzerland";"Puriya Abbassi, Dieter Nautz and  Christian J. Offermanns";"C14";"2009-12-09";"E52,  E58";" The maturity of the operational target of monetary policy is a distinguishing feature of the SNB's operational framework of monetary policy. While most central banks use targets for the overnight rate to signal the policy-intended interest rate level, the SNB announces a target range for the three-month Libor. This paper investigates the working and the consequences of the SNB's unique operational framework for the behavior of Swiss money market rates before and during the financial crisis."
"363";363;"2009-063";"Quantifying High-Frequency Market Reactions to Real-Time News Sentiment Announcements";"Axel Groß-Klußmann and  Nikolaus Hautsch";"B8";"2009-12-09";"G14,  C32";" We examine intra-day market reactions to news in stock-specific sentiment disclosures. Using pre-processed data from an automated news analytics tool based on linguistic pattern recognition we extract information on the relevance as well as the direction of company-specific news. Information-implied reactions in returns, volatility as well as liquidity demand and supply are quantified by a high-frequency VAR model using 20 second intervals. Analyzing a cross-section of stocks traded at the London Stock Exchange (LSE), we find market-wide robust news-dependent responses in volatility and trading volume. However, this is only true if news items are classified as highly relevant. Liquidity supply reacts less distinctly due to a stronger influence of idiosyncratic noise. Furthermore, evidence for abnormal highfrequency returns after news in sentiments is shown."
"364";364;"2009-064";"Altered Function of Ventral Striatum during Reward-Based Decision Making in Old Age";"Mell,  T.,  Wartenburger,  I.,  Marschner,  A.,  Villringer,  A.,  Reischies,  F. M.,  & Heekeren and   H. R.";"A12";"2009-12-31";"";""
"365";365;"2009-065";"Neuroeconomics and aging: Neuromodulation of economic decision making in old age";"Mohr,  P. N. C.,  Li,  S. C.,  & Heekeren and   H. R.";"A12";"2009-12-31";"";""
"366";366;"2010-001";"Volatility Investing with Variance Swaps";"Wolfgang Karl Härdle and  Elena Silyakova";"B1";"2010-01-07";"C14,  G13";" Traditionally volatility is viewed as a measure of variability, or risk, of an underlying asset. However recently investors began to look at volatility from a different angle. It happened due to emergence of a market for new derivative instruments - variance swaps. In this paper first we introduse the general idea of the volatility trading using variance swaps. Then we describe valuation and hedging methodology for vanilla variance swaps as well as for the 3-rd generation volatility derivatives: gamma swaps, corridor variance swaps, conditional variance swaps. Finally we show the results of the performance investigation of one of the most popular volatility strategies - dispersion trading. The strategy was implemented using variance swaps on DAX and its constituents during the 5-years period from 2004 to 2008."
"367";367;"2010-002";"Partial Linear Quantile Regression and Bootstrap Confidence Bands";"Wolfgang Karl Härdle, Ya’acov Ritov and  Song Song";"B1";"2010-01-07";"C14,  C21,  C31,  J01,  J31,  ";" In this paper uniform confidence bands are constructed for nonparametric quantile estimates of regression functions. The method is based on the bootstrap, where resampling is done from a suitably estimated empirical density function (edf) for residuals. It is known that the approximation error for the uniform confidence band by the asymptotic Gumbel distribution is logarithmically slow. It is proved that the bootstrap approximation provides a substantial improvement. The case of multidimensional and discrete regressor variables is dealt with using a partial linear model. Comparison to classic asymptotic uniform bands is presented through a simulation study. An economic application considers the labour market differential effect with respect to different education levels."
"368";368;"2010-003";"Uniform confidence bands for pricing kernels";"Wolfgang Karl Härdle, Yarema Okhrin and  Weining Wang";"B1";"2010-01-07";"C00,  C14,  J01,  J31";" Pricing kernels implicit in option prices play a key role in assessing the risk aversion over equity returns. We deal with nonparametric estimation of the pricing kernel (Empirical Pricing Kernel) given by the ratio of the risk-neutral density estimator and the subjective density estimator. The former density can be represented as the second derivative w.r.t. the European call option price function, which we estimate by nonparametric regression. The subjective density is estimated nonparametrically too. In this framework, we develop the asymptotic distribution theory of the EPK in the L1 sense. Particularly, to evaluate the overall variation of the pricing kernel, we develop a uniform confidence band of the EPK. Furthermore, as an alternative to the asymptotic approach, we propose a bootstrap confidence band. The developed theory is helpful for testing parametric specifications of pricing kernels and has a direct extension to estimating risk aversion patterns. The established results are assessed and compared in a Monte-Carlo study. As a real application, we test risk aversion over time induced by the EPK."
"369";369;"2010-004";"Bayesian Inference in a Stochastic Volatility Nelson-Siegel Model";"Nikolaus Hautsch and  Fuyu Yang";"B8";"2010-01-13";"C5,  C11,  C32";" In this paper, we develop and apply Bayesian inference for an extended Nelson-Siegel (1987) term structure model capturing interest rate risk. The so-called Stochastic Volatility Nelson-Siegel (SVNS) model allows for stochastic volatility in the underlying yield factors. We propose a Markov chain Monte Carlo (MCMC) algorithm to efficiently estimate the SVNS model using simulation-based inference. Applying the SVNS model to monthly U.S. zero-coupon yields, we find significant evidence for time-varying volatility in the yield factors. This is mostly true for the level and slope volatility revealing also the highest persistence. It turns out that the inclusion of stochastic volatility improves the model's goodness-of-fit and clearly reduces the forecasting uncertainty particularly in low-volatility periods. The proposed approach is shown to work efficiently and is easily adapted to alternative specifications of dynamic factor models revealing (multivariate) stochastic volatility."
"370";370;"2010-005";"The Impact of Macroeconomic News on Quote Adjustments, Noise, and Informational Volatility";"Nikolaus Hautsch, Dieter Hess and  David Veredas";"B8";"2010-01-13";"C32,  G14,  E44";" We study the impact of the arrival of macroeconomic news on the informational and noise-driven components in high-frequency quote processes and their conditional variances. Bid and ask returns are decomposed into a common (""efficient return"") factor and two market-side-specific components capturing market microstructure effects. The corresponding variance components reflect information-driven and noise-induced volatilities. We find that all volatility components reveal distinct dynamics and are positively influenced by news. The proportion of noise-induced variances is highest before announcements and significantly declines thereafter. Moreover, news-affected responses in all volatility components are influenced by order flow imbalances."
"371";371;"2010-006";"Bayesian Estimation and Model Selection in the Generalised Stochastic Unit Root Model";"Fuyu Yang and  Roberto Leon-Gonzalez";"B8";"2010-01-20";"C11,  C32";" We develop Bayesian techniques for estimation and model comparison in a novel Generalised Stochastic Unit Root (GSTUR) model. This allows us to investigate the presence of a deterministic time trend in economic series, while allowing the degree of persistence to change over time. In particular the model allows for shifts from stationarity I(0) to nonstationarity I(1) or vice versa. The empirical analysis demonstrates that the GSTUR model provides new insights on the properties of some macroeconomic time series such as stock market indices, inflation and exchange rates."
"372";372;"2010-007";"Two-sided Certification: The market for Rating Agencies";"Erik R. Fasten and  Dirk Hofmann";"A8";"2010-01-20";"G14,  G24,  L15,  D82";" Certifiers contribute to the sound functioning of markets by reducing asymmetric information. They, however, have been heavily criticized during the 2008-09 financial crisis. This paper investigates on which side of the market a monopolistic profit-maximizing certifier offers his service. If the seller demands a rating, the certifier announces the product quality publicly, whereas if the buyer requests a rating it remains his private information. The model shows that the certifier offers his service to sellers and buyers to maximize his own profit with a higher share from the sellers. Overall, certifiers increase welfare in specific markets. Revenue shifts due to the financial crisis are also explained. "
"373";373;"2010-008";"Characterising Equilibrium Selection in Global Games with Strategic Complementarities";"Christian Basteck, Tijmen R. Daniëls and  Frank Heinemann";"C10";"2010-01-21";"C72,  D82";" Global games are widely used for equilibrium selection to predict behaviour in complete information games with strategic complementarities. We establish two results on the global game selection. First, we show that it is independent of the payoff functions of the global game embedding, though it may depend on the noise distribution. Second, we give a simple sufficient criterion for noise independence in many action games. A many action game may be noise independent if it can be suitably decomposed into smaller (say, binary action) games, for which there are simple criteria guaranteeing noise independence. We delineate the games where noise independence may be established by counting the number of players or actions. In addition, we give an elementary proof that robustness to incomplete information implies noise independence."
"374";374;"2010-009";"Predicting extreme VaR: Nonparametric quantile regression with refinements from extreme value theory";"Julia Schaumburg";"B8";"2010-02-04";"C14,  C22,  C52,  C53";" This paper studies the performance of nonparametric quantile regression as a tool to predict Value at Risk (VaR). The approach is flexible as it requires no assumptions on the form of return distributions. A monotonized double kernel local linear estimator is applied to estimate moderate (1%) conditional quantiles of index return distributions. For extreme (0.1%) quantiles, where particularly few data points are available, we propose to combine nonparametric quantile regression with extreme value theory. The out-of-sample forecasting performance of our methods turns out to be clearly superior to different specifications of the Conditionally Autoregressive VaR (CAViaR) models."
"375";375;"2010-010";"On Securitization, Market Completion and Equilibrium Risk Transfer";"Ulrich Horst, Traian A. Pirvu and  Gonçalo Dos Reis";"A11";"2010-03-08";"G12,  D52,  C62,  C68";" We propose an equilibrium framework within which to price financial securities written on non- tradable underlyings such as temperature indices. We analyze a financial market with a finite set of agents whose preferences are described by a convex dynamic risk measure generated by the solution of a backward stochastic differential equation. The agents are exposed to financial and non-financial risk factors. They can hedge their financial risk in the stock market and trade a structured derivative whose payoff depends on both financial and external risk factors. We prove an existence and uniqueness of equilibrium result for derivative prices and characterize the equilibrium market price of risk in terms of a solution to a non-linear BSDE."
"376";376;"2010-011";"Illiquidity and Derivative Valuation";"Ulrich Horst and  Felix Naujokat";"A11";"2010-02-04";"C73,  G12,  G13";" In illiquid markets, option traders may have an incentive to increase their portfolio value by using their impact on the dynamics of the underlying. We provide a mathematical framework within which to value derivatives under market impact in a multi-player framework by introducing strategic interactions into the model of Almgren and Chriss (2001). Specifically, we consider a financial market model with several strategically interacting players that hold European contingent claims and whose trading decisions have an impact on the price evolution of the underlying. We establish existence and uniqueness of equilibrium results for risk neutral and CARA investors and show that the equilibrium dynamics can be characterized in terms of a coupled system of possibly non-linear PDEs. For the linear cost function used in Almgren and Chriss (2001), we obtain a (semi) closed form solution. Analyzing this solution, we show how market manipulation can be reduced."
"377";377;"2010-012";"Dynamic Systems of Social Interactions";"Ulrich Horst";"A11";"2010-02-04";"C63,  D50,  D71";" We state conditions for existence and uniqueness of equilibria in evolutionary models with an infinity of locally and globally interacting agents. Agents face repeated discrete choice problems. Their utility depends on the actions of some designated neighbors and the average choice throughout the whole population. We show that the dynamics on the level of aggregate behavior can be described by a deterministic measure-valued integral equation. If some form of positive complementarities prevails we establish convergence and ergodicity results for aggregate activities. We apply our convergence results to study a class of population games with random matching."
"378";378;"2010-013";"The dynamics of hourly electricity prices";"Wolfgang Karl Härdle and  Stefan Trück";"B1";"2010-02-04";"G12,  C19,  C13,  Q47";" The dynamics of hourly electricity prices in day-ahead markets is an important element of competitive power markets that were only established in the last decade. In electricity markets, the market microstructure does not allow for continuous trading, since operators require advance notice in order to verify that the schedule is feasible and lies within transmission constraints. Instead agents have to submit their bids and offers for delivery of electricity for all hours of the next day before a specified market closing time. We suggest the use of dynamic semiparametric factor models (DSFM) for the behavior of hourly electricity prices. We find that a model with three factors is able to explain already a high proportion of the variation in hourly electricity prices. Our analysis also provides insights into the characteristics of the market, in particular with respect to the driving factors of hourly prices and their dynamic behavior through time."
"379";379;"2010-014";"Crisis? What Crisis? Currency vs. Banking in the Financial Crisis of 1931";"Albrecht Ritschl and  Samad Sarferaz";"Z";"2010-02-04";"N12,  N13,  E37,  E47,  C53";" This paper examines the role of currency and banking in the German financial crisis of 1931 for both Germany and the U.S. We specify a structural dynamic factor model to identify financial and monetary factors separately for each of the two economies. We find that monetary transmission through the Gold Standard played only a minor role in causing and propagating the crisis, while financial distress was important. We also find evidence of crisis propagation from Germany to the U.S. via the banking channel. Banking distress in both economies was apparently not endogenous to monetary policy. Results confirm Bernanke's (1983) conjecture that an independent, non-monetary financial channel of crisis propagation was operative in the Great Depression."
"380";380;"2010-015";"Estimation of the characteristics of a Lévy process observed at arbitrary frequency";"Johanna Kappus and  Markus Reiß";"C12";"2010-02-04";"G13,  C14";" A LÃ©vy process is observed at time points of distance delta until time T. We construct an estimator of the LÃ©vy-Khinchine characteristics of the process and derive optimal rates of convergence simultaneously in T and delta. Thereby, we encompass the usual low- and high-frequency assumptions and obtain also asymptotics in the mid-frequency regime."
"381";381;"2010-016";"Honey, I’ll Be Working Late Tonight. The Effect of Individual Work Routines on Leisure Time Synchronization of Couples";"Juliane Scheffel";"C7";"2010-02-10";"D13,  J12,  J16,  J22";" German time use data for 2001/02 are used to assess the impact of workplace characteristics on the private life of couples. The major aim is to solve the endogeneity resulting from individual preferences for work and leisure to identify the pure effects of the workplace independent from other diluting personal influences in a cross-sectional setting when no appropriate instruments are available. I propose a repeated random assignment of people into pseudo couples as a solution. By this approach, I am able to uncover additional marriage inherent mechanisms that result in a (de-)synchronization of joint time that are still family friendly."
"382";382;"2010-017";"The Impact of ICT Investments on the Relative Demand for High-, Medium-, and Low-Skilled Workers: Industry versus Country Analysis";"Dorothee Schneider";"C7";"2010-02-10";"J21,  J23,  J31";" In this paper I analyze the effects of information and communication technology (ICT) on compensation shares of high-, medium-, and low-skilled workers. Com- pared to other studies, I investigate this question using a considerably richer data set with respect to the length of time series, set of countries and industries, and information on ICT. Next to investigating the influence of ICT in 14 countries, I concentrate on the analysis in 23 separate industries. The results I find show that the skill-biased technological change hypothesis is rejected if single countries are analyzed with an industry panel, while I find that technological change is a cause of changes in the relative compensation shares in single industries. Here there is a positive influence of ICT on high-skilled workers' relative compensation for the time before 1995, while ICT investments drive the medium- and low-skilled com- pensation shares together for a substantial amount of industries, especially since 1995."
"383";383;"2010-018";"Time varying Hierarchical Archimedean Copulae";"Wolfgang Karl Härdle, Ostap Okhrin and  Yarema Okhrin";"B1B10";"2010-02-17";"C13,  C14,  C50";" There is increasing demand for models of time-varying and non-Gaussian dependencies for mul- tivariate time-series. Available models suffer from the curse of dimensionality or restrictive assumptions on the parameters and the distribution. A promising class of models are the hierarchical Archimedean copulae (HAC) that allow for non-exchangeable and non-Gaussian dependency structures with a small number of parameters. In this paper we develop a novel adaptive estimation technique of the parameters and of the structure of HAC for time-series. The approach relies on a local change point detection procedure and a locally constant HAC approximation. Typical applications are in the financial area but also recently in the spatial analysis of weather parameters. We analyse the time varying dependency structure of stock indices and exchange rates. We find that for stock indices the copula parameter changes dynam- ically but the hierarchical structure is constant over time. Interestingly in our exchange rate example both structure and parameters vary dynamically."
"384";384;"2010-019";"Monetary Transmission Right from the Start: The (Dis)Connection Between the Money Market and the ECB’s Main Refinancing Rates";"Puriya Abbassi and  Dieter Nautz";"C14Z";"2010-03-19";"E43,  E52,  E58,  D44";" The relation between the ECBÂ’s main refinancing (MRO) rates and the money market is key for the monetary transmission process in the euro area. This paper investigates how money market rates respond to the new information revealed by MRO auctions. Our results confirm a stabilizing level relationship between the overnight rate Eonia and MRO rates before the financial crisis. Since the start of the financial crisis, however, we find that MRO auction outcomes even exacerbated the disconnection of money market rates from the policy-intended interest rate level. These findings support the fixed rate full allotment policy introduced by the ECB as an unconventionalmeasure to re-stabilize banksÂ’ refinancing conditions."
"385";385;"2010-020";"Aggregate Hazard Function in Price-Setting: A Bayesian Analysis Using Macro Data";"Fang Yao";"C7";"2010-04-01";"E12,  E31";" This paper presents an approach to identify aggregate price reset hazards from the joint dynamic behavior of inflation and macroeconomic aggregates. The identification is possible due to the fact that inflation is composed of current and past reset prices and that the composition depends on the price reset hazard function. The derivation of the generalized NKPC links those compostion effects to the hazard function, so that only aggregate data is needed to extract information about the price reset hazard function. The empirical hazard function is generally increasing with the age of prices, but with spikes at the 1st and 4th quarters. The implication of this finding for sticky price modeling is that the pricing decision is characterized by both time- and state-dependent aspects."
"386";386;"2010-021";"Nonparametric Estimation of Risk-Neutral Densities";"Maria Grith, Wolfgang Karl Härdle and  Melanie Schienle";"B1B11";"2010-04-01";"C13,  C14,  G12";" This chapter deals with nonparametric estimation of the risk neutral density. We present three different approaches which do not require parametric functional assumptions on the underlying asset price dynamics nor on the distributional form of the risk neutral density. The first estimator is a kernel smoother of the second derivative of call prices, while the second procedure applies kernel type smoothing in the implied volatility domain. In the conceptually different third approach we assume the existence of a stochastic discount factor (pricing kernel) which establishes the risk neutral density conditional on the physical measure of the underlying asset. Via direct series type estimation of the pricing kernel we can derive an estimate of the risk neutral density by solving a constrained optimization problem. The methods are compared using European call option prices. The focus of the presentation is on practical aspects such as appropriate choice of smoothing parameters in order to facilitate the application of the techniques."
"387";387;"2010-022";"Fitting high-dimensional Copulae to Data";"Ostap Okhrin";"B10";"2010-04-15";"C13,  C14,  C50";" This paper make an overview of the copula theory from a practical side. We consider different methods of copula estimation and different Goodness-of-Fit tests for model selection. In the GoF section we apply Kolmogorov-Smirnov and Cramer-von-Mises type tests and calculate power of these tests under different assumptions. Novating in this paper is that all the procedures are done in dimensions higher than two, and in comparison to other papers we consider not only simple Archimedean and Gaussian copulae but also Hierarchical Archimedean Copulae. Afterwards we provide an empirical part to support the theory."
"388";388;"2010-023";"The (In)stability of Money Demand in the Euro Area: Lessons from a Cross-Country Analysis";"Dieter Nautz and  Ulrike Rondorf";"C14";"2010-04-15";"E41,  E51,  E52";" The instability of standard money demand functions has undermined the role of monetary aggregates for monetary policy analysis in the euro area. This paper uses country-specific monetary aggregates to shed more light on the economics behind the instability of euro area money demand. Our results obtained from panel estimation indicate that the observed instability of standard money demand functions could be explained by omitted variables like e.g. technological progress that are important for money demand but constant across member countries."
"389";389;"2010-024";"The optimal industry structure in a vertically related market";"Raffaele Fiocco";"A8";"2010-04-22";"D82,  L11,  L51";" We consider a vertically related market characterized by downstream imperfect competition and by the monopolistic provision of an essential facility-based input, whose price is set by a social-welfare maximizing regulator. Our model shows that the regulatory knowledge about the cost for providing the monopolistic input crucially affects the design of the optimal industry structure. In particular, we compare ownership separation, which prevents a single company from having the control of both upstream and downstream operations, and legal separation, under which these activities are legally unbundled but common ownership is allowed. As long as the regulator has full information, the two industry patterns yield the same social welfare level. However, under asymmetric information about the input costs legal separation can make the whole society better off."
"390";390;"2010-025";"Herding of Institutional Traders";"Stephanie Kremer";"C14";"2010-04-27";"G11,  G24,  C23";" This paper sheds new light on herding of institutional investors by using a unique and superior database that identifies every transaction of financial institutions. First, the analysis reveals herding behavior of institutions. Second, the replica- tion of the analysis with low-frequent and anonymous transaction data, on which the bulk of literature is based, indicates an overestimation of herding by previous studies. Third, our results suggest that herding by large financial institutions is not intentional but results from sharing the same preference and investment style. Fourth, a panel analysis shows that herding on the sell side in stocks is positively related to past returns and past volatility while herding on the buy side is nega- tively related to past returns. In contrast to the literature, this indicates that large financial institutions do not show positive feedback strategies."
"391";391;"2010-026";"Non-Gaussian Component Analysis: New Ideas, New Proofs, New Applications";"Vladimir Panov";"B7";"2010-05-11";"C13,  C14";" In this article, we present new ideas concerning Non-Gaussian Component Analysis (NGCA). We use the structural assumption that a high-dimensional random vector X can be represented as a sum of two components - a lowdimensional signal S and a noise component N. We show that this assumption enables us for a special representation for the density function of X. Similar facts are proven in original papers about NGCA ([1], [5], [13]), but our representation differs from the previous versions. The new form helps us to provide a strong theoretical support for the algorithm; moreover, it gives some ideas about new approaches in multidimensional statistical analysis. In this paper, we establish important results for the NGCA procedure using the new representation, and show benefits of our method."
"392";392;"2010-027";"Liquidity and Capital Requirements and the Probability of Bank Failure";"Philipp Johann König";"C10";"2010-05-18";"G21,  G28";" Using the model of Rochet and Vives (2004), this note shows that a prudential regulator can in general not mitigate a bankÂ’s failure risk solely by means of liquidity requirements. However, their effectiveness can be restored if, in addition, minimum capital requirements are met. This provides a rationale for capital requirements beyond the commonly envoked reasoning that they are to be used to control the riskiness of banksÂ’ asset portfolios."
"393";393;"2010-028";"Social Relationships and Trust";"Christine Binzel and  Dietmar Fehr";"A6";"2010-05-20";"C72,  C93,  D82,  O12";" While social relationships play an important role for individuals to cope with missing market institutions, they also limit individuals' range of trading partners. This paper aims at understanding the determinants of trust at various social distances when information asymmetries are present. Among participants from an informal housing area in Cairo we find that the increase in trust following a reduction in social distance comes from the fact that trustors are much more inclined to follow their beliefs when interacting with their friend. When interacting with an ex-ante unknown agent instead, the decision to trust is mainly driven by social preferences. Nevertheless, trustors underestimate their friend's intrinsic motivation to cooperate, leading to a loss in social welfare. We relate this to the agents' inability to signal their trustworthiness in an environment characterized by strong social norms."
"394";394;"2010-029";"Adaptive Interest Rate Modelling";"Mengmeng Guo and  Wolfgang Karl Härdle";"B1";"2010-05-26";"E44,  G12,  G32,  N22";" A good description of the dynamics of interest rates is crucial to price derivatives and to hedge corresponding risk. Interest rate modelling in an unstable macroeconomic context motivates one factor models with time varying parameters. In this paper, the local parameter approach is introduced to adaptively estimate interest rate models. This method can be generally used in time varying coefficient parametric models. It is used not only to detect the jumps and structural breaks, but also to choose the largest time homogeneous interval for each time point, such that in this interval, the coefficients are statistically constant. We use this adaptive approach and apply it in simulations and real data. Using the three month treasure bill rate as a proxy of the short rate, we nd that our method can detect both structural changes and stable intervals for homogeneous modelling of the interest rate process. In more unstable macroeconomy periods, the time homogeneous interval can not last long. Furthermore, our approach performs well in long horizon forecasting."
"395";395;"2010-030";"Can the New Keynesian Phillips Curve Explain Inflation Gap Persistence?";"Fang Yao";"C7";"2010-06-03";"E12;E31";" Whelan (2007) found that the generalized Calvo-sticky-price model fails to replicate a typical feature of the empirical reduced-form Phillips curve - the positive dependence of inflation on its own lags. In this paper, I show hat it is the 4-period-Taylor-contract hazard function he chose that gives rise to this result. In contrast, an empirically-based aggregate price reset hazard function can generate simulated data that are consistent with inflation gap persistence found in US CPI data. I conclude that a non-constant price reset hazard plays a crucial role for generating realistic inflation dynamics."
"396";396;"2010-031";"Modeling Asset Prices";"James E. Gentle and   Wolfgang Karl Härdle";"B1";"2010-06-08";"C15";" As an asset is traded, its varying prices trace out an interesting time series. The price, at least in a general way, reflects some underlying value of the asset. For most basic assets, realistic models of value must involve many variables relating not only to the individual asset, but also to the asset class, the industrial sector(s) of the asset, and both the local economy and the general global economic conditions. Rather than attempting to model the value, we will confine our interest to modeling the price. The underlying assumption is that the price at which an asset trades is a ""fair market price"" that reflects the actual value of the asset. Our initial interest is in models of the price of a basic asset, that is, not the price of a derivative asset. Usually instead of the price itself, we consider the relative change in price, that is, the rate of return, over some interval of time."
"397";397;"2010-032";"Learning Machines Supporting Bankruptcy Prediction";"Wolfgang Karl Härdle, Rouslan Moro and  Linda Hoffmann";"B1";"2010-06-08";"C14,  G33,  C45";" In many economic applications it is desirable to make future predictions about the financial status of a company. The focus of predictions is mainly if a company will default or not. A support vector machine (SVM) is one learning method which uses historical data to establish a classification rule called a score or an SVM. Companies with scores above zero belong to one group and the rest to another group. Estimation of the probability of default (PD) values can be calculated from the scores provided by an SVM. The transformation used in this paper is a combination of weighting ranks and of smoothing the results using the PAV algorithm. The conversion is then monotone. This discussion paper is based on the Creditreform database from 1997 to 2002. The indicator variables were converted to financial ratios; it transpired out that eight of the 25 were useful for the training of the SVM. The results showed that those ratios belong to activity, profitability, liquidity and leverage. Finally, we conclude that SVMs are capable of extracting the necessary information from financial balance sheets and then to predict the future solvency or insolvent of a company. Banks in particular will benefit from these results by allowing them to be more aware of their risk when lending money."
"398";398;"2010-033";"Sensitivity of risk measures with respect to the normal approximation of total claim distributions";"Volker Krätschmer and  Henryk Zähle";"B5";"2010-06-15";"G22,  G32";" A simple and commonly used method to approximate the total claim distribution of a (possible weakly dependent) insurance collective is the normal approximation. In this article, we investigate the error made when the normal approximation is plugged in a fairly general distribution-invariant risk measure. We focus on the rate of the convergence of the error relative to the number of clients, we specify the relative errorÂ’s asymptotic distribution, and we illustrate our results by means of a numerical example. Regarding the risk measure, we take into account distortion risk measures as well as distribution-invariant coherent risk measures."
"399";399;"2010-034";"Sociodemographic, Economic, and Psychological Drivers of the Demand for Life Insurance: Evidence from the German Retirement Income Act";"Carolin Hecht and  Katja Hanewald";"B9";"2010-07-06";"D12,  D14,  D91,  G22,  K34";" We exploit the natural experiment of the 2005 income tax reform in Germany to study the effects of tax incentives on consumer behavior in life insurance markets. Our empirical analysis of sociodemographic, economic, and psychological household characteristics elicited in the German SAVE study shows that two very different consumer groups buy (endowment) life insurance before and after the tax reform. We find that education plays a central role in reactions to the modified tax environment. Our stylized characterization of Â“arbitrageurÂ” and Â“stragglerÂ” buyers will assist both life insurance firms and regulatory authorities design effective policies."
"400";400;"2010-035";"Efficiency and Equilibria in Games of Optimal Derivative Design";"Ulrich Horst and  Santiago Moreno-Bromberg";"A11";"2010-07-06";"C62,  C72,  D43,  D82,  G14";" In this paper the problem of optimal derivative design, profit maximization and risk minimization under adverse selection when multiple agencies compete for the business of a continuum of heterogenous agents is studied. In contrast with the principal-agent models that are extended within, here the presence of ties in the agents' best-response correspondences yields discontinuous payoff functions for the agencies. These discontinuities are dealt with via efficient tie-breaking rules. The main results of this paper are a proof of existence of (mixed-strategies) Nash equilibria in the case of profit-maximizing agencies, and of socially efficient allocations when the firms are risk minimizers. It is also shown that in the particular case of the entropic risk measure, there exists an efficient ""fix-mix"" tie-breaking rule, in which case firms share the whole market over given proportions. "
"401";401;"2010-036";"Why Do Financial Market Experts Misperceive Future Monetary Policy Decisions?";"Sandra Schmidt and  Dieter Nautz";"C14";"2010-07-06";"E47,  E52,  E58,  C23";" This paper investigates why financial market experts misperceive the interest rate policy of the European Central Bank (ECB). Assuming a Taylor-rule-type reaction function of the ECB, we use qualitative survey data on expectations about the future interest rate, inflation, and output to discover the sources of individual interest rate forecast errors. Based on a panel random coefficient model, we show that financial experts have systematically misperceived the ECB's interest rate rule. However, although experts tend to overestimate the impact of inflation on future interest rates, perceptions of monetary policy have become more accurate since clarification of the ECB's monetary policy strategy in May 2003. We find that this improved communication has reduced disagreement over the ECB's response to expected inflation during the financial crisis."
"402";402;"2010-037";"Dynamical systems forced by shot noise as a new paradigm in the interest rate modeling";"Alexander L. Baranovski";"B7";"2010-07-06";"C13,  C20andC22";" In this paper we give a generalized model of the interest rates term structure including Nelson-Siegel and Svensson structure. For that we introduce a continuous m-factor exponential-polynomial form of forward interest rates and demonstrate its considerably better performance in a fitting of the zero-coupon curves in comparison with the well known Nelson-Siegel and Svensson ones. In the sequel we transform the model into a dynamic model for interest rates by designing a switching dynamical system of the considerably reduced dimension n "
"403";403;"2010-038";"Pre-Averaging Based Estimation of Quadratic Variation in the Presence of Noise and Jumps: Theory, Implementation, and Empirical Evidence";"Nikolaus Hautsch and  Mark Podolskij";"B8";"2010-07-22";"C14,  C22,  G10";" This paper provides theory as well as empirical results for pre-averaging estimators of the daily quadratic variation of asset prices. We derive jump robust inference for pre-averaging estimators, corresponding feasible central limit theorems and an explicit test on serial dependence in microstructure noise. Using transaction data of different stocks traded at the NYSE, we analyze the estimatorsÂ’ sensitivity to the choice of the pre-averaging bandwidth and suggest an optimal interval length. Moreover, we investigate the dependence of pre-averaging based inference on the sampling scheme, the sampling frequency, microstructure noise properties as well as the occurrence of jumps. As a result of a detailed empirical study we provide guidance for optimal implementation of pre-averaging estimators and discuss potential pitfalls in practice."
"404";404;"2010-039";"High Dimensional Nonstationary Time Series Modelling with Generalized Dynamic Semiparametric Factor Model";"Song Song, Wolfgang K. Härdle and  Ya'acov Ritov";"B1";"2010-08-03";"C14,  C32,  G12";" (High dimensional) time series which reveal nonstationary and possibly periodic behavior occur frequently in many fields of science. In this article, we separate the modeling of high dimensional time series to time propagation of low dimensional time series and high dimensional time invariant functions via functional factor analysis. We propose a two-step estimation procedure. At the first step, we detect the deterministic trends of the time series by incorporating time basis selected by the group Lasso-type technique and choose the space basis based on smoothed functional principal component analysis. We show properties of this  estimator under various situations extending current variable selection studies. At the second step, we obtain the detrended low dimensional stochastic process, but it also poses an important question: is it justified, from an inferential point of view, to base further statistical inference on the estimated stochastic time series? We show that the difference of the inference based on the estimated time series and ""true"" unobserved time series is asymptotically negligible, which finally allows one to study the dynamics of the whole high-dimensional system with a low dimensional representation together with the deterministic trend. We apply the method to our motivating empirical problems: studies of the dynamic behavior of temperatures (further used for pricing weather derivatives), implied volatilities and risk patterns and correlated brain activities (neuro-economics related) using fMRI data, where a panel version model is also presented."
"405";405;"2010-040";"Stochastic Mortality, Subjective Survival Expectations, and Individual Saving Behavior";"Thomas Post and  Katja Hanewald";"B9";"2010-08-04";"D14,  D84,  D91,  H31,  J11";" Theoretical studies suggest that unexpected changes in future mortality and survival probabilities (stochastic mortality) are important determinants of individualsÂ’ decisions about consumption, saving, asset allocation, and retirement timing. Using data on subjective survival expectations elicited in the Survey of Health, Ageing and Retirement in Europe (SHARE) and corresponding life table data from the Human Mortality Database (HMD), we find evidence of respondentsÂ’ awareness of stochastic mortality. We also find that respondentsÂ’ saving behavior is influenced by stochastic mortality perceptions."
"406";406;"2010-041";"Prognose mit nichtparametrischen Verfahren";"Wolfgang Karl Härdle, Rainer Schulz and  Weining Wang";"B1";"2010-08-31";"C14,  C32,  G12";" Statistische Prognosen basieren auf der Annahme, dass ein funktionaler Zusammenhang zwischen der zu prognostizierenden Variable y und anderen j-dimensional beobachtbaren Variablen x = (x1,...xl) besteht. Kann der funktionale Zusammenhang geschÃ¤tzt werden, so kann im Prinzip fÃ¼r jedes x der zugehÃ¶rige Wert y prognostiziert werden. Bei den meisten Anwendungen wird angenommen, dass der funktionale  Zusammenhang einem niedrigdimensionalen parametrischen Modell entspricht oder durch dieses zumindest gut wiedergegeben wird. Ein Beispiel im univariaten Fall ist das lineare Modell y = b0 + b1x. Sind die beiden unbekannten Parameter b0 und b1 mithilfe historischer Daten geschÃ¤tzt, so lÃ¤sst sich fÃ¼r jedes gegebene x sofort der zugehÃ¶rige Wert y prognostizieren. Allerdings besteht hierbei die Gefahr, dass der wirkliche funktionale Zusammenhang nicht dem gewÃ¤hlten Modell entspricht. Dies kann infolge zu schlechten Prognosen fÃ¼hren. Nichtparametrische Verfahren gehen ebenfalls von einem funktionalen Zusammenhang aus, geben aber kein festes parametrisches Modell vor und zwÃ¤ngen die Daten damit in kein Prokrustes Bett. Sie sind deshalb hervorragend geeignet, um 1) Daten explorativ darzustellen, 2) parametrische Modelle zu Ã¼berprÃ¼fen und 3) selbst als SchÃ¤tzer fÃ¼r den funktionalen Zusammenhang zu dienen (Cleveland [2], Cleveland und Devlin [3]). Nichtparametrische Verfahren kÃ¶nnen daher problemlos auch zur Prognose eingesetzt werden.  Dieses Kapitel ist wie folgt strukturiert. Abschnitt 9.2 stellt nichtparametrische Verfahren vor und erlÃ¤utert deren grundsÃ¤tzliche Struktur. Der Schwerpunkt liegt auf dem univariaten Regressionsmodell und auf der Motivation der vorgestellten Verfahren. Abschnitt 9.3 prÃ¤sentiert eine praktische Anwendung fÃ¼r eine Zeitreihe von WechselkursvolatilitÃ¤ten. Es werden Prognosen mit nichtparametrischen Verfahren berechnet und deren GÃ¼te mit den Prognosen eines AR(1)-Zeitreihenmodells verglichen, vgl. auch Kapitel 14 dieses Buches. Es ze"
"407";407;"2010-042";"Payroll Taxes, Social Insurance and Business Cycles";"Michael C. Burda and  Mark Weder";"C7";"2010-08-31";"E24,  J64,  E32";" Payroll taxes represent a major distortionary influence of governments on labor markets. This paper examines the role of payroll taxation and the social safety net for cyclical fluctuations in a nonmonetary economy with labor market frictions and unemployment insurance, when the latter is only imperfectly related to search effort. A balanced social insurance budget renders gross wages more rigid over the cycle and, as a result, strengthens the modelÂ’s endogenous propagation mechanism. For conventional calibrations, the model generates a negatively sloped Beveridge curve as well as substantial volatility and persistence of vacancies and unemployment. herunterladen."
"408";408;"2010-043";"Meteorological forecasts and the pricing of weather derivatives";"Matthias Ritter, Oliver Mußhoff and  Martin Odening";"C11";"2010-09-07";"C53,   G13,   G17,   N23";" In usual pricing approaches for weather derivatives, forward-looking information such as meteorological weather forecasts is not considered. Thus, important knowledge used by market participants is ignored in theory. By extending a standard model for the daily temperature, this paper allows the incorporation of meteorological forecasts in the framework of weather derivative pricing and is able to estimate the information gain compared to a benchmark model without meteorological forecasts. This approach is applied for temperature futures referring to New York, Minneapolis and Cincinnati with forecast data 13 days in advance. Despite this relatively short forecast horizon, the models using meteorological forecasts outperform the classical approach and more accurately forecast the market prices of the temperature futures traded at the Chicago Mercantile Exchange (CME). Moreover, a concentration on the last two months or on days with actual trading improves the results."
"409";409;"2010-044";"The High Sensitivity of Employment to Agency Costs: The Relevance of Wage Rigidity";"Atanas Hristov";"C7";"2010-09-07";"E24,  E32,  J64,  G24";" This paper studies the interaction of financing constraints and labor market imperfections on the labor market and economic activity. My analysis builds on the agency cost framework of Carlstrom and Fuerst [1998. Agency costs and business cycles. Economic Theory, 12(3):583-597]. The aim of this article is to show that financing constraints can substantially amplify and propagate total factor productivity shocks in cyclical labor market dynamics. I find that under the Nash bargaining solution financing constraints increase substantially the volatility of wages, and in turn, amplification for the labor variables falls short of the observed volatilities in the data. Atop of this, the comovement between output and labor share is counterfactual. However, there is substantial scope for any type of wage rigidity and financing constraints to reinforce each other, and to generate the observed volatilities in the labor market, moreover, to produce a wide range of comovements between output and labor share."
"410";410;"2010-045";"Parametric estimation of risk neutral density functions";"Maria Grith and  Volker Krätschmer";"B1B5";"2010-09-09";"C13,  C16,  G12,  G13";" This chapter deals with the estimation of risk neutral distributions for pricing index options resulting from the hypothesis of the risk neutral valuation principle. After justifying this hypothesis, we shall focus on parametric estimation methods for the risk neutral density functions determining the risk neutral distributions. We we shall differentiate between the direct and the indirect way. Following the direct way, parameter vectors are estimated which characterize the distributions from selected statistical families to model the risk neutral distributions. The idea of the indirect approach is to calibrate characteristic parameter vectors for stochastic models of the asset price processes, and then to extract the risk neutral density function via Fourier methods. For every of the reviewed methods the calculation of option prices under hypothetically true risk neutral distributions is a building block. We shall give explicit formula for call and put prices w.r.t. reviewed parametric statistical families used for direct estimation. Additionally, we shall introduce the Fast Fourier Transform method of call option pricing developed in [6]. It is intended to compare the reviewed estimation methods empirically."
"411";411;"2010-046";"Mandatory IFRS adoption and accounting comparability";"Stefano Cascino and  Joachim Gassen";"A7";"2010-10-07";"M41,  G14,  F42";" The adoption of IFRS by many countries worldwide fuels the expectation that financial accounting might become more comparable across countries. This expectation is opposed to an alternative view that stresses the importance of incentives in shaping accounting information. We provide early evidence on this debate by investigating the effects of mandatory IFRS adoption on the comparability of financial accounting information around the world. Our results suggest that while mandatory adoption of IFRS increases the comparability of some prominent balance sheet line items across countries, it has no clear effect on the cross-country comparability of earnings attributes. To provide a rationale for these mixed findings, we investigate the IFRS measurement and disclosure compliance choices for a hand-collected sample of German and Italian firms. We find that predictable country-, region-, and firm-level incentives continue to shape the outcome of the financial reporting process and thus limit the crosssectional comparability of financial accounting information. Overall, our results suggest that the mandatory adoption of IFRS has a limited impact on accounting comparability and that accounting information continues to be shaped by both reporting standards and incentives."
"412";412;"2010-047";"FX Smile in the Heston Model";"Agnieszka Janek, Tino Kluge, Rafal Weron and  Uwe Wystup";"B1";"2010-10-14";"C5,  C63,  G13";" Abstract: The Heston model stands out from the class of stochastic volatility (SV) models mainly for two reasons. Firstly, the process for the volatility is nonnegative and mean-reverting, which is what we observe in the markets. Secondly, there exists a fast and easily implemented semi-analytical solution for European options. In this article we adapt the original work of Heston (1993) to a foreign exchange (FX) setting. We discuss the computational aspects of using the semi-analytical formulas, performing Monte Carlo simulations, checking the Feller condition, and option pricing with FFT. In an empirical study we show that the smile of vanilla options can be reproduced by suitably calibrating three out of five model parameters."
"413";413;"2010-048";"Building Loss Models";"Krzysztof Burnecki, Joanna Janczura and  Rafal Weron";"B1";"2010-10-14";"C15,  C46,  C63,  G22,  G32";" This paper is intended as a guide to building insurance risk (loss) models. A typical model for insurance risk, the so-called collective risk model, treats the aggregate loss as having a compound distribution with two main components: one characterizing the arrival of claims and another describing the severity (or size) of loss resulting from the occurrence of a claim. In this paper we first present efficient simulation algorithms for several classes of claim arrival processes. Then we review a collection of loss distributions and present methods that can be used to assess the goodness-of-fit of the claim size distribution. The collective risk model is often used in health insurance and in general insurance, whenever the main risk components are the number of insurance claims and the amount of the claims. It can also be used for modeling other non-insurance product risks, such as credit and operational risk."
"414";414;"2010-049";"Models for Heavy-tailed Asset Returns";"Szymon Borak, Adam Misiorek and  Rafal Weron";"B1";"2010-10-14";"C13,  C15,  C16,  G32";" Many of the concepts in theoretical and empirical finance developed over the past decades Â– including the classical portfolio theory, the Black- Scholes-Merton option pricing model or the RiskMetrics variance-covariance approach to VaR Â– rest upon the assumption that asset returns follow a normal distribution. But this assumption is not justified by empirical data! Rather, the empirical observations exhibit excess kurtosis, more colloquially known as fat tails or heavy tails. This chapter is intended as a guide to heavy-tailed models. We first describe the historically oldest heavy-tailed model Â– the stable laws. Next, we briefly characterize their recent lighter-tailed generalizations, the socalled truncated and tempered stable distributions. Then we study the class of generalized hyperbolic laws, which Â– like tempered stable distributions Â– can be classified somewhere between infinite variance stable laws and the Gaussian distribution. Finally, we provide numerical examples."
"415";415;"2010-050";"Estimation of the signal subspace without estimation of the inverse covariance matrix";"Vladimir Panov";"B7";"2010-10-14";"C13,  C14";" Let a high-dimensional random vector X can be represented as a sum of two components - a signal S, which belongs to some low-dimensional subspace S, and a noise component N. This paper presents a new approach for estimating the subspace S based on the ideas of the Non-Gaussian Component Analysis. Our approach avoids the technical difficulties that usually exist in similar methods - it doesnÂ’t require neither the estimation of the inverse covariance matrix of X nor the estimation of the covariance matrix of N."
"416";416;"2010-051";"Executive Compensation Regulation and the Dynamics of the Pay-Performance Sensitivity";"Ralf Sabiwalsky";"Z";"2010-10-27";"G38,  K22,  M52";" A substantial number of empirical studies on the linear relationship between executive compensation and firm performance for European firms suggest that the pay-performance sensitivity is not significantly positive. We argue that a nonlinear structure fits the data better, because compensation contracts provide for minimum performance benchmarks and an upper limit to the variable component of compensation. We test for such discontinuities in the pay performance relationship, and confirm their existence, using hand collected data from German Prime All Share firmsÂ’ CEO bonus compensation. It turns out that there is a significant positive relationship between return on assets and CEO bonus for ROA between -3% and +20%. Performance sensitivity is then tested for changes over time between 2006 and 2009. Results reveal that during the first three years after the introduction of a statutory transparency rule in 2005 governing the disclosure of individual CEO compensation, significant changes to compensation contracts did not occur; but that in 2009 the pay-performance sensitivity exhibited a significant increase, which coincides with the passing of a law that requires supervisory boards to ensure that new CEO employment contracts provide for Â’reasonableÂ’ compensation."
"417";417;"2010-052";"Central limit theorems for law-invariant coherent risk measures";"Denis Belomestny and  Volker Krätschmer";"B5B7";"2010-10-27";"D81,  G32";" In this paper we study the asymptotic properties of the canonical plug-in estimates for law-invariant coherent risk measures. Under rather mild conditions not relying on the explicit representation of the risk measure under consideration, we rst prove a central limit theorem for independent identically distributed data and then extend it to the case of weakly dependent ones. Finally, a number of illustrating examples is presented."
"418";418;"2010-053";"Systemic Weather Risk and Crop Insurance: The Case of China";"Wei Xu, Ostap Okhrin, Martin Odening and  Ji Cao";"B10C11";"2010-10-27";"C14,  Q19";" The supply of affordable crop insurance is hampered by the existence of systemic weather risk which results in large risk premiums. In this article, we assess the systemic nature of weather risk for 17 agricultural production regions in China and explore the possibility of spatial diversification of this risk. We simulate the buffer load of hypothetical temperature-based insurance and investigate the relation between the size of the buffer load and the size of the trading area of the insurance. The analysis makes use of a hierarchical Archimedean copula approach (HAC) which allows flexible modeling of the joint loss distribution and reveals the dependence structure of losses in different insured regions. Our results show a significant decrease of the required risk loading when the insured area expands. Nevertheless, a considerable part of undiversifiable risk remains with the insurer. We find that the spatial diversification effect depends on the type of the weather index and the strike level of the insurance. Our findings are relevant for insurers and insurance regulators as they shed light on the viability of private crop insurance in China."
"419";419;"2010-054";"Spatial Dependencies in German Matching Functions";"Franziska Schulze";"B8";"2010-11-03";"C21,  C23,  J64,  J63,  R12";" This paper proposes a spatial panel model for German matching functions to avoid possibly biased and inefficient estimates due to spatial dependence. We provide empirical evidence for the presence of spatial dependencies in matching data. Based on an official data set containing monthly information for 176 local employment offices, we show that neglecting spatial dependencies in the data results in overestimated coefficients. For the incorporation of spatial information into our model, we use data on commuting relations between local employment offices. Furthermore, our results suggest that a dynamic modeling is more appropriate for matching functions."
"420";420;"2010-055";"Capturing the Zero: A New Class of Zero-Augmented Distributions and Multiplicative Error Processes";"Nikolaus Hautsch, Peter Malec and  Melanie Schienle";"B8B11";"2010-11-18";"C22,  C25,  C14,  C16,  C51";" We propose a novel approach to model serially dependent positive-valued variables which realize a non-trivial proportion of zero outcomes. This is a typical phenomenon in financial time series observed on high frequencies, such as cumulated trading volumes or the time between potentially simultaneously occurring market events. We introduce a flexible point-mass mixture distribution and develop a semiparametric specification test explicitly tailored for such distributions. Moreover, we propose a new type of multiplicative error model (MEM) based on a zero-augmented distribution, which incorporates an autoregressive binary choice component and thus captures the (potentially different) dynamics of both zero occurrences and of strictly positive realizations. Applying the proposed model to high-frequency cumulated trading volumes of liquid NYSE stocks, we show that the model captures both the dynamic and distribution properties of the data very well and is able to correctly predict future distributions."
"421";421;"2010-056";"Context Effects as Customer Reaction on Delisting of Brands";"Nicole Wiebach and  Lutz Hildebrandt";"B2";"2010-11-22";"M31,  C12,  C13,  C25,  C38";" The delisting of brands is frequently used by retailers to strengthen their negotiating position with the manufacturers and suppliers of their product assortment. However, retailers and manufacturers have  to consider the risk of potential reactions when customers are faced with a reduced or modified assortment and thus,  different choice. In this paper, two studies are presented which investigate customers` switching behavior if a  (sub-)brand is unavailable and key determinants of the resulting behavior are discussed. Various conditions are  tested by taking into account context theory. The results reveal that customer responses depend significantly on  the context. A real-life quasi-experiment suggests that manufacturers may encounter substantially larger  losses than retailers. Managerial implications for both parties can be derived and recommendations for further  research are developed."
"422";422;"2010-057";"Consumption Growth and Volatility with Consumption Externalities";"Runli Xie";"C7";"2010-11-22";"E21,  D91,  D31,  D64";" This paper studies the link between group-specific consumption growth and volatility within a framework of heterogeneous agents, under the assumption of a consumption externality. Household preferences are related to the volatility through asset holding decisions: volatility decreases with groups' degree of patience, and increases with household eagerness to keep up with the group average. Moreover, consumption growth is expected to relate positively to the volatility. This last hypothesis is tested using household data imputed from GSOEP and the German Income and Expenditure Survey (EVS), where a U-shaped relationship is found for the nondurable consumption. Moreover, examining the growth-inequality relationship using EVS data alone shows that it is positive for nondurable and negative for durable consumption."
"423";423;"2010-058";"Inflation, Price Dispersion and Market Integration through the Lens of a Monetary Search Model";"Sascha S. Becker and  Dieter Nautz";"C14";"2010-11-25";"C23,  D40,  E31,  F40";" Monetary search theory implies that the real effects of inflation via its impact on price dispersion depend on the level of search costs and, thus, on the level of market integration. For less integrated markets, the inflation-price dispersion nexus is predicted to be asymmetrically V-shaped which results in an optimal inflation rate above zero. For highly integrated markets with low search costs, however, the impact of inflation on price dispersion should only be small. Using price data of the European Union member states, this paper is the first that tests and confirms these predictions of monetary search theory."
"424";424;"2010-059";"Nonparametric Regression with Nonparametrically Generated Covariates";"Enno Mammen, Christoph Rothe and  Melanie Schienle";"B11";"2010-12-06";"C14,  C31";" We analyze the properties of non- and semiparametric estimation procedures involving nonparametric regression with generated covariates. Such estimators appear in numerous econometric applications, including nonparametric estimation of simultaneous equation models, sample selection models, treatment effect models, and censored regression models, but so far there seems to be no unified theory to establish their statistical properties. Our paper provides such results, allowing to establish asymptotic properties like rates of consistency or asymptotic normality for a wide range of semi- and nonparametric estimators. We also show how to account for the presence of nonparametrically generated regressors when computing standard errors. "
"425";425;"2010-060";"Communal Responsibility and the Coexistence of Money and Credit Under Anonymous Matching";"Lars Boerner and  Albrecht Ritschl";"Z";"2010-12-21";"E41,  D51,  N2";" Communal responsibility, a medieval institution studied by Greif (2006),  supported the use of credit among European merchants in the absence of modern enforcement technologies. This paper shows how this mechanism helps to overcome enforcement problems in anonymous buyer/seller transactions. In a village economy version of the Lagos and Wright (2005) model, agents trading anonymously in decentralized markets can be identified by their citizenship and thus be held liable for each other. Enforceability within each village's  centralized afternoon market ensures collateralization of credit in decentralized markets. In the resulting equilibrium, money and credit coexist in decentralized  markets if the use of credit is costly. Our analysis easily extends itself to other payment systems like credit cards that provide a group identity to otherwise anonymous agents."
"426";426;"2010-061";"Every Symmetric 3 x 3 Global Game of Strategic Complementarities Is Noise Independent";"Christian Basteck and  Tijmen R. Daniëls";"C10";"2010-12-21";"C72,  D82";" We prove that the global game selection in all 3 x 3 payoff-symmetric supermodular games is independent of the noise structure. As far as we know, all other proofs of noise independence of such games rely on the existence of a so-called monotone potential (MP) maximiser. Our result is more general, since some 3 x 3 symmetric supermodular games do not admit an MP maximiser. Moreover, a corollary is that noise independence does not imply the existence of an MP maximiser."
"427";427;"2010-062";"The Norges Bank’s key rate projections and the news element of monetary policy: a wavelet based jump detection approach";"Lars Winkelmann";"C14";"2010-12-21";"E52,  E58,  C14";" This paper investigates the information content of the Norges BankÂ’s key rate projections. Wavelet spectrum estimates provide the basis for estimating jump probabilities of short- and long-term interest rates on monetary policy announcement days before and after the introduction of key rate projections. The behavior of short-term interest rates reveals that key rate projections have only little effects on marketÂ’s forecasting ability of current target rate changes. In contrast, longer-term interest rates indicate that the announcement of key rate projections has significantly reduced market participantsÂ’ revisions of the expected future policy path. Therefore, the announcement of key rate projections further improves central bank communication."
"428";428;"2010-063";"How the brain integrates costs and benefits during decision making.";"Basten U.,  Biele G. P.,  Heekeren H. R. and   Fiebach C. J.";"A12";"2010-12-31";"";""
"429";429;"2010-064";"Variability in brain activity as an individual difference measure in neuroscience?";"Mohr,  P.N.C.,  Nagel and   I.E.";"A12";"2010-12-31";"";""
"430";430;"2010-065";"Neural Processing of Risk";"Mohr,  P.N.C.,  Biele,  G.,  Heekeren and   H.R.";"A12";"2010-12-31";"";""
"431";431;"2011-001";"Localising temperature risk";"Wolfgang Karl Härdle, Brenda López Cabrera, Ostap Okhrin and  Weining Wang";"B1B10";"2011-01-03";"G19,   G29,   G22,   N23,   N5";" On the temperature derivative market, modeling temperature volatility is an important issue for pricing and hedging. In order to apply pricing tools of nancial mathematics, one needs to isolate a Gaussian risk factor. A conventional model for temperature dynamics is a stochastic model with seasonality and inter temporal autocorrelation. Empirical work based on seasonality and autocorrelation correction reveals that the obtained residuals are heteroscedastic with a periodic pattern. The object of this research is to estimate this heteroscedastic function so that after scale normalisation a pure standardised Gaussian variable appears. Earlier work investigated this temperature risk in different locations and showed that neither parametric component functions nor a local linear smoother with constant smoothing parameter are flexible enough to generally describe the volatility process well. Therefore, we consider a local adaptive modeling approach to find at each time point, an optimal smoothing parameter to locally estimate the seasonality and volatility. Our approach provides a more flexible and accurate fitting procedure of localised temperature risk process by achieving excellent normal risk factors."
"432";432;"2011-002";"A Confidence Corridor for Sparse Longitudinal Data Curves";"Shuzhuan Zheng, Lijian Yang and  Wolfgang Karl Härdle";"B1";"2011-01-03";"C14,  C33";" Longitudinal data analysis is a central piece of statistics. The data are curves and they are observed at random locations. This makes the construction of a simultaneous confidence corridor (SCC) (confidence band) for the mean function a challenging task on both the theoretical and the practical side. Here we propose a method based on local linear smoothing that is implemented in the sparse (i.e., low number of nonzero coefficients) modelling situation. An SCC is constructed based on recent results obtained in applied probability theory. The precision and performance is demonstrated in a spectrum of simulations and applied to growth curve data. Technically speaking, our paper intensively uses recent insights into extreme value theory that are also employed to construct a shoal of confidence intervals (SCI)."
"433";433;"2011-003";"Mean Volatility Regressions";"Lu Lin, Feng Li, Lixing Zhu and  Wolfgang Karl Härdle";"B1";"2011-01-03";"C00,  C14,  J01,  J31";" Motivated by increment process modeling for two correlated random and non-random systems from a discrete-time asset pricing with both risk free asset and risky security, we propose a class of semiparametric regressions for a combination of a non-random and a random system. Unlike classical regressions, mean regression functions in the new model contain variance components and the model variables are related to latent variables, for which certain economic interpretation can be made. The motivating example explains why the GARCH-M of which the mean function contains a variance component cannot cover the newly proposed models. Further, we show that statistical inference for the increment process cannot be simply dealt with by a two-step procedure working separately on the two involved systems although the increment process is a weighted sum of the two systems. We further investigate the asymptotic behaviors of estimation by using sophisticated nonparametric smoothing. Monte Carlo simulations are conducted to examine finite-sample performance, and a real dataset published in Almanac of ChinaÂ’s Finance and Banking (2004 and 2005) is analyzed for illustration about the increment process of wealth in financial market of China from 2003 to 2004."
"434";434;"2011-004";"A Confidence Corridor for Expectile Functions";"Esra Akdeniz Duran, Mengmeng Guo and  Wolfgang Karl Härdle";"B1";"2011-01-03";"C00,  C14,  J01,  J31";" Let (X1; Y1), Â…, (Xn; Yn) be i.i.d. rvs and let v(x) be the unknown Ï„ - expectile regression curve of Y conditional on X. An expectile-smoother vn(x) is a localized, nonlinear estimator of v(x). The strong uniform consistency rate is established under general conditions. In many applications it is necessary to know the stochastic fluctuation of the process {vn(x) Â– v(x)}. Using strong approximations of the empirical process and extreme value theory, we consider the asymptotic maximal deviation sup0â‰¤xâ‰¤1 |vn(x) Â– v(x)|. The derived result helps in the construction of a uniform confidence band for the expectile curve v(x). This paper considers fitting a simultaneous confidence corridor (SCC) around the estimated expectile function of the conditional distribution of Y given x based on the observational data generated according to a nonparametric regression model. Moreover, we construct the simultaneous confidence corridors around the expectiles of the residuals from the temperature models to investigate the temperature risk drivers."
"435";435;"2011-005";"Local Quantile Regression";"Wolfgang Karl Härdle, Vladimir Spokoiny and  Weining Wang";"B1B5B10";"2011-01-03";"C00,  C14,  J01,  J31";" Conditional quantile curves provide a comprehensive picture of a response contingent on explanatory variables. Quantile regression is a technique to estimate such curves. In a flexible modeling framework, a specific form of the quantile is not a priori fixed. Indeed, the majority of applications do not per se require specific functional forms. This motivates a local parametric rather than a global fixed model fitting approach. A nonparametric smoothing estimate of the conditional quantile curve requires to consider a balance between local curvature and variance. In this paper, we analyze a method based on a local model selection technique that provides an adaptive estimate. Theoretical properties on mimicking the oracle choice are offered and applications to stock market and weather analysis are presented."
"436";436;"2011-006";"Sticky Information and Determinacy";"Alexander Meyer-Gohde";"C7";"2011-01-12";"C62,  E31,  E43,  E52";" The infinite-dimensional sticky-information Phillips curve is cast as a finite-dimensional timevarying system of difference equations in order to directly assess determinacy in the model with demand given by the forward-looking IS equation and monetary policy by an interest rate rule. An equivalence to the model without lagged expectations holds (albeit tenuously) for the particular specification and a common truncation method produces spurious determinacy."
"437";437;"2011-007";"Mean-Variance Cointegration and the Expectations Hypothesis";"Till Strohsal and  Enzo Weber";"C14";"2011-02-07";"E43,  C32";" The present paper sheds further light on a well-known (alleged) violation of the expec- tations hypothesis of the term structure (EHT) - the frequent finding of unit roots in interest rate spreads. We show that the EHT implies (i) that the nonstationarity stems from the holding premium, which is hence (ii) cointegrated with the spread. In a stochas- tic discount factor framework we model the premium as being driven by the integrated variance of excess returns. Introducing the concept of mean-variance cointegration we actually find cointegration relations between spreads and premia in US data."
"438";438;"2011-008";"Monetary Policy, Trend Inflation and Inflation Persistence";"Fang Yao";"C7";"2011-02-17";"E31,  E52";" This paper presents a new mechanism through which monetary policy rules affect inflation persistence. When assuming that price reset hazard functions are not constant, backward- looking dynamics emerge in the NKPC. This new mechanism makes the traditional demand channel of monetary transmission have a long-lasting effect on inflation dynamics. The Calvo model fails to convey this insight, because its constant hazard function leads those important backward-looking dynamics to be canceled out. I first analytically show how it works in a simple setup, and then solve a log-linearized model numerically around positive trend inflation. With realistic calibration of trend inflation and the monetary policy rule, the model can account for the pattern of changes in inflation persistence observed in the post-wwii U.S. data. In addition, with increasing hazard functions, the ""Taylor principle"" is sufficient to guarantee the determinate equilibrium even under extremely high trend inflation."
"439";439;"2011-009";"Exclusion in the All-Pay Auction: An Experimental Investigation";"Dietmar Fehr and  Julia Schmid";"A6";"2011-02-17";"C72,  C92,  D84";" Contest or auction designers who want to maximize the overall revenue are frequently con- cerned with a trade-off between contest homogeneity and inclusion of contestants with high valuations. In our experimental study, we find that it is not profitable to exclude the most able contestant in favor of greater homogeneity among the remaining contestants, even if the theoretical exclusion principle predicts otherwise. This is because the strongest contestants con- siderably overexert. A possible explanation is that these contestants are afraid they will regret a low but risky bid if they lose and thus prefer a strategy which gives them a low but secure pay-off."
"440";440;"2011-010";"Unwillingness to Pay for Privacy: A Field Experiment";"Alastair R. Beresford, Dorothea Kübler and  Sören Preibusch";"A6";"2011-02-17";"C93,  D12";" We measure willingness to pay for privacy in a field experiment. Participants were given the choice to buy a maximum of one DVD from one of two online stores. One store consistently required more sensitive personal data than the other, but otherwise the stores were identical. In one treatment, DVDs were one Euro cheaper at the store requesting more personal information, and almost all buyers chose the cheaper store. Surprisingly, in the second treatment when prices were identical, participants bought from both shops equally often."
"441";441;"2011-011";"Human Capital Formation on Skill-Specific Labor Markets";"Runli Xie";"C7";"2011-02-21";"E24,  E32,  J24,  J63";" Human capital investment is formed through households' endogenous decision, and competes with physical capital investment. Idiosyncratic shock shifts the skilled labor share and changes tightness in both skilled and unskilled markets. Given inelastic labor participation, the model can generate downward-sloping Beveridge curves in aggregate, skilled and unskilled labor markets. Upon a neutral shock, total unemployment decrease is two-staged: firstly with a reduction in unskilled unemployment, and then due to a sharp decline of skilled unemployment when skill substitution dominates. A higher elasticity of substitution between two types of labor leads to higher volatility of the model variables and higher u - v correlation."
"442";442;"2011-012";"A strategic mediator who is biased into the same direction as the expert can improve information transmission";"Lydia Mechtenberg and  Johannes Münster";"A6";"2011-03-03";"C72,  D82,  D83";" This note reconsiders communication between an informed expert and an uninformed decision maker with a strategic mediator in a discrete Crawford and Sobel (1982) setting. We show that a strategic mediator may improve communication even when he is biased into the same direction as the expert. The mediator improves communication, however, only if some information transmission is possible with unmediated communication."
"443";443;"2011-013";"Spatial Risk Premium on Weather Derivatives and Hedging Weather Exposure in Electricity";"Wolfgang Karl Härdle and  Maria Osipenko";"B1";"2011-03-03";"C01,  C31";" Due to dependency of energy demand on temperature, weather derivatives enable the effective hedging of temperature related fluctuations. However, temperature varies in space and time and therefore the contingent weather derivatives also vary. The spatial derivative price distribution involves a risk premium. We examine functional principal components of temperature variation for this spatial risk premium. We employ a pricing model for temperature derivatives based on dynamics modelled via a vectorial Ornstein-Uhlenbeck process with seasonal variation. We use an analytical expression for the risk premia depending on variation curves of temperature in the measurement period. The dependence is exploited by a functional principal component analysis of the curves. We compute risk premia on cumulative average temperature futures for locations traded on CME and fit to it a geographically weighted regression on functional principal component scores. It allows us to predict risk premia for nontraded locations and to adopt, on this basis, a hedging strategy, which we illustrate in the example of Leipzig."
"444";444;"2011-014";"Difference based Ridge and Liu type Estimators in Semiparametric Regression Models";"Esra Akdeniz Duran, Wolfgang Karl Härdle and  Maria Osipenko";"B1";"2011-03-03";"C14,  C51";" We consider a difference based ridge regression estimator and a Liu type estimator of the regression parameters in the partial linear semiparametric regression model, y = XÎ² + f + Îµ. Both estimators are analysed and compared in the sense of mean-squared error. We consider the case of independent errors with equal variance and give conditions under which the proposed estimators are superior to the unbiased difference based estimation technique. We extend the results to account for heteroscedasticity and autocovariance in the error terms. Finally, we illustrate the performance of these estimators with an application to the determinants of electricity consumption in Germany."
"445";445;"2011-015";"Short-Term Herding of Institutional Traders: New Evidence from the German Stock Market";"Stephanie Kremer and  Dieter Nautz";"C14";"2011-03-10";"D81,  G11,  G24";" This paper employs a new and comprehensive data set to investigate short-term herding behavior of institutional investors. Using data of all transactions made by financial institutions in the German stock market, we show that herding behavior occurs on a daily basis. However, in contrast to longer-term herding measures obtained from quarterly data, results based on daily data do not indicate that short-term herding tends to be more pronounced in small capitalized stocks or in times of market stress. Moreover, we find that herding measures based on anony- mous transactions can lead to misleading results about the behavior of institutional investors during the recent financial crisis."
"446";446;"2011-016";"Oracally Efficient Two-Step Estimation of Generalized Additive Model";"Rong Liu, Lijian Yang and  Wolfgang Karl Härdle";"B1";"2011-03-14";"C00,  C14,  J01,  J31";" Generalized additive models (GAM) are multivariate nonparametric regressions for non-Gaussian responses including binary and count data. We propose a spline-backfitted kernel (SBK) estimator for the component functions. Our results are for weakly dependent data and we prove oracle efficiency. The SBK techniques is both computational expedient and theoretically reliable, thus usable for analyzing high-dimensional time series. Inference can be made on component functions based on asymptotic normality. Simulation evidence strongly corroborates with the asymptotic theory."
"447";447;"2011-017";"The Law of Attraction: Bilateral Search and Horizontal Heterogeneity";"Dirk Hofmann and  Salmai Qari";"A8";"2011-03-18";"D13,  D61,  J12";" We study a matching model with heterogeneous agents, nontransferable utility and search frictions. Agents differ along a horizontal dimension (e.g. taste) and a vertical dimension (e.g. income). AgentsÂ’ preferences coincide only in the vertical dimension. This approach introduces individual preferences in this literature as seems suitable in applications like labor markets (e.g. regional preferences). We analyze how the notion of assortativeness generalizes to integration or segregation outcomes depending on search frictions. Contrary to results from the purely vertical analysis, here, agents continuously adjust their reservation utility strategies to changing search frictions. The model is easily generalizable in the utility specification, the distribution of taste-related payoffs and the number of vertical types. Extreme utility specifications can be treated as a case of horizontal heterogeneity only."
"448";448;"2011-018";"Can crop yield risk be globally diversified?";"Xiaoliang Liu, Wei Xu and  Martin Odening";"C11";"2011-03-18";"C14,  Q19";" In 2007 and 2008 world food markets observed a significant price boom. Crop failures simultaneously occurring in some of the worldÂ’s major production regions have been quoted as one factor among others for the price boom. Against this background, we analyse the stochasticity of crop yields in major production areas. The analysis is exemplified for wheat, which is one of the most important crops worldwide. Particular attention is given to the stochastic dependence of yields in different regions. Thereby we address the question of whether local fluctuations of yields can be smoothed by international agricultural trade, i.e. by global diversification. The analysis is based on the copula approach, which requires less restrictive assumptions compared with linear correlations. The use of copulas allows for a more reliable estimation of extreme yield shortfalls, which are of particular interest in this application. Our calculations reveal that a production shortfall, such as in 2007, is not a once in a lifetime event. Instead, from a statistical point of view, similar production conditions will occur every 15 years."
"449";449;"2011-019";"What Drives the Relationship Between Inflation and Price Dispersion? Market Power vs. Price Rigidity";"Sascha Becker";"C14";"2011-03-23";"C23,  D40,  E31,  F15";" Recent monetary search and Calvo-type models predict that the relationship between inflation and price dispersion is U-shaped, implying an optimal rate of inflation above zero. Moreover, monetary search models emphasize a critical dependence of the real effects of inflation on sellersÂ’ market power, whereas Calvotype models suggest that the degree of price rigidity significantly affects the inflation - price dispersion nexus. Using a new set of highly disaggregated sectoral price data from a panel of European countries, this paper contributes to the literature by testing the empirical relevance of these two theoretical predictions. In line with monetary search theory, a U-shaped profile is found, provided that markups are sufficiently high, but the relationship breaks down under a more competitive environment. Contrarily, no evidence is found to support the contentions of Calvo-type models: U-shaped effects of inflation occur in product sectors with sticky as well as highly flexible prices."
"450";450;"2011-020";"How Computational Statistics Became the Backbone of Modern Data Science";"James E. Gentle, Wolfgang Karl Härdle and  Yuichi Mori";"B1";"2011-05-03";"C15";" This first chapter serves as an introduction and overview for a collection of articles surveying the current state of the science of computational statistics. Earlier versions of most of these articles appeared in the first edition of Handbook of Computational Statistics: Concepts and Methods, published in 2004. There have been advances in all of the areas of computational statistics, so we feel that it is time to revise and update this Handbook. This introduction is a revision of the introductory chapter of the first edition."
"451";451;"2011-021";"Customer Reactions in Out-of-Stock Situations – Do promotion-induced phantom positions alleviate the similarity substitution hypothesis?";"Jana Luisa Diels and  Nicole Wiebach";"B2";"2011-05-03";"M31,  C12,  C13,  C81";" Out-of-Stock (OOS) is a prevalent problem customers face at the POS. In this paper, we demonstrate both theoretically and empirically how OOS-induced substitution patterns can be explained and predicted by means of context and phantom theory. We further analyze the relevance of promotions, for which OOS is most pronounced, as essential driver of differences in customersÂ’ OOS reactions. The results of an online experiment demonstrate that customers substitute unavailable items in accordance to a negative similarity effect which is reduced, however, for OOS items on promotion. The empirical findings further suggest that customersÂ’ OOS responses differ for promoted vs. non-promoted items. We find that customers being affected by a stock-out of promotional products significantly more often postpone purchases and tend to avoid substitution resulting in severe losses for the retailer. However, for non-promoted items, customers easily switch to alternative brands. That way, manufacturers lose profit and possibly loyal customers."
"452";452;"2011-022";"Extreme value models in a conditional duration intensity framework";"Rodrigo Herrera and  Bernhard Schipp";"Z";"2011-05-10";"C22,  C58,  F30";" The analysis of return series from financial markets is often based on the Peaks-over-threshold (POT) model. This model assumes independent and identically distributed observations and therefore a Poisson process is used to characterize the occurrence of extreme events. However, stylized facts such as clustered extremes and serial dependence typically violate the assumption of independence. In this paper we concentrate on an alternative approach to overcome these difficulties. We consider the stochastic intensity of the point process of exceedances over a threshold in the framework of irregularly spaced data. The main idea is to model the time between exceedances through an Autoregressive Conditional Duration (ACD) model, while the marks are still being modelled by generalized Pareto distributions. The main advantage of this approach is its capability to capture the short-term behaviour of extremes without involving an arbitrary stochastic volatility model or a prefiltration of the data, which certainly impacts the estimation. We make use of the proposed model to obtain an improved estimate for the Value at Risk. The model is then applied and illustrated to transactions data from Bayer AG, a blue chip stock from the German stock market index DAX."
"453";453;"2011-023";"Forecasting Corporate Distress in the Asian and Pacific Region";"Russ Moro, Wolfgang Härdle, Saeideh Aliakbari and  Linda Hoffmann";"B1";"2011-05-20";"C14,  G33,  C45";" This study analyses credit default risk for firms in the Asian and Pacific region by applying two methodologies: a Support Vector Machine (SVM) and a logistic regression (Logit). Among different financial ratios suggested as predictors of default, leverage ratios and the company size display a higher discriminating power compared to others. An analysis of the dependencies between PD and financial ratios is provided along with a comparison with Europe (Germany). With respect to forecasting accuracy the SVM has a lower model risk than the Logit on average and displays a more robust performance. This result holds true across different years."
"454";454;"2011-024";"Identifying the Effect of Temporal Work Flexibility on Parental Time with Children";"Juliane Scheffel";"C7";"2011-05-20";"J08,  J13,  J21,  J22";" It is recognized that employment policies must grant flexibility to theworking schedules to allow parents to reconcile family and work. By exploiting the particularity of the East German labor market, I identify the causal effect of temporal work flexibility on parental time with children. The analysis unambiguously shows that it allows parents to spend about 30 percent more time with their children. The results can be generalized to Germany as a whole. It can be concluded that temporal work flexibility can be used as a device to mitigate the adverse effect of parental employment on the childÂ’s cognitive development."
"455";455;"2011-025";"How do Unusual Working Schedules Affect Social Life?";"Juliane Scheffel";"C7";"2011-05-20";"J22,  J28,  J81,  D62";" The widening of the working hour distribution complicates the coordination of social leisure. This paper examines the short- and long-run impact of unusual working schedules on social life using German Time Use Data for 2001/02. I find evidence that younger workers with higher than median earnings seem to accept higher levels of solitary leisure as investment and because of the substantial wage premia. Younger workers tend to substitute sleep with free time. Older workers, in contrast, tend to sleep less which can be interpreted as elevated risk of mental and physical health."
"456";456;"2011-026";"Compensation of Unusual Working Schedules";"Juliane Scheffel";"C7";"2011-05-20";"J22,  J31,  J33,  J81";" This paper examines pecuniary aspects of work during unusual hours based on the German Time Use Data for 2001/02. The findings show positive wage premia of 9 Â– 10 percent for shift workers and men who work during unusual hours. There is some evidence of negative selection which suggests that men with lower potential daytime earnings have a higher propensity to choose these jobs because of the associated wage premium. The findings further show a U-shaped impact of temporal work disamenity across the wage distribution with higher wage premia paid to the extreme 5-percentiles."
"457";457;"2011-027";"Estimation of the characteristics of a Lévy process observed at arbitrary frequency";"Johanna Kappus and  Markus Reiß";"C12";"2011-05-30";"C14,  C22";" A LÃ©vy process is observed at time points of distance Î” until time T. We construct an estimator of the LÃ©vy-Khinchine characteristics of the process and derive optimal rates of convergence simultaneously in T and Î”. Thereby, we encompass the usual low- and high-frequency assumptions and obtain also asymptotics in the mid-frequency regime."
"458";458;"2011-028";"Asymptotic equivalence and sufficiency for volatility estimation under microstructure noise";"Markus Reiß";"C12";"2011-05-30";"C14,  C58";" The basic model for high-frequency data in finance is considered, where an efficient price process is observed under microstructure noise. It is shown that this nonparametric model is in Le Cam's sense asymptotically equivalent to a Gaussian shift experiment in terms of the square root of the volatility function Ïƒ. As an application, simple rateoptimal estimators of the volatility and efficient estimators of the integrated volatility are constructed."
"459";459;"2011-029";"Pointwise adaptive estimation for quantile regression";"Markus Reiß, Yves Rozenholc and  Charles A. Cuenod";"C12";"2011-05-30";"C14,  C31";" A nonparametric procedure for quantile regression, or more generally nonparametric M-estimation, is proposed which is completely data-driven and adapts locally to the regularity of the regression function. This is achieved by considering in each point M-estimators over different local neighbourhoods and by a local model selection procedure based on sequential testing. Non-asymptotic risk bounds are obtained, which yield rate-optimality for large sample asymptotics under weak conditions. Simulations for different univariate median regression models show good finite sample properties,  also in comparison to traditional methods. The approach is the basis for  denoising CT scans in cancer research."
"460";460;"2011-030";"Developing web-based tools for the teaching of statistics: Our Wikis and the German Wikipedia";"Sigbert Klinke";"B1";"2011-05-31";"A22,  A23";" When we started the development of our CD to support the teaching of our basic statistic courses our work was twofold: we had to develop the content and the necessary software ourselves. Maintaining our software development in a university environment turned out to be especially difficult since it is time-consuming. At the same time the available open source software has become much more powerful such that our own development has become super uous. The use of \standard software"" (wikis) allows us to concentrate more on content rather than on software development. In several projects we have used wikis to support the teaching of our students and to publish their projects and homework. However the introduction of the bachelor/master system has reduced the willingness of students to make contributions to our wikis. As a consequence we have now started to involve ourselves and our students in more direct contributions to the German Wikipedia."
"461";461;"2011-031";"What Explains the German Labor Market Miracle in the Great Recession?";"Michael C. Burda and  Jennifer Hunt";"C7";"2011-06-03";"E24,  E65,  J23,  J33";" Germany experienced an even deeper fall in GDP in the Great Recession than the United States with little employment loss. EmployersÂ’ reticence to hire in the preceding expansion - associated in part with a lack of confidence it would last - contributed to an employment shortfall equivalent to 40 percent of the missing employment decline in the recession. Another 20 percent may be explained by wage moderation. A third important element was the widespread adoption of working time accounts, which permit employers to avoid overtime pay if hours per worker average to standard hours over a window. We find that this provided disincentives for employers to lay off workers in the downturn. While the overall cuts in hours per worker were consistent with the severity of the Great Recession, reduction of working time account balances substituted for traditional government-sponsored short time work. "
"462";462;"2011-032";"The information content of central bank interest rate projections: Evidence from New Zealand";"Gunda-Alexandra Detmers and  Dieter Nautz";"C14";"2011-06-07";"E52,  E58";" The Reserve Bank of New Zealand (RBNZ) has been the first central bank that began to publish interest rate projections in order to improve its guidance of monetary policy. This paper provides new evidence on the role of interest rate projections for market expectations about future shortterm rates and the behavior of long-term interest rates in New Zealand. We find that interest rate projections up to four quarters ahead play a significant role for the RBNZs expectations management before the crisis, while their empirical relevance has decreased ever since. For interest rate projections at longer horizons, the information content seems to be only weak and partially destabilizing."
"463";463;"2011-033";"Asymptotics of Asynchronicity";"Markus Bibinger";"C12";"2011-06-10";"C14,  C32,  C58,  G10";" In this article we focus on estimating the quadratic covariation of continuous semimartingales from discrete observations that take place at asynchronous observation times. The Hayashi-Yoshida estimator serves as synchronized realized covolatility for that we give our own distinct illustration based on an iterative synchronization algorithm. We consider high-frequency asymptotics and prove a feasible stable central limit theorem. The characteristics of non-synchronous observation schemes affecting the asymptotic variance are captured by a notion of asymptotic covariations of times. These are precisely illuminated and explicitly deduced for the important case of independent time-homogeneous Poisson sampling."
"464";464;"2011-034";"An estimator for the quadratic covariation of asynchronously observed Itô processes with noise: Asymptotic distribution theory";"Markus Bibinger";"C12";"2011-06-10";"C14,  C32,  C58,  G10";"  The article is devoted to the nonparametric estimation of the quadratic covariation of non-synchronously observed ItÃ´ processes in an additive microstructure noise model. In a high-frequency setting, we aim at establishing an asymptotic distribution theory for a generalized multiscale estimator including a feasible central limit theorem with optimal convergence rate on convenient regularity assumptions. The inevitably remaining impact of asynchronous deterministic sampling schemes and noise corruption on the asymptotic distribution is precisely elucidated. A case study for various important examples, several generalizations of the model and an algorithm for the implementation warrant the utility of the estimation method in applications."
"465";465;"2011-035";"The economics of TARGET2 balances";"Ulrich Bindseil and  Philipp Johann König";"C10";"2011-06-15";"E58,  F33,  F55";" It has recently been argued that intra-eurosystem claims and liabilities in the form of TARGET2 balances would raise fundamental issues within the European monetary union. This article provides a framework for the economic analysis of TARGET2 balances and discusses the key arguments behind this recent debate. The analysis is conducted within a system of financial accounts in which TARGET2 balances can arise either due to current account transactions or cross-border capital flows. It is argued that the recent volatility of TARGET2 balances reflects capital flow movements, while the previously prevailing current account positions did not find a strong reflection in TARGET2 balances. Some recent statements regarding TARGET2 appear to be due to a failure to distinguish between the monetary base (a central bank liability concept) and the liquidity deficit of the banking system vis-Ã -vis the central bank (a central bank asset concept). Furthermore, the article highlights the importance of TARGET2 for the stability of the euro area and points out that the proposal to limit the size of TARGET2 liabilities essentially contradicts the idea of a monetary union."
"466";466;"2011-036";"An Indicator for National Systems of Innovation - Methodology and Application to 17 Industrialized Countries";"Heike Belitz, Marius Clemens, Christian von Hirschhausen, Jens Schmidt-Ehmcke, Axel Werwatz and  Petra Zloczysti";"B3";"2011-06-27";"O30,  C81,  H52";" We develop a composite indicator measuring the performance of national innovation systems. The indicator takes into account both Â“hardÂ” factors that are quantifiable (such as R&D spending, number of patents) and Â“softÂ” factors like the assessment of preconditions for innovation by managers. We apply the methodology to a set of 17 industrialized countries on a yearly basis between 2007 and 2009. The indicator combines results from public opinion surveys on the process of change, social capital, trust and science and technology to achieve an assessment of a countryÂ’s social climate for innovation. After calculating and ranking the innovation indictor scores for the 17 countries, we group them into three classes: innovation leader, middle group and end section. Using multiple sensitivity analysis approaches, we show that the indicator reacts robustly to different weights within these country groups. While leading countries like Switzerland, the USA and the Nordic countries have an innovation system with high scores and ranks in every sub indicator, the middle group consisting among others of Germany Japan, the UK and France, can be characterized by higher variation within ranks. In the end section, countries like Italy and Spain have bad scores for almost all indicators."
"467";467;"2011-037";"Neurobiology of value integration: When value impacts valuation";"Soyoung Q. Park, Thorsten Kahnt, Jörg Rieskamp and  Hauke R. Heekeren";"A12";"2011-06-24";"";""
"468";468;"2011-038";"The Neural Basis of Following Advice";"Guido Biele, Jörg Rieskamp, Lea K. Krugel and  Hauke R. Heekeren";"A12";"2011-06-24";"";""
"469";469;"2011-039";"The Persistence of ""Bad"" Precedents and the Need for Communication: A Coordination Experiment";"Dietmar Fehr";"A6";"2011-06-28";"C72,   C92,   D23,   L23";" Precedents can facilitate successful coordination within groups by reducing strategic uncertainty,  but they may lead to coordination failure when two groups with diverging precedents have to interact. This paper describes an experiment to explore how such coordination failure can be mitigated and whether subjects are aware of it. In an initial phase, groups were able to establish a precedent in a repeated weakest-link game, and in a second phase two groups with dierent precedents are merged into a larger group. As expected, this leads to coordination failures. Unlike most of the previous literature, subjects could endogenously choose to communicate  in the merged group for a small fee. The results suggest that communication can mitigate the coordination failure in the merged group and, in most cases, leads to efficient coordination. However, subjects in particular from groups with an efficient precedent in the initial phase are inattentive to the potential coordination failure and choose not to communicate. This can have profound consequences since groups who fail to implement communication are unable to achieve efficient coordination in the second phase. The results may be useful for the understanding of how groups learn to solve coordination problems from past coordination success or failure."
"470";470;"2011-040";"News-driven Business Cycles in SVARs";"Patrick Bunk";"C7";"2011-07-04";"E30,  E32";" Recent studies proposed news about future technology growth as the main driver of macroeconomic fluctuations. The identification of these news through stock prices in SVARs has been criticized in the past. Therefore, I propose a series of experiments to test that hypothesis by examining its implications. If business cycles are mainly driven by news then these shocks should be captured by other time series as well. I find that news shocks identified through S&P 500 prices exhibit the same dynamics as news identified through a broader stock price index, patent applications, the relative price of investment or shocks to the real interest rate. The common theme among these identifications is a technological change in productivity that demands time to build, economic activity and natural resources to come into effect."
"471";471;"2011-041";"The Basel III framework for liquidity standards and monetary policy implementation";"Ulrich Bindseil and  Jeroen Lamoot";"C10";"2011-07-11";"E58,  G21,  G28";" Basel III introduces for the first time an international framework for liquidity risk regulation, reflecting the experience of excessive liquidity risk taking of banks in the run up to the financial crisis that erupted in August 2007, and associated negative externalities. As central banks play a crucial role in the liquidity provision to banks during normal times and in a financial crisis, the treatment of central bank operations in the regulation is obviously important. To ensure internalisation of liquidity risks (i.e. pricing of liquidity risk) and to address excessive reliance ex ante on central bank liquidity support by the banks, the regulation deliberately does not establish a direct close link with the monetary policy operational framework. While this reflects the purpose of the regulation and is also natural outcome of an international rule being applied under a multitude of very different monetary policy operational frameworks, this paper shows that the interaction between the two areas can be substantial, depending on the operational and collateral framework of the central bank. This implies the need for further study and the development of policies at the central bank and regulatory/supervisory side on how to handle these potential interactions in practice."
"472";472;"2011-042";"Pollution permits, Strategic Trading and Dynamic Technology Adoption";"Santiago Moreno-Bromberg and  Luca Taschini";"A11";"2011-07-11";"D8,  H2,  L5,  Q5";" This paper analyzes the dynamic incentives for technology adoption under a transferable permits system, which allows for strategic trading on the permit market. Initially, firms can both invest in low- emitting production technologies and trade permits. In the model, technology adoption and allowance prices are generated endogenously and are inter-dependent. It is shown that the non-cooperative permit trading game possesses a pure-strategy Nash equilibrium, where the allowance value reflects the level of uncovered pollution (demand), the level of unused allowances (supply), and the technological status. These conditions are also satisfied when a price support instrument (dubbed European-cash- for{permits), which is contingent on the adoption of the new technology, is introduced. Numerical investigation confirms that this policy generates a floating price floor for the allowances, and it restores the dynamic incentives to invest. Given that this policy comes at a cost, a criterion for the selection of a self-financing policy (based on convex risk measures) is proposed and implemented."
"473";473;"2011-043";"CRRA Utility Maximization under Risk Constraints";"Santiago Moreno-Bromberg, Traian A. Pirvu and  Anthony Réveillac";"A11";"2011-07-11";"G10";" This paper studies the problem of optimal investment with CRRA (constant, relative risk aversion) preferences, subject to dynamic risk constraints on trading strategies. The market model considered is continuous in time and incomplete; furthermore, financial assets are modeled by ItÃ´ processes. The dynamic risk constraints (time, state dependent) are generated by risk measures. The optimal trading strategy is characterized by a quadratic BSDE. Special risk measures (Value-at-Risk, Tail Value-at-Risk and Limited Expected Loss ) are considered and a three-fund separation result is established in these cases. Numerical results emphasize the effect of imposing risk constraints on trading."
"474";474;"2011-044";"Predicting Bid-Ask Spreads Using Long Memory Autoregressive Conditional Poisson Models";"Axel Groß-Klußmann and  Nikolaus Hautsch";"B8";"2011-07-12";"G14,  C32";" We introduce a long memory autoregressive conditional Poisson (LMACP) model to model highly persistent time series of counts. The model is applied to forecast quoted bid-ask spreads, a key parameter in stock trading operations. It is shown that the LMACP nicely captures salient features of bid-ask spreads like the strong autocorrelation and discreteness of observations. We discuss theoretical properties of LMACP models and evaluate rolling window forecasts of quoted bid-ask spreads for stocks traded at NYSE and NASDAQ. We show that Poisson time series models significantly outperform forecasts from ARMA, ARFIMA, ACD and FIACD models. The economic significance of our results is supported by the evaluation of a trade schedule. Scheduling trades according to spread forecasts we realize cost savings of up to 13 % of spread transaction costs."
"475";475;"2011-045";"Bayesian Networks and Sex-related Homicides";"Stephan Stahlschmidt, Helmut Tausendteufel and  Wolfgang K. Härdle";"B1";"2011-07-25";"C49,  C81,  K42";" We present a statistical investigation on the domain of sex-related homicides. As general sociological and psychological theory on this specific type of crime is incomplete or even lacking, a data-driven approach is implemented. In detail, graphical modelling is applied to learn the dependency structure and several structure learning algorithms are combined to yield a skeleton corresponding to distinct Bayesian Networks. This graph is subsequently analysed and presents a distinction between an offender and a situation driven crime."
"476";476;"2011-046";"The Regulation of Interdependent Markets";"Raffaele Fiocco and   Carlo Scarpa";"A8";"2011-07-28";"D82,  L51";" We examine the issue of whether two monopolists which produce substitutable goods should be regulated by one (centralization) or two (decentralization) regulatory authorities, when the regulator(s) can be partially captured by industry. Under full information, two decentral- ized agencies - each regulating a single market - charge lower prices than a unique regulator, making consumers better off. However, this leads to excessive costs for the taxpayers who subsidize the Â…rms, so that centralized regulation is preferable. Under asymmetric informa- tion about the firms' costs, lobbying induces a unique regulator to be more concerned with the industry's interests, and this decreases social welfare. When the substitutability between the goods is high enough, the firms'lobbying activity may be so strong that decentralizing the regulatory structure may be social welfare enhancing. Classification-JEL: D82, L51 Keywords: regulation, lobbying, asymmetric information, energy markets"
"477";477;"2011-047";"Bargaining and Collusion in a Regulatory Model";"Raffaele Fiocco and   Mario Gilli";"A8";"2011-07-28";"D73,  D82,  L51";" Within a standard three-tier regulatory model, a benevolent prin- cipal delegates to a regulatory agency two tasks: the supervision of the Â…rmÂ’s (two-type) costs and the arrangement of a pricing mecha- nism. The agency may have an incentive to manipulate information to the principal to share the gains of collusion with the Â…rm. The novelty of this paper is that both the regulatory mechanism and the side contracting between the agency and the Â…rm are modelled as a bargaining process. While as usual the ineÂ¢ cient Â…rm does not have any interest in cost manipulation, we Â…nd that the eÂ¢ cient Â…rm has an incentive to collude only if the agencyÂ’s bargaining power is high enough, and the total gains of collusion are now lower than those the two partners would appropriate if the agency could make a take-it-or- leave-it oÂ¤er. Then, we focus on the optimal institutional responses to the possibility of collusion. In our setting, where the incomplete- ness of contracts prevents the principal from designing of a screening mechanism and thus TiroleÂ’s equivalence principle does not apply, we show how the playersÂ’bargaining powers crucially drive the optimal response to collusion. Keywords: bargaining, collusion, regulation. JEL classiÂ…cation: D73, D82, L51."
"478";478;"2011-048";"Large Vector Auto Regressions";"Song Song and   Peter J. Bickel";"B1";"2011-08-03";"C13,  C14,  C32,  E30,  E40,  ";" One popular approach for nonstructural economic and financial forecasting is to include a large number of economic and financial variables, which has been shown to lead to significant improvements for forecasting, for example, by the dynamic factor models. A challenging issue is to determine which variables and (their) lags are relevant, especially when there is a mixture of serial correlation (temporal dynamics), high dimensional (spatial) dependence structure and moderate sample size (relative to dimensionality and lags). To this end, an integrated solution that addresses these three challenges simultaneously is appealing. We study the large vector auto regressions here with three types of estimates. We treat each variable's own lags different from other variables' lags, distinguish various lags over time, and is able to select the variables and lags simultaneously. We first show the consequences of using Lasso type estimate directly for time series without considering the temporal dependence. In contrast, our proposed method can still produce an estimate as efficient as an oracle under such scenarios. The tuning parameters are chosen via a data driven ""rolling scheme"" method to optimize the forecasting performance. A macroeconomic and financial forecasting problem is considered to illustrate its superiority over existing estimators."
"479";479;"2011-049";"Monetary Policy, Determinacy, and the Natural Rate Hypothesis";"Alexander Meyer-Gohde";"C7";"2011-08-03";"C62;E31;E43;E52";" Imposing the natural rate hypothesis (NRH) can dramatically alter the determinacy bounds on monetary policy by closing the output gap in the long run. I show that the hypothesis eliminates any role for the output gap in determinacy and renders the conditions for determinacy identical for all conforming supply equations. Specializing further to IS demand, determinacy depends only on the parameters in the interest rate rule and a pure forward or backward-looking inflation target is inconsistent with determinacy. Monetary policy that embodies the Taylor principle with respect to contemporaneous inflation delivers a determinate equilibrium in all models that satisfy the NRH."
"480";480;"2011-050";"The impact of context and promotion on consumer responses and preferences in out-of-stock situations";"Nicole Wiebach and   Jana L. Diels";"B2";"2011-08-09";"M31,  C12,  C13,  C81";" In general, consumer preferences depend on the context of a decision situation. This paper highlights  the context-dependence of substitution behavior in out-of-stock (OOS) situations and provides evidence for the  relevance of promotion as essential driver of customers' OOS reactions. We demonstrate both theoretically  and empirically how OOS-induced preference shifts can be explained and predicted using context and phantom theory.  In a series of experiments, we show that consumers substitute in accordance to a negative similarity effect,  which is reduced for stock-outs of promoted low-involvement FCMGs. If a similar substitute is offered at  a reduced price, the effect is enforced. For dissimilar substitutes, we show the contrary. The empirical findings  further suggest an augmented probability of purchase postponement and a significant smaller chance of brand switching  for stock-outs of promotional products. Furthermore, our study emphasizes outlet switching as a so far uninvestigated  OOS reaction and discusses implications for retailers and manufacturers."
"481";481;"2011-051";"A Network Model of Financial System Resilience";"Kartik Anand,  Prasanna Gai,  Sujit Kapadia,  Simon Brennan and   Matthew Willison";"C10";"2011-08-12";"C63,  G01,  G17,  G21.";" We examine the role of macroeconomic fluctuations, asset market liquidity, and network structure in determining contagion and aggregate losses in a financial system. Systemic instability is explored in a financial network comprising three distinct, but interconnected, sets of agents Â– domestic banks, international financial institutions, and firms. Calibrating the model to advanced country banking sector data, we obtain sensible aggregate loss distributions which are bimodal in nature. We demonstrate how systemic crises may occur and analyze how our results are influenced by firesale externalities and the feedback effects from curtailed lending in the macroeconomy. We also illustrate the resilience of our model financial system to stress scenarios with sharply rising corporate default rates and falling asset prices."
"482";482;"2011-052";"Rollover risk, network structure and systemic financial crises";"Kartik Anand,  Prasanna Gai and   Matteo Marsili";"C10";"2011-08-12";"C72,  G01,  G21";" The breakdown of short-term funding markets was a key feature of the global financial crisis of 2007/8. Combining insights from the literature on global games and network growth, we develop a simple model that sheds light on how network topology interacts with the funding structure of financial institutions to determine system-wide crises. We show how the arrival of bad news about a financial institution leads others to lose confidence in it and how this, in turn, spreads across the entire interbank network. The rate of system-wide bank failure is rendered endogenous, depending crucially on both the rate at which bad news arrives and on the maturity of debt contracts. The conditions under which the financial system makes a sharp transition from a dense network of credit relations to a sparse network where credit freezes readily occur are characterized. Our results also emphasize the role of hysteresis Â– once broken, credit relations take a long time to re-establish as a result of common knowledge of the equilibrium. Our findings shed light on the nature of public policy responses both during and after the crisis."
"483";483;"2011-053";"When to Cross the Spread: Curve Following with Singular Control";"Felix Naujokat and   Ulrich Horst";"A11";"2011-08-15";"C61,  G11";" In this article the problem of curve following in an illiquid market is addressed. Using techniques of singular stochastic control, we extend the results of [NW11] to a two- sided limit order market with temporary market impact and resilience, where the bid ask spread is now also controlled. We first show existence and uniqueness of an optimal control. In a second step, a suitable version of the stochastic maximum principle is derived which yields a characterisation of the optimal trading strategy in terms of a nonstandard coupled FBSDE. We show that the optimal control can be characterised via buy, sell and no-trade regions. The new feature is that we now get a nondegenerate no-trade region, which implies that market orders are only used when the spread is small. This allows to describe precisely when it is optimal to cross the bid ask spread, which is a fundamental problem of algorithmic trading. We also show that the controlled system can be described in terms of a reflected BSDE. As an application, we solve the portfolio liquidation problem with passive orders. AMS 2000 subject classifications: 93E20, 91G80 JEL classification: C61, G11. Keywords and phrases: Stochastic maximum principle, Convex analysis, Fully coupled forward backward stochastic differential equations, Trading in illiquid markets.."
"484";484;"2011-054";"TVICA - Time Varying Independent Component Analysis and Its Application to Financial Data";"Ray-Bing Chen,  Ying Chen and   Wolfgang K. Härdle";"B1";"2011-08-19";"C14,  C58,  G17";" Source extraction and dimensionality reduction are important in analyzing high dimensional and complex nancial time series that are neither Gaussian distributed nor stationary. Independent component analysis (ICA) method can be used to factorize the data into a linear combination of independent compo- nents, so that the high dimensional problem is converted to a set of univariate ones. However conventional ICA methods implicitly assume stationarity or stochastic homogeneity of the analyzed time series, which leads to a low accu- racy of estimation in case of a changing stochastic structure. A time varying ICA (TVICA) is proposed here. The key idea is to allow the ICA lter to change over time, and to estimate it in so-called local homogeneous intervals. The question of how to identify these intervals is solved by the LCP (local change point) method. Compared to a static ICA, the dynamic TVICA pro- vides good performance both in simulation and real data analysis. The data example is concerned with independent signal processing and deals with a port- folio of highly traded stocks. JEL code: C14; C58; G17 Keywords: Adaptive Sequential Testing; Independent Component Analysis; Local Homogeneity; Signal Processing; Realized Volatility."
"485";485;"2011-055";"Pricing Chinese rain: a multisite multi-period equilibrium pricing model for rainfall derivatives";"Wolfgang K. Härdle and   Maria Osipenko";"B1";"2011-08-21";"C22,  C51,  G13";" Many industries are exposed to weather risk which they can transfer on nancial markets via weather derivatives. Equilibrium models based on partial market clearing became a useful tool for pricing such kind of nancial instruments. In a multi-period equilibrium pricing model agents rebalance their portfolio of weather bonds and a risk free asset in each period such that they maximize the expected utility of their incomes constituted by possibly weather dependent prots and payos of portfolio positions. We extend the model to a multisite version and apply it to pricing rainfall derivatives for Chinese provinces. By simulating realistic market conditions with two agent types, farmers with prots highly exposed to weather risk and a nancial investor diversifying her nancial portfolio, we obtain equilibrium prices for weather derivatives on cumulative monthly rainfall. Dynamic portfolio optimization under market clearing and utility indifference of these representative agents determines equilibrium quantity and price for rainfall derivatives. Keywords: rainfall derivatives; equilibrium pricing; space-time Markov model JEL Classication: C22, C51, G13."
"486";486;"2011-056";"Limit Order Flow, Market Impact and Optimal Order Sizes: Evidence from NASDAQ TotalView-ITCH Data";"Nikolaus Hautsch and   Ruihong Huang";"B8";"2011-08-25";"G14,  C32,  G17";" In this paper, we provide new empirical evidence on order submission activity and price impacts of limit orders at NASDAQ. Employing NASDAQ TotalView-ITCH data, we find that market participants dominantly submit limit orders with sizes equal to a round lot. Most limit orders are canceled almost immediately after submission if not getting executed. Moreover, only very few market orders walk through the book, i.e., directly move the best ask or bid quote. Estimates of impulse-response functions on the basis of a cointegrated VAR model for quotes and market depth allow us to quantify the market impact of incoming limit orders. We propose a method to predict the optimal size of a limit order conditional on its position in the book and a given fixed level of expected market impact. Keywords: price impact, limit order, impulse response function, cointegration, optimal order size JEL classification: G14, C32, G17"
"487";487;"2011-057";"Optimal Display of Iceberg Orders";"Gökhan Cebiroglu and   Ulrich Horst";"A11";"2011-08-29";"C51,  C60,  C67,  D01,  D4,  D";" We develop a sequential trade model of Iceberg order execution in a limit order book. The Iceberg-trader has the freedom to expose his trading intentions or (partially) shield the true order size against other market participants. Order exposure can cause drastic market reactions (Â“market impactÂ”) in the end leading to higher transaction costs. On the other hand the Iceberg trader faces a loss-in-priority when he hides his intentions, as most electronic limit order books penalize the usage of hidden liquidity. Thus the Iceberg-trader is faced with the problem to find the right trade-off. Our model provides optimal exposure strategies for Iceberg traders in limit order book markets. In particular, we provide a range of analytical statements that are in line with recent empirical findings on the determinants of traderÂ’s exposure strategies. In this framework, we also study the market impact also market impact of limit orders. We provide optimal exposure profiles for a range of high- tech stocks from the US S&P500 and how they scale with the state-of-the-book. We finally test the IcebergÂ’s performance against the limit orders and find that Iceberg orders can significantly enhance trade performance by up to 60%. JEL classification: C51,C60,C67,D01,D4,D49,G1 Keywords: Hidden Liquidity, Iceberg Orders, Limit Order Book, Market Impact of Limit Orders, Optimal Exposure, Trading Strategies, Iceberg versus Limit Order, Pre-trade trans- parency, Agency-Trading."
"488";488;"2011-058";"Optimal liquidation in dark pools";"Peter Kratz and   Torsten Schöneborn";"A11";"2011-09-01";"C02,  C61,  G11";" We consider a large trader seeking to liquidate a portfolio using both a transparent trading venue and a dark pool. Our model captures the price impact of trading in transparent traditional venues as well as the execution uncertainty of trading in a dark pool. The unique optimal execution strategy uses both venues continuously. The order size in the dark pool can over- or underrepresent the portfolio size depending on adverse selection and the correlation structure of the assets in the portfolio. Introduction a dark pool results in delayed trading at the traditional venue. The appeal of the dark pool is increased by liquidity but reduced by adverse selection. By pushing up prices at the traditional venue and parallel selling in the dark pool, a trader might generate profits; we provide sufficient conditions to rule out such profitable price manipulation strategies. Keywords: Dark pools, Optimal liquidation, Adverse selection, Market microstructure, Illiquid markets JEL: C02, C61, G11"
"489";489;"2011-059";"The Merit of High-Frequency Data in Portfolio Allocation";"Nikolaus Hautsch,  Lada M. Kyj and   Peter Malec";"B8B11";"2011-09-29";"G11,  G17,  C58,  C14,  C38";" This paper addresses the open debate about the effectiveness and practical relevance of highfrequency (HF) data in portfolio allocation. Our results demonstrate that when used with proper econometric models, HF data offers gains over daily data and more importantly these gains are maintained over longer horizons than previous studies have shown. We propose a Multi-Scale Spectral Components model for forecasting high-dimensional covariance matrices based on realized measures employing HF data. Extensive performance evaluation confirms that the proposed approach dominates prevailing methods and validates the intuition that HF data used properly can translate into better portfolio allocation decisions."
"490";490;"2011-060";"On the Continuation of the Great Moderation:New evidence from G7 Countries";"Wenjuan Chen";"C14";"2011-09-30";"E20,  F01,  G01,  N10";" This paper employs a Markov regime-switching approach to investigate whether the Great Moderation is over since the start of the late 2000s recession. The results conrm that the recent nancial crisis did cause a simultaneous high-volatility period among the G7 countries. However, the nancial crisis may not mark the end of the Great Moderation. There is strong evidence that each G7 country has again returned to the low-variance state since 2009 or the beginning of 2010."
"491";491;"2011-061";"Forward-backward systems for expected utility maximization";"Ulrich Horst,  Ying Hu,  Peter Imkeller,  Anthony Reveillac and   Jianing Zhang";"A11";"2011-10-14";"C61,  D52,  D53";" In this paper we deal with the utility maximization problem with a general utility function. We derive a new approach in which we reduce the utility maximization prob- lem with general utility to the study of a fully-coupled Forward-Backward Stochastic Differential Equation (FBSDE).  AMS Subject Classification: Primary 60H10, 93E20 JEL Classification: C61, D52, D53"
"492";492;"2011-062";"On heterogeneous latent class models with applications to the analysis of rating scores";"Aurélie Bertrand and  Christian M. Hafner";"Z";"2011-10-17";"C35,  C38,  C87,  M31";" Discovering the preferences and the behaviour of consumers is a key challenge in mar- keting. Information about such topics can be gathered through surveys in which the respondents must assign a score to a number of items. In this article we suggest a strat- egy to analyze such data and achieve this objective: it consists in identifying groups of consumers whose response patterns are similar and characterizing them in terms of pref- erences and covariates. We use latent class models allowing for heterogeneity of both latent class and within-class probabilities across individuals. We illustrate the proposed methodology using data about the preferences of Belgian households for supermarkets."
"493";493;"2011-063";"Multivariate Volatility Modeling of Electricity Futures";"Luc Bauwens, Christian M. Hafner and  Diane Pierret";"Z";"2011-10-17";"C32,  C53,  C58";" We model the dynamic volatility and correlation structure of electricity futures of the European Energy Exchange index. We use a new multiplicative dynamic conditional correlation (mDCC) model to separate long-run from short-run components. We allow for smooth changes in the unconditional volatilities and correlations through a multiplicative component that we estimate nonparametrically. For the short-run dynamics, we use a GJR-GARCH model for the conditional variances and augmented DCC models for the conditional correlations. We also introduce exogenous variables to account for congestion and delivery-date effects in short-term conditional variances. We find different correlation dynamics for long and short-term contracts and the new model achieves higher forecasting performance compared to a standard DCC model."
"494";494;"2011-064";"Semiparametric Estimation with Generated Covariates";"Enno Mammen, Christoph Rothe and  Melanie Schienle";"B11";"2011-10-17";"C14,  C31";" In this paper, we study a general class of semiparametric optimization estimators of a vector-valued parameter. The criterion function depends on two types of innite-dimensional nuisance parameters: a conditional expectation function that has been estimated nonparametrically using generated covariates, and another estimated function that is used to compute the generated covariates in the first place. We study the asymptotic properties of estimators in this class, which is a nonstandard problem due to the presence of generated covariates. We give conditions under which estimators are root-n consistent and asymptotically normal, and derive a general formula for the asymptotic variance."
"495";495;"2011-065";"Linking corporate reputation and shareholder value using the publication of reputation rankings";"Sven Tischer and  Lutz Hildebrandt";"B2";"2011-10-19";"M14,  M31,  G14,  G11,  C12,  ";" Good corporate reputation is seen as one of the most valuable assets.  It is believed to cause a multitude of favorable impacts within different stakeholder groups.  As a consequence, a multitude of studies analyzed the relationship between corporate reputation  and financial performance. However, the most of them raised the question of causation due to their methodology.  In order to isolate the impact of corporate reputation on financial performance, some authors had conducted  event studies, but without any success. Therefore, this study provides a comprehensive theoretical background,  why reputation has to affect financial performance. According to this theory, two event studies are conducted to  analyze the impact of publishing reputation rankings of the German Manager Magazine from 1998 to 2008 on share  prices. As expected, we find positive or negative announcement effects regarding upgraded or respectively  downgraded companies. Consequently, investors gain new information from the published rankings  (increase or decrease in reputation) to adjust share prices."
"496";496;"2011-066";"Monitoring, Information Technology and the Labor Share";"Dorothee Schneider";"C7";"2011-10-19";"D24,  J30,  E25";" This paper assesses empirically the hypotheses by Bental and Demougin (2010) that innovations in ICT (Information and Communication Technology) reduce the labor share in OECD countries by improving the monitoring technology. In a first step, I show that data trends for the labor share, wages in efficiency units, and labor in efficiency units over capital can be matched by a simulation of the model of Bental and Demougin (2010). In a second approach, I confirm increasing monitoring of workers using micro data for Germany. I argue that ICT influences labor not only through substitutability of labor with ICT and foreign work, but also through to lowering rents of workers as monitoring technology improves."
"497";497;"2011-067";"Minimal Supersolutions of BSDEs with Lower Semicontinuous Generators";"Gregor Heyne, Michael Kupper and  Christoph Mainberger";"A11";"2011-10-19";"C61,  C65,  G11";" We study the existence and uniqueness of minimal supersolutions of backward stochastic differential equations with generators that are jointly lower semicontinuous, bounded below by an affine function of the control variable and satisfy a specific normalization property."
"498";498;"2011-068";"Bargaining, Openness, and the Labor Share";"Dorothee Schneider";"C7";"2011-10-19";"E25,  J23,  F16,  O33,  E02";" This paper investigates determinants of changes of the labor share in developed countries with a focus on Western Europe. Using a country-industry panel that covers the private sector, the paper focuses on long and short-run changes within industries. The results show a large and time-persistent impact of increasing globalization on the labor share, especially if the within-industry changes are con- sidered. Openness seems to be the driving force for downward movements in the industry level labor shares while technological and institutional forces impact these shares positively. Furthermore, while investments into information and communi- cation technology (ICT) increase productivity of workers, it has a negative impact on the labor share as it enables higher economic integration which lowers the labor share. Economic integration has stronger impact on the polarization in Western European labor markets than ICT."
"499";499;"2011-069";"The Labor Share: A Review of Theory and Evidence";"Dorothee Schneider";"C7";"2011-10-19";"E25,  J23,  J01";" The labor share is defined as the share of value added which is payed out to workers. It is therefore often also called the wage share. Generally it is assumed that value added is produced with capital and labor as input factors so that Y = F(K;L) where Y is value added or output, K the capital input, and L labor."
"500";500;"2011-070";"The Power of Sunspots: An Experimental Analysis";"Dietmar Fehr, Frank Heinemann and  Aniol Llorente-Saguer";"A6C10";"2011-10-26";"C72,  C92,  D84";" We present an experiment in which extrinsic information (signals) may generate sunspot equilibria. The underlying coordination game has a unique symmetric non-sunspot equilibrium, which is also risk-dominant. Other equilibria can be ordered according to risk dominance. We compare treatments with different salient, but extrinsic signals. By increasing the precision of private signals, we manipulate the available public information, which allows us to measure the force of extrinsic signals. We also vary the number of signals and combine public and private signals, allowing us to see how subjects aggregate available (and possibly irrelevant) information. Results indicate that sunspot equilibria emerge naturally if there are salient (but extrinsic) public signals. However, salient private signals of high precision may also cause sunspot-driven behavior, even though this is no equilibrium. The higher the precision of signals and the easier they can be aggregated, the more powerful they are in dragging behavior away from the risk-dominant to risk-dominated strategies. Sunspot-driven behavior may lead to welfare losses and exert negative externalities on agents, who do not receive extrinsic signals."
"501";501;"2011-071";"Econometric analysis of volatile art markets";"Fabian Y. R. P. Bocart and  Christian M. Hafner";"Z";"2011-10-27";"C14,  C43,  Z11";" A new heteroskedastic hedonic regression model is suggested which takes into account time-varying volatility and is applied to a blue chips art market. A nonparametric local likelihood estimator is proposed, and this is more precise than the often used dummy variables method. The empirical analysis reveals that errors are considerably non-Gaussian, and that a student distribution with time-varying scale and degrees of freedom does well in explaining deviations of prices from their expectation. The art price index is a smooth function of time and has a variability that is comparable to the volatility of stock indices."
"502";502;"2011-072";"Financial Network Systemic Risk Contributions";"Nikolaus Hautsch, Julia Schaumburg and  Melanie Schienle";"B8B11";"2011-10-26";"G01,  G18,  G32,  G38,  C21,  ";" We propose the systemic risk beta as a measure for financial companiesÂ’ contribution to systemic risk given network interdependence between firmsÂ’ tail risk exposures. Conditional on statistically pre-identified network spillover effects and market and balance sheet information, we define the systemic risk beta as the time-varying marginal effect of a firmÂ’s Value-at-risk (VaR) on the systemÂ’s VaR. Suitable statistical inference reveals a multitude of relevant risk spillover channels and determines companiesÂ’ systemic importance in the U.S. financial system. Our approach can be used to monitor companiesÂ’ systemic importance allowing for a transparent macroprudential regulation."
"503";503;"2011-073";"Calibration of selfdecomposable Lévy models";"Mathias Trabs";"C12";"2011-11-03";"C14,   G13";" We study the nonparametric calibration of exponential, self-decomposable LÃ©vy models whose jump density can be characterized by the k-function, which is typically nonsmooth at zero. On the one hand the estimation of the drift, the activity measure alpha:= k(0+) + k(0-) and analog parameters for the derivatives are considered and on the other hand we estimate the k-function outside of a neighborhood of zero. Minimax convergence rates are derived, which depend on . Therefore, we construct estimators adapting to this unknown parameter. Our estimation method is based on spectral representations of the observed option prices and on regularization by cutting off high frequencies. Finally, the procedure is applied to simulations and real data."
"504";504;"2011-074";"Time-Varying Occupational Contents: An Additional Link between Occupational Task Profiles and Individual Wages";"Alexandra Fedorets";"A9";"2011-11-03";"J24,  J62,  I21,  O39";" By analyzing occupational task profiles, an occupational change can be split up into two components: (1) transferability of task portfolios between occupations and (2) change in the value of the occupation-employee match. Extending the task-based approach of Gathmann and SchÃ¶nberg (2009) by relaxing their assumption of time-invariant occupational contents, I estimate the association of dynamic aspects of occupational task portfolios with individual wages for medium-skilled German workers in 1991 and 1998. Estimated wage returns to the components of an occupational change generally differ in short and long run, as well as in East and West Germany. Wage returns to the changes of task porfolios for the occupational stayers are estimated to be positive."
"505";505;"2011-075";"Changes in Occupational Demand Structure and their Impact on Individual Wages";"Alexandra Fedorets";"A9";"2011-11-03";"J24,  J62,  I21,  P21";" This paper estimates wage losses arising due to changes in the structure of demand for occupations. The data on occupational changes made for the sake of adjustment to the changes in the demand structure come from the German reunification of 1990. Endogenous occupational changes are instrumented by the post-reunification demand properties of the occupation of the apprenticeship completed in the GDR. The IV computation reveals a negative wage effect of nearly 35 log points in 1991/92. This effect is persistent over time: after almost 10 years after reunification the negative wage effect associated with occupational changes due to the relocation of individual human capital across occupations is more than 20 log points."
"506";506;"2011-076";"Nonparametric Nonstationary Regression with Many Covariates";"Melanie Schienle";"B11";"2011-11-10";"C14,  C22";" This article studies nonparametric estimation of a regression model for d â‰¥ 2 potentially non- stationary regressors. It provides the first nonparametric procedure for a wide and important range of practical problems, for which there has been no applicable nonparametric estimation technique before. Additive regression allows to circumvent the usual nonparametric curse of dimensionality and the additionally present, nonstationary curse of dimensionality while still pertaining high mod- eling flexibility. Estimation of an additive conditional mean function can be conducted under weak conditions: It is sufficient that the response Y and all univariate Xj and pairs of bivariate marginal components Xjk of the vector of all covariates X are (potentially nonstationary) Î²-null Harris re- current processes. The full dimensional vector of regressors X itself, however, is not required to be Harris recurrent. This is particularly important since e.g. random walks are Harris recurrent only up to dimension two. Under different types of independence assumptions, asymptotic distributions are derived for the general case of a (potentially nonstationary) Î²-null Harris recurrent noise term Îµ but also for the special case of Îµ being stationary mixing. The later case deserves special attention since the model might be regarded as an additive type of cointegration model. In contrast to existing more general approaches, the number of cointegrated regressors is not restricted. Finite sample properties are illustrated in a simulation study."
"507";507;"2011-077";"Increasing Weather Risk: Fact or Fiction?";"Weining Wang, Ihtiyor Bobojonov, Wolfgang Karl Härdle and  Martin Odening";"B1B10C11";"2011-11-10";"C00,  C14,  J01,  J31";" It is an undisputed fact that weather risk increases over time due to climate change. However, qualification of this statement with regard to the type of weather risk and geographical location is needed. We investigate the application of novel statistical tools for assessing changes in weather risk over time. We apply local t-test, change point tests and Mann-Kendall test as well as quantile regression to weather risk indicators that are relevant from the viewpoint of agricultural production. Our results show that weather risk follows different pattern depending on the type of risk and the location."
"508";508;"2011-078";"Spatially Adaptive Density Estimation by Localised Haar Projections";"Florian Gach, Richard Nickl and  Vladimir Spokoiny";"B5";"2011-11-10";"C14";" Given a random sample from some unknown density f0 : R â†’ [0;âˆž) we devise Haar wavelet estimators for f0 with variable resolution levels constructed from localised test procedures (as in Lepski, Mammen, and Spokoiny (1997, Ann. Statist.)). We show that these estimators adapt to spatially heterogeneous smoothness of f0, simultaneously for every point x in a fixed interval, in sup-norm loss. The thresholding constants involved in the test procedures can be chosen in practice under the idealised assumption that the true density is locally constant in a neighborhood of the point x of estimation, and an information theoretic justication of this practice is given."
"509";509;"2011-079";"Martingale approach in pricing and hedging European options under regime-switching";"Grigori N. Milstein and  Vladimir Spokoiny";"B5";"2011-11-10";"C58";" The paper focuses on the problem of pricing and hedging a European contingent claim for an incomplete market model, in which evolution of price processes for a saving account and stocks depends on an observable Markov chain. The pricing function is evaluated using the martingale approach. The equivalent martingale measure is introduced in a way that the Markov chain remains the historical one, and the pricing function satises the Cauchy problem for a system of linear parabolic equations. It is shown that any European contingent claim is attainable using a generalized self-financing replicating strategy. For such a strategy, apart from the initial endowment, some additional funds are required both step-wise at the jump moments of the Markov chain and continuously between the jump moments. It is proved that the additional funds (the additional investments and consumptions) are present in the proposed strategy in a risk-neutral manner, hence the generalized self- nancing strategy is self-nancing in mean. A payment for the considered option should consist of two parts: the initial endowment and a fair insurance premium in order to compensate for contributions and consumptions arising in future."
"510";510;"2011-080";"Sparse Non Gaussian Component Analysis by Semidefinite Programming";"Elmar Diederichs, Anatoli Juditsky, Arkadi Nemirovski and  Vladimir Spokoiny";"B5";"2011-11-10";"C14";" Sparse non-Gaussian component analysis (SNGCA) is an unsupervised method of extracting a linear structure from a high dimensional data based on estimating a low-dimensional non-Gaussian data component. In this paper we discuss a new approach to direct estimation of the projector on the target space based on semidefinite programming which improves the method sensitivity to a broad variety of deviations from normality. We also discuss the procedures which allows to recover the structure when its eective dimension is unknown."
"511";511;"2011-081";"Parametric estimation. Finite sample theory";"Vladimir Spokoiny";"B5";"2011-11-16";"C13,   C14";" The paper aims at reconsidering the famous Le Cam LAN theory. The main features of the approach which make it different from the classical one are: (1) the study is non-asymptotic, that is, the sample size is xed and does not tend to infinity; (2) the parametric assumption is possibly misspecified and the underlying data distribution can lie beyond the given parametric family. The main results include a large deviation bounds for the (quasi) maximum likelihood and the local quadratic majorization of the log-likelihood process. The latter yields a number of important corollaries for statistical inference: concentration, confidence and risk bounds, expansion of the maximum likelihood estimate, etc. All these corollaries are stated in a non-classical way admitting a model misspecification and finite samples. However, the classical asymptotic results including the efficiency bounds can be easily derived as corollaries of the obtained non-asymptotic statements. The general results are illustrated for the i.i.d. set-up as well as for generalized linear and median estimation. The results apply for any dimension of the parameter space and provide a quantitative lower bound on the sample size yielding the root-n accuracy. We also discuss the procedures which allows to recover the structure when its eective dimension is unknown."
"512";512;"2011-082";"Continuous Equilibrium under Base Preferences and Attainable Initial Endowments";"Ulrich Horst, Michael Kupper, Andrea Macrina and  Christoph Mainberger";"A11";"2011-11-16";"C62,  D52,  D53";" We consider a full equilibrium model in continuous time comprising a finite number of agents and tradable securities.We show that, if the agentsÂ’ endowments are spanned by the securities and if the agents have entropic utilities, an equilibrium exists and the agentsÂ’ optimal trading strategies are constant. Affine processes, and the theory of information-based asset pricing are used to model the endogenous asset price dynamics and the terminal payoff. Semi-explicit pricing formulae are obtained and applied to numerically analyze the impact of the agentsÂ’ risk aversion on the implied volatility of simultaneously-traded European-style options."
"513";513;"2011-083";"Equilibrium Pricing in Incomplete Markets under Translation Invariant Preferences";"Patrick Cheridito, Ulrich Horst and  Traian A. Pirvu";"A11";"2011-11-16";"C62,  D52,  D53";" We provide results on the existence and uniqueness of equilibrium in dynamically incomplete financial markets in discrete time. Our framework allows for heterogeneous agents, unspanned random endowments and convex trading constraints. In the special case where all agents have preferences of the same type and all random endowments are replicable by trading in the financial market we show that a one-fund theorem holds and give an explicit expression for the equilibrium pricing kernel. If the underlying noise is generated by finitely many Bernoulli random walks, the equilibrium dynamics can be described by a system of coupled backward stochastic difference equations, which in the continuous-time limit becomes a multi-dimensional backward stochastic differential equation. If the market is complete in equilibrium, the system of equations decouples, but if not, one needs to keep track of the prices and continuation values of all agents to solve it. As an example we simulate option prices in the presence of stochastic volatility, demand pressure and short-selling constraints."
"514";514;"2011-084";"Competition and regulation in a differentiated good market";"Raffaele Fiocco";"A8";"2011-11-30";"D82,  L11,  L51";" This paper addresses the issue of how to design the institutional structure of an industry which provides two differentiated products. One good is supplied by a regulated monopoly and the other is produced  in a competitive (unregulated) segment. Two possible institutional patterns  are compared. Under ""concentration"" the regulated firm can enter the  competitive segment by owning one firm which operates there (even  though the two firms must be legally unbundled). Theregime of ""separation""  implies that regulated activities are totally unbundled from the unregulated  ones, that is, common ownership is not allowed. When the regulator does not  know the regulated monopoly's cost of production, we find that the pattern  of separation improves (expected) social welfare as long as goods are  substitutes. Conversely, concentration performs better in case of complementarity."
"515";515;"2011-085";"Risk Patterns and Correlated Brain Activities. Multidimensional statistical analysis of fMRI data with application to risk patterns";"Alena Myšicková, Song Song, Piotr Majer, Peter N.C. Mohr, Hauke R. Heekeren and  Wolfgang K. Härdle";"A12B1";"2011-12-01";"D8,  C14,  C3";" Decision making usually involves uncertainty and risk. Understanding which parts of the human brain are activated during decisions under risk and which neural processes underly (risky) investment decisions are important goals in neuroeconomics. Here, we reanalyze functional magnetic resonance imaging (fMRI) data on 17 subjects which were exposed to an investment decision task from Mohr et al. (2010b). We obtain a time series of three-dimensional images of the blood-oxygen-level dependent (BOLD) fMRI signals. Our goal is to capture the dynamic behavior of specific brain regions of all subjects in this high-dimensional time series data, by a flexible factor approach resulting in a low dimensional representation. We apply a panel version of the dynamic semiparametric factor model (DSFM) presented in Park et al. (2009) and identify task-related activations in space and dynamics in time. Further, we classify the risk attitudes of all subjects based on the estimated lowdimensional time series. Our classification analysis successfully confirms the estimated risk attitudes derived directly from subjects' decision behavior."
"516";516;"2011-086";"Spectral estimation of covolatility from noisy observations using local weights";"Markus Bibinger and  Markus Reiß";"C12";"2011-12-07";"C14,  C32,  C58,  G10";" We propose localized spectral estimators for the quadratic covariation and the spot covolatility of diffusion processes which are observed discretely with additive observation noise. The eligibility of this approach to lead to an appropriate estimation for time-varying volatilities stems from an asymptotic equivalence of the underlying statistical model to a white noise model with correlation and volatility processes being constant over small intervals. The asymptotic equivalence of the continuous-time and the discrete-time experiments are proved by a construction with linear interpolation in one direction and local means for the other. The new estimator outperforms earlier nonparametric approaches in the considered model. We investigate its finite sample size characteristics in simulations and draw a comparison between the various proposed methods."
"517";517;"2011-087";"Solving DSGE Models with a Nonlinear Moving Average";"Hong Lan and  Alexander Meyer-Gohde";"C7";"2011-12-15";"C61,  C63,  E17";" We introduce a nonlinear infinite moving average as an alternative to the standard state-space policy function for solving nonlinear DSGE models. Perturbation of the nonlinear moving average policy function provides a direct mapping from a history of innovations to endogenous variables, decomposes the contributions from individual orders of uncertainty and nonlinearity, and enables familiar impulse response analysis in nonlinear settings. When the linear approximation is saddle stable and free of unit roots, higher order terms are likewise saddle stable and first order corrections for uncertainty are zero. We derive the third order approximation explicitly and examine the accuracy of the method using Euler equation tests."
"518";518;"2012-001";"HMM in dynamic HAC models";"Wolfgang Karl Härdle, Ostap Okhrin and  Weining Wang";"B1B10";"2012-01-04";"C13,  C14,  G50";" Understanding the dynamics of high dimensional non-normal dependency structure is a challenging task. This research aims at attacking this problem by building up a hidden Markov model (HMM) for Hierarchical Archimedean Copulae (HAC), where the HAC represent a wide class of models for high dimensional dependency, and HMM is a statistical technique to describe time varying dynamics. HMM applied to HAC provide flexible modeling for high dimensional non Gaussian time series. Consistency results for both parameters and HAC structures are established in an HMM framework. The model is calibrated to exchange rate data with a VaR application, where the modelÂ’s performance is compared with other dynamic models, and in the second application we simulate rainfall process."
"519";519;"2012-002";"Dynamic Activity Analysis Model Based Win-Win Development Forecasting Under the Environmental Regulation in China";"Shiyi Chen and  Wolfgang Karl Härdle";"B1";"2012-01-05";"D24,  O47,  Q25,  Q32";" Porter Hypothesis states that environmental regulation may lead to win-win opportunities, that is, improve the productivity and reduce the undesirable output simultaneously. Based on directional distance function, this paper proposes a novel dynamic activity analysis model to forecast the possibilities of win-win development in Chinese Industry between 2009 and 2049. The evidence reveals that the appropriate energy-saving and emission-abating regulation will result in both the improvement in net growth of potential output and the steadily increasing growth of total factor productivity. This favors Porter Hypothesis."
"520";520;"2012-003";"A Donsker Theorem for Lévy Measures";"Richard Nickl and  Markus Reiß";"C12";"2012-01-05";"C14,  C22";" Given n equidistant realisations of a LÃ©vy process (Lt; t >= 0), a natural estimator  for the distribution function N of the LÃ©vy measure is constructed. Under a polynomial decay restriction on the characteristic function, a Donsker-type theorem is proved, that is, a functional central limit theorem for the process in the space of bounded functions away from zero. The limit distribution is a generalised Brownian bridge process with bounded and continuous sample paths whose covariance structure depends on the Fourier-integral operator. The class of LÃ©vy processes covered includes several relevant examples such as compound Poisson, Gamma and self-decomposable processes. Main ideas in the proof include establishing pseudo-locality of the Fourier-integral operator and recent techniques from smoothed empirical processes."
"521";521;"2012-004";"Computational Statistics (Journal)";"Wolfgang Karl Härdle, Yuichi Mori and  Jürgen Symanzik";"B1";"2012-01-05";"C63,  C88,  Y30";" Computational Statistics is an international journal that fosters the publication of applications and methodological research in the field of computational statistics. In this article, we will discuss the motivation, history, some specialties, and the future scope of this journal."
"522";522;"2012-005";"Implementing quotas in university admissions: An experimental analysis";"Sebastian Braun, Nadja Dwenger, Dorothea Kübler and  Alexander Westkamp";"A6";"2012-01-11";"C78,  C92,  D78,  I20";" Quotas for special groups of students often apply in school or university admission procedures. This paper studies the performance of two mechanisms to implement such quotas in a lab experiment. The first mechanism is a simplified version of the mechanism currently employed by the German central clearinghouse for university admissions, which first allocates seats in the quota for top-grade students before allocating all other seats among remaining applicants. The second is a modied version of the student-proposing deferred acceptance (SDA) algorithm, which simultaneously allocates seats in all quotas. Our main result is that the current procedure, designed to give top-grade students an advantage, actually harms them, as students often fail to grasp the strategic issues involved. The modified SDA algorithm significantly improves the matching for top-grade students and could thus be a valuable tool for redesigning university admissions in Germany."
"523";523;"2012-006";"Quantile Regression in Risk Calibration";"Shih-Kang Chao, Wolfgang Karl Härdle and  Weining Wang";"B1B10";"2012-01-25";"C14,  C21,  C22,  C53,  G01,  ";" Financial risk control has always been challenging and becomes now an even harder problem as joint extreme events occur more frequently. For decision makers and government regulators, it is therefore important to obtain accurate information on the interdependency of risk factors. Given a stressful situation for one market participant, one likes to measure how this stress affects other factors. The CoVaR (Conditional VaR) framework has been developed for this purpose. The basic technical elements of CoVaR estimation are two levels of quantile regression: one on market risk factors; another on individual risk factor. Tests on the functional form of the two-level quantile regression reject the linearity. A flexible semiparametric modeling framework for CoVaR is proposed. A partial linear model (PLM) is analyzed. In applying the technology to stock data covering the crisis period, the PLM outperforms in the crisis time, with the justification of the backtesting procedures. Moreover, using the data on global stock markets indices, the analysis on marginal contribution of risk (MCR) defined as the local first order derivative of the quantile curve sheds some light on the source of the global market risk."
"524";524;"2012-007";"Total Work and Gender: Facts and Possible Explanations";"Michael Burda, Daniel S. Hamermesh and  Philippe Weil";"C7";"2012-02-02";"J22,  J16,  D13";" Time-diary data from 27 countries show a negative relationship between real GDP per capita and female-male differences in total work timeÂ—work for pay and work at home. In rich non-Catholic countries on four continents men and women do about the same average amount of total work. Survey results demonstrate, however, that labor economists, macroeconomists, sociologists and the general public believe that women work more. The widespread average equality does not arise from gender differences in the price of time, from intra-family bargaining or from spousal complementarity. Several theories, including ones based on social norms, might explain these findings and are consistent with cross-national evidence from the World Values Surveys and sets of microeconomic data from Australia and Germany."
"525";525;"2012-008";"Does Basel II Pillar 3 Risk Exposure Data help to Identify Risky Banks?";"Ralf Sabiwalsky";"Z";"2012-02-02";"G17,   G21";" Basel II Pillar 3 reports provide information about banks' exposure towards a number of risk factors, such as corporate credit risk and interest rate risk. Previous studies nd that the quality of such information is likely to be weak. We analyze the marginal contribution of pillar 3 exposure data to the quality of equity volatility forecasts for individual banks. Our method uses (local in time) measures of risk factor risk using a multivariate stochastic volatility model for ve risk factors, and uses measures of bank sensitivity with respect to these risk factors. We use two sets of sensitivity measures. One takes into account pillar 3 information, and the other one does not. Generally, we generate volatility forecasts as if no market prices of equity were available for the bank the forecast is made for. We do this for banks for which such data is, in fact, available so that we can conduct ex post - tests of the quality of volatility forecasts. We nd that (1) pillar 3 information allows for a better-than-random ranking of banks according to their risk, but (2) pillar 3 exposure data does not help reduce volatility forecast error magnitude."
"526";526;"2012-009";"Comparability Effects of Mandatory IFRS Adoption";"Stefano Cascino and  Joachim Gassen";"A7";"2012-02-08";"M41,  G14,  F42";" The mandatory adoption of IFRS by many countries worldwide fuels the expectation that financial accounting information might become more comparable across countries. This expectation is opposed to an alternative view that stresses the importance of incentives in shaping accounting information. We provide early evidence on this debate by investigating the effects of mandatory IFRS adoption on the comparability of financial accounting information around the world. Using two comparability proxies based on De Franco et al. [2011], our results suggest that the overall comparability effect of mandatory IFRS adoption is marginal at best. To investigate the reasons for this finding, we first hand-collect data on IFRS compliance for a sample of German and Italian firms and find that firm-, region-, and country-level incentives systematically shape accounting compliance. We then use the identified compliance incentives to explain the variance in the comparability effect of mandatory IFRS adoption and find it to vary systematically with firm-level incentives, suggesting that only firms with high compliance incentives experience substantial increases in comparability."
"527";527;"2012-010";"Fair Value Reclassifications of Financial Assets during the Financial Crisis";"Jannis Bischof, Ulf Brüggemann and  Holger Daske";"A7";"2012-02-08";"G14,  G21,  G28,  M41,  M48";" At the peak of the financial crisis in October 2008, the IASB amended IAS 39 to grant companies the option of abandoning fair value recognition for selected financial assets. Using a comprehensive global sample of publicly listed IFRS banks, we find that banks use the reclassification option to forgo the recognition of fair value losses and ultimately the regulatory costs of supervisory intervention. Analyses of stock market reactions suggest that a small subset of the most troubled banks benefit from such reclassifications. However, analyses of related footnote disclosures reveal that two-thirds of reclassifying banks do not fully comply with the accompanying IFRS 7 requirements. These banks experience a significant increase in bid-ask spreads in the long run."
"528";528;"2012-011";"Intended and unintended consequences of mandatory IFRS adoption: A review of extant evidence and suggestions for future research";"Ulf Brüggemann, Jörg-Markus Hitz and  Thorsten Sellhorn";"A7";"2012-02-08";"G38,  K12,  K22,  K34,  M41,  ";" This paper discusses empirical evidence on the economic consequences of mandatory adoption of International Financial Reporting Standards (IFRS) in the European Union (EU) and provides suggestions on how future research can add to our understanding of these effects. Based on the explicitly stated objectives of the EUâ€Ÿs so-called Â„IAS Regulationâ€Ÿ, we distinguish between intended and unintended consequences of mandatory IFRS adoption. Empirical research on the intended consequences generally fails to document an increase in the comparability or transparency of financial statements. In contrast, there is rich and almost unanimous evidence of positive effects on capital markets and at the macroeconomic level. We argue that certain research design issues are likely to contribute to this apparent mismatch in findings and we suggest areas for future research to address it. The literature investigating unintended consequences of mandatory IFRS adoption is still in its infancy. However, extant empirical evidence and insights from non-IFRS settings suggest that mandatory IFRS adoption has the potential to materially affect contractual outcomes. We conclude that both the intended and the unintended consequences deserve further scrutiny to assess the costs and benefits of mandatory IFRS adoption, which may help provide a basis for evaluating the effectiveness of the IAS Regulation. We provide specific guidance for future research in this field."
"529";529;"2012-012";"Confidence sets in nonparametric calibration of exponential Lévy models";"Jakob Söhl";"C12";"2012-02-08";"G13,  C14";" Confidence intervals and joint confidence sets are constructed for the nonparametric calibration of exponential LÃ©vy models based on prices of European options. This is done by showing joint asymptotic normality for the estimation of the volatility, the drift, the intensity and the LÃ©vy density at nitely many points in the spectral calibration method. Furthermore, the asymptotic normality result leads to a test on the value of the volatility in exponential LÃ©vy models."
"530";530;"2012-013";"The Polarization of Employment in German Local Labor Markets";"Charlotte Senftleben and  Hanna Wielandt";"A9";"2012-02-08";"J24,  J31,  J62,  O33,  R23";" This paper uses the task-based view of technological change to study employment and wage polarization at the level of local labor markets in Germany between 1979 and 2007. In order to directly relate technological change to subsequent employment trends, we exploit variation in the regional task structure which reflects a regionÂ’s potential of being affected by computerization. We build a measure of regional routine intensity to test whether there has been a reallocation from routine towards non-routine labor conditional on a regionÂ’s initial computerization potential. We find that routine intensive regions have witnessed a differential reallocation towards non-routine employment and an increase in low- and medium-skilled service occupations. Our results corroborate the predictions of the task-based framework and confirm previous evidence on employment polarization in Germany in the sense that employment growth deteriorates at the middle of the skill distribution relative to the lower and the upper tail of the distribution."
"531";531;"2012-014";"On the Dark Side of the Market: Identifying and Analyzing Hidden Order Placements";"Nikolaus Hautsch and  Ruihong Huang";"B8";"2012-02-09";"G14,  C24,  C25,  G17";" Trading under limited pre-trade transparency becomes increasingly popular on financial markets. We provide first evidence on tradersÂ’ use of (completely) hidden orders which might be placed even inside of the (displayed) bid-ask spread. Employing TotalView-ITCH data on order messages at NASDAQ, we propose a simple method to conduct statistical inference on the location of hidden depth and to test economic hypotheses. Analyzing a wide cross-section of stocks, we show that market conditions reflected by the (visible) bid-ask spread, (visible) depth, recent price movements and trading signals significantly affect the aggressiveness of Â’darkÂ’ liquidity supply and thus the Â’hidden spreadÂ’. Our evidence suggests that traders balance hidden order placements to (i) compete for the provision of (hidden) liquidity and (ii) protect themselves against adverse selection, front-running as well as Â’hidden order detection strategiesÂ’ used by high-frequency traders. Accordingly, our results show that hidden liquidity locations are predictable given the observable state of the market."
"532";532;"2012-015";"Existence and Uniqueness of Perturbation Solutions to DSGE Models";"Hong Lan and  Alexander Meyer-Gohde";"C7";"2012-02-15";"C61,  C63,  E17";" We prove that standard regularity and saddle stability assumptions for linear approximations are sufficient to guarantee the existence of a unique solution for all undetermined coefficients of nonlinear perturbations of arbitrary order to discrete time DSGE models. We derive the perturbation using a matrix calculus that preserves linear algebraic structures to arbitrary orders of derivatives, enabling the direct application of theorems from matrix analysis to prove our main result. As a consequence, we provide insight into several invertibility assumptions from linear solution methods, prove that the local solution is independent of terms first order in the perturbation parameter, and relax the assumptions needed for the local existence theorem of perturbation solutions."
"533";533;"2012-016";"Nonparametric adaptive estimation of linear functionals for low frequency observed Lévy processes";"Johanna Kappus";"C12";"2012-02-15";"C14";" For a LÃ©vy process X having finite variation on compact sets and finite first moments, Âµ( dx) = xv( dx) is a finite signed measure which completely describes the jump dynamics. We construct kernel estimators for linear functionals of Âµ and provide rates of convergence under regularity assumptions. Moreover, we consider adaptive estimation via model selection and propose a new strategy for the data driven choice of the smoothing parameter."
"534";534;"2012-017";"Option calibration of exponential Lévy models: Implementation and empirical results";"Jakob Söhl and  Mathias Trabs";"C12";"2012-02-29";"";" Observing prices of European put and call options, we calibrate exponential LÃ©vy models nonparametrically. We discuss the implementation of the spectral estimation procedures for LÃ©vy models of finite jump activity as well as for self-decomposable LÃ©vy models and improve these methods. Confidence intervals are constructed for the estimators in the finite activity case. They allow inference on the behavior of the parameters when the option prices are observed in a sequence of trading days. We compare the performance of the procedures for finite and infinite jump activity based on real option data."
"535";535;"2012-018";"Managerial Overconfidence and Corporate Risk Management";"Tim R. Adam, Chitru S. Fernando and  Evgenia Golubeva";"A13";"2012-02-21";"G11,  G14,  G32,  G39";" We show that managerial overconfidence, which has been found to influence a number of corporate financial decisions, also affects corporate risk management. We find that managers increase their speculative activities using derivatives following speculative gains, while they do not reduce their speculative activities following speculative losses. This asymmetric response follows from selective selfattribution: successes tend to be attributed to oneÂ’s own skill, while failures tend to be attributed to bad luck. Thus, our results show that managerial behavioral biases can also impact corporate risk management."
"536";536;"2012-019";"Why Do Firms Engage in Selective Hedging?";"Tim R. Adam, Chitru S. Fernando and  Jesus M. Salas";"A13";"2012-02-21";"G11,  G14,  G32,  G39";" Surveys of corporate risk management document that selective hedging, where managers incorporate their market views into firmsÂ’ hedging programs, is widespread in the U.S. and other countries. Stulz (1996) argues that selective hedging could enhance the value of firms that possess an information advantage relative to the market and have the financial strength to withstand the additional risk from market timing. We study the practice of selective hedging in a 10-year sample of North American gold mining firms and find that selective hedging is most prevalent among firms that are least likely to meet these valuemaximizing criteria -- (a) smaller firms, i.e., firms that are least likely to have private information about future gold prices; and (b) firms that are closest to financial distress. The latter finding provides support for the alternative possibility suggested by Stulz that selective hedging may also be driven by asset substitution motives. We detect weak relationships between selective hedging and some corporate governance measures, especially board size, but find no evidence of a link between selective hedging and managerial compensation."
"537";537;"2012-020";"A Slab in the Face: Building Quality and Neighborhood Effects";"Rainer Schulz and  Martin Wersing";"B3";"2012-02-23";"R31,   D62,   C31";" The quality of newly constructed single-family houses is usually homogeneous in and heterogeneous between neighborhoods. Such quality-clustering will be caused by the variation of natural amenities throughout a suburban area. Clustering will be enforced if the quality of neighboring buildings increases the value of newly constructed ones. To disentangle the natural amenity eect and the neighborhood eect, we use data from Berlin and exploit that the endogenous eect was weakened during the socialist period. Our results show that the exogenous variation caused by buildings constructed during this period still causes lower quality new buildings in the East of the city."
"538";538;"2012-021";"A Strategy Perspective on the Performance Relevance of the CFO";"Andreas Venus and  Andreas Engelen";"Z";"2012-02-29";"G30,  M10";" This paper is locked on request of the author. It will be publicy available again on the 01.10.2013."
"539";539;"2012-022";"Assessing the Anchoring of Inflation Expectations";"Till Strohsal and  Lars Winkelmann";"C14";"2012-02-29";"E52,  E58,  C32";" This paper proposes an ESTAR modeling framework to analyze the anchoring of inflation expectations. Anchoring criteria are empirical estimates of a market implied inflation target as well as the strength of the anchor that holds expectations at the target. Results from daily financial market expectations in the United States, European Monetary Union, United Kingdom and Sweden indicate: First, shorter-term expectations are better anchored than longer-term expectations. Second, expectations are best anchored in the EU, followed by US, Sweden and UK. Third, during the crisis market implied targets mostly decline while the strength of the anchor remains mostly unaffected."
"540";540;"2012-023";"Hidden Liquidity: Determinants and Impact";"Gökhan Cebiroglu";"A11";"2012-03-07";"G10,  G11,  G12,  G14,  G24";" We cross-sectionally analyze the presence of aggregated hidden depth and trade volume in the S&P 500 and identify its key determinants. We find that the spread is the main predictor for a stockÂ’s hidden dimension, both in terms of traded and posted liquidity. Our findings moreover suggest that large hidden orders are associated with larger transaction costs, higher price impact and increased volatility. In particular, as large hidden orders fail to attract (latent) liquidity to the market, hidden liquidity provision gives rise to negative liquidity externalities."
"541";541;"2012-024";"Bye Bye, G.I. - The Impact of the U.S. Military Drawdown on Local German Labor Markets";"Jan Peter aus dem Moore and   Alexandra Spitz-Oener";"A9";"2012-03-07";"J23,  R23";" What is the impact of a local negative demand shock on local labor markets? We exploit the unique natural experiment provided by the drawdown of U.S. military forces in West Germany after the end of the Cold War to investigate this question. We find persistent negative effects of the reduction in the U.S. forces on private sector employment, with con- siderable heterogeneity in terms of age and education groups, and sectors. In addition, the U.S. forces reduction resulted in a rise in local unemployment, whereas migration patterns and wages were not affected."
"542";542;"2012-025";"Is socially responsible investing just screening? Evidence from mutual funds";"Markus Hirschberger, Ralph E. Steuer, Sebastian Utz and  Maximilian Wimmer";"Z";"2012-03-07";"C61,   G11";" This paper presents the results of an empirical study concerning conventional and socially responsible mutual funds.We apply a sophisticated operations research algorithm embedded in inverse portfolio optimization on financial market data, ESG-scores and CRSP fund data. Due to our results we cannot find strong evidence of dierences between conventional and socially responsible mutual funds. In particular, the calculated risk tolerance parameters describing the real portfolio composition best show that socially responsible mutual funds may be even less concerned about the ESG-scores in the preference functional than conventional funds."
"543";543;"2012-026";"Explaining regional unemployment differences in Germany: a spatial panel data analysis";"Franziska Lottmann";"B8";"2012-03-16";"C23,  R12,  R23";" This paper analyzes determinants for regional differences in German unemployment rates. We specify a spatial panel model to avoid biased and inefficient estimates  due to spatial dependence. Additionally, we control for temporal dynamics in the data.  Our study covers the whole of Germany as well as East andWest Germany separately.  We exploit district-level data on 24 possible explanatory variables for the period from 1999  until 2007. Our results suggest that the spatial dynamic panel model is the best model for this analysis. Furthermore, we find that German regional unemployment is of disequilibrium  nature, which justifies political interventions."
"544";544;"2012-027";"Forecast based Pricing of Weather Derivatives";"Wolfgang Karl Härdle, Brenda López-Cabrera and  Matthias Ritter";"B1C11";"2012-03-16";"G19,  G29,  G22,  N23,  N53,  ";" Forecasting based pricing of Weather Derivatives (WDs) is a new approach in valuation of contingent claims on nontradable underlyings. Standard techniques are based on historical weather data. Forward-looking information such as meteorological forecasts or the implied market price of risk (MPR) are often not incorporated. We adopt a risk neutral approach (for each location) that allows the incorporation of meteorological forecasts in the framework of WD pricing. We study weather Risk Premiums (RPs) implied from either the information MPR gain or the meteorological forecasts. The size of RPs is interesting for investors and issuers of weather contracts to take advantages of geographic diversification, hedging effects and price determinations. By conducting an empirical analysis to London and Rome WD data traded at the Chicago Mercantile Exchange (CME), we find out that either incorporating the MPR or the forecast outperforms the standard pricing techniques."
"545";545;"2012-028";"Does umbrella branding really work? Investigating cross-category brand loyalty";"Nadja Silberhorn and  Lutz Hildebrandt";"B2";"2012-04-25";"M31,  C43";" Numerous studies on the drivers of brand extension success [Aaker and Keller, 1990, Broniarczyk and Alba, 1994, Hem et al., 2003, VÃ¶lckner and Sattler, 2006] found evidence that parent-brand characteristics and the fit between parent brand and transfer product are the main and most influential factors driving brand extension success. However, the ability of a brand to transfer its brand loyal customers from the parent to the extension category has been widely neglected. Brand loyalty can be regarded as a consequence of the underlying assumption of customers transferring their quality perceptions, their brand knowledge, and their experience with the brand from one category to the other [Erdem and Swait, 1998]. We find empirical evidence that consumers who are loyal to the brand in the leading (parent) product category show a higher probability to be loyal to that same brand in another (extension) category compared to those consumers who are not loyal in the leading category. Moreover, as the overall success of the extension includes positive retroactive effects of the extension product on the parent product or brand [Erdem, 1998], the arising question is whether there are differences between extension product categories regarding their attachment to the parent category and their ability to stimulate brand loyal purchases in the parent category, i.e., speaking of Â’leaderÂ’ and Â’followerÂ’ categories in terms of brand loyal purchase behavior. This might even hold true for the relationship of any two categories the brand competes."
"546";546;"2012-029";"Statistical Modelling of Temperature Risk";"Zografia Anastasiadou and  Brenda López-Cabrera";"C11";"2012-04-25";"G19,   G29,   G22,   N23,   N5";" Recently the topic of global warming has become very popular. The literature has concentrated its attention on the evidence of such eect, either by detecting regime shifts or change points in time series. The majority of these methods are designed to nd shifts in mean, but only few can do this for the variance. In this paper we attempt to investigate the statistical evidence of global warming by identi- fying shifts in seasonal mean of daily average temperatures over time and in seasonal variance of temperature residuals. We present a time series approach for modelling temperature dynamics. A seasonal mean Lasso-type technique based with a multi- plicative structure of Fourier and GARCH terms in volatility is proposed. The model describes well the stylised facts of temperature: seasonality, intertemporal correla- tions and the heteroscedastic behaviour of residuals. The application to European temperature data indicates that the multiplicative model for the seasonal variance performs better in terms of out of sample forecast than other models proposed in the literature for modelling temperature dynamics. We study the dynamics of the seasonal variance by implementing quantile and expectile functions with condence corridor to detrended and deseasonalized residuals. We show that shifts in seasonal mean and variance vary from location to location, indicating that all sources of trends other than mean and variance would rise trends over spatial scales. The local eects of temperature risk support the existence of global warming."
"547";547;"2012-030";"Support Vector Machines with Evolutionary Feature Selection for Default Prediction";"Wolfgang Karl Härdle, Dedy Dwi Prastyo and  Christian Hafner";"B1";"2012-04-25";"C14,  C45,  C61,  C63,  G33";" Predicting default probabilities is at the core of credit risk management and is becoming more and more important for banks in order to measure their client's degree of risk, and for rms to operate successfully. The SVM with evolutionary feature selection is applied to the CreditReform database. We use classical methods such as discriminan analysis (DA), logit and probit models as benchmark On overall, GA-SVM is outperforms compared to the benchmark models in both training and testing dataset."
"548";548;"2012-031";"Local Adaptive Multiplicative Error Models for High-Frequency Forecasts";"Wolfgang Karl Härdle, Nikolaus Hautsch and  Andrija Mihoci";"B1B8";"2012-04-27";"C41,  C51,  C53,  G12,  G17";" We propose a local adaptive multiplicative error model (MEM) accommodating timevarying parameters. MEM parameters are adaptively estimated based on a sequential testing procedure. A data-driven optimal length of local windows is selected, yielding adaptive forecasts at each point in time. Analyzing one-minute cumulative trading volumes of five large NASDAQ stocks in 2008, we show that local windows of approximately 3 to 4 hours are reasonable to capture parameter variations while balancing modelling bias and estimation (in)efficiency. In forecasting, the proposed adaptive approach significantly outperforms a MEM where local estimation windows are fixed on an ad hoc basis."
"549";549;"2012-032";"Copula Dynamics in CDOs";"Barbara Choros-Tomczyk, Wolfgang Karl Härdle and  Ludger Overbeck";"B1";"2012-05-08";"C13,  C22,  C53,  G32";" Values of tranche spreads of collateralized debt obligations (CDOs) are driven by the joint default performance of the assets in the collateral pool. The dependence between the names in the portfolio mainly depends on current economic conditions. Therefore, a correlation implied from tranches can be seen as a measure of the general health of the credit market. We analyse the European market of standardized CDOs using tranches of iTraxx index in the periods before and during the global financial crisis. We investigate the evolution of the correlations using different copula models: the standard Gaussian, the NIG, the double-t, and the Gumbel copula model. After calibration of these models one obtains a time varying vector of parameters. We analyse the dynamic pattern of these coefficients. That enables us to forecast future parameters and consequently calculate Value-at-Risk measures for iTraxx Europe tranches."
"550";550;"2012-033";"Simultaneous Statistical Inference in Dynamic Factor Models";"Thorsten Dickhaus";"A14";"2012-05-08";"C12,  C32,  C52";" Based on the theory of multiple statistical hypothesis testing, we elaborate simultaneous statistical inference methods in dynamic factor models. In particular, we employ structural properties of multivariate chi-squared distributions in order to construct critical regions for vectors of likelihood ratio statistics in such models. In this, we make use of the asymptotic distribution of the vector of test statistics for large sample sizes, assuming that the model is identified and model restrictions are testable. Examples of important multiple test problems in dynamic factor models demonstrate the relevance of the proposed methods for practical applications."
"551";551;"2012-034";"Realized Copula";"Matthias R. Fengler and  Ostap Okhrin";"B10";"2012-05-22";"G12,   C13,   C14,   C22,   C5";" We introduce the notion of realized copula. Based on assumptions of the marginal distri- butions of daily stock returns and a copula family, realized copula is dened as the copula structure materialized in realized covariance estimated from within-day high-frequency data. Copula parameters are estimated in a method-of-moments type of fashion through Hoeding's lemma. Applying this procedure day by day gives rise to a time series of copula parameters that is suitably approximated by an autoregressive time series model. This allows us to capture time-varying dependency in our framework. Studying a portfolio risk-management applica- tion, we find that time-varying realized copula is superior to standard benchmark models in the literature."
"552";552;"2012-035";"Correlated Trades and Herd Behavior in the Stock Market";"Simon Jurkatis, Stephanie Kremer and  Dieter Nautz";"C14";"2012-05-22";"G11,   G24,   C23";" Herd behavior is often viewed as a signicant threat for the stability and eciency of nancial markets. This paper sheds new light on the relevance of herd behavior for observed correlation of trades. We introduce numerical simulations of a herd model to derive theory-guided predictions regarding the impact of various aspects of uncertainty on herding intensity. We test the predictions using a novel data set including all real-time transactions of institutional investors in the German stock market. In light of the model simulations, empirical results strongly suggest that the observed correlation of trades is mainly due to the common reaction of investors to new public information and should not be misinterpreted as herd behavior."
"553";553;"2012-036";"Hierarchical Archimedean Copulae: The HAC Package";"Ostap Okhrin and  Alexander Ristig";"B10";"2012-05-22";"C51,   C87";" This paper aims at explanation of the R-package HAC, which provides user friendly methods for dealing with high-dimensional hierarchical Archimedean copulae (HAC). A computationally ecient estimation procedure allows to recover the structure and the parameters of HACs from data. In addition, arbitrary HACs can be constructed to sample random vectors and to compute the values of the corresponding cumulative distribution as well as density functions. Accurate graphics of the important characteristics of the package's object hac can be produced by the generic plot function."
"554";554;"2012-037";"Do Japanese Stock Prices Reflect Macro Fundamentals?";"Wenjuan Chen and  Anton Velinov";"C14";"2012-05-22";"G12,  E23";" This paper investigates to what extent the fundamentals of the real economy are re ected in the stock prices of Japan. A Markov switching VAR model with switching variances is used to test the structural identi cation scheme. Identification of fundamental and nonfundamental shocks is shown to be supported by the data. Based on the appropriate structural restriction, the historical stock prices are decomposed into fundamental components and nonfundamental components. The decomposition shows that the linkage between Japanese stock prices and real activity shocks became strengthened since the bubble collapsed in the beginning of 1990s."
"555";555;"2012-038";"The Aging Investor: Insights from Neuroeconomics";"Peter N. C. Mohr and  Hauke R. Heekeren";"A12";"2012-05-22";"D03,  D87,  G02,  G11";" Individuals in most industrialized countries have to make investment decisions throughout their adult life span to save for their retirement. These decisions substantially affect their living standards in old age. Research on cognitive aging has already demonstrated several changes in cognitive functions (e.g., processing speed) that likely influence investment decisions. This review brings together research on behavioral and neural aspects of financial decision making and aging to advance knowledge on age-related changes in financial decision making. The dopaminergic system plays a key role in financial decision making, both in financial decisions from description and financial decisions from experience. Importantly, both dopaminergic neuromodulation and financial decision making change during healthy aging. Especially when the parameters of the return distribution have to be learned from experience, older adults show a different and suboptimal choice behavior compared to younger adults. Based on these observations we suggest ways to circumvent the age-related bias in financial decision making to improve older adultsÂ’ wealth."
"556";556;"2012-039";"Volatility of price indices for heterogeneous goods";"Fabian Y.R.P. Bocart and  Christian M. Hafner";"Z";"2012-05-30";"C14,  C43,  Z11";" Price indices for heterogenous goods such as real estate or fine art constitute crucial information for institutional or private investors considering alternative investments in times of financial markets turmoil. Classical mean-variance analysis of alternative investments has been hampered by the lack of a systematic treatment of volatility in these markets. This may seem surprising as derivatives on subsets of the traded goods require a precise modelling and estimation of the underlying volatility. For example, in art markets, auction houses often give price guarantees to the seller that resemble put options. In this paper we propose a hedonic regression framework which explicitly defines an underlying stochastic process for the price index, allowing to treat the volatility parameter as the object of interest. The model can be estimated using maximum likelihood in combination with the Kalman filter. We derive theoretical properties of the volatility estimator and show that it outperforms the standard estimator. We show that extensions to allow for time-varying volatility are straightforward using a local-likelihood approach. In an application to a large data set of international blue chip artists, we show that volatility of the art market, although generally lower than that of financial markets, has risen over the last years and, in particular, during the recent European debt crisis."
"557";557;"2012-040";"Location, location, location: Extracting location value from house prices";"Jens Kolbe, Rainer Schulz, Martin Wersing and  Axel Werwatz";"B3";"2012-05-30";"R31,  C14";" The price for a single-family house depends both on the characteristics of the building and on its location. We propose a novel semiparametric method to extract location values from house prices. After splitting house prices into building and land components, location values are estimated with adaptive weight smoothing. The adaptive estimator requires neither strong smoothness assumptions nor local symmetry. We apply the method to house transactions from Berlin, Germany. The estimated surface of location values is highly correlated with expert-based land values and location ratings. The semiparametric method can therefore be used for applications where no other location value information exists or where this information is not reliable."
"558";558;"2012-041";"Multiple Point Hypotheses Testing problems and effective numbers of tests";"Thorsten Dickhaus and  Jens Stange";"A14";"2012-06-26";"C12,   C44";" We consider a special class of multiple testing problems, consisting of M simultaneous point hypothesis tests in local statistical experiments. Under certain structural assumptions the global hypothesis contains exactly one element # (say), and # is least favourable parameter configuration with respect to the family-wise error rate (FWER) of multiple single-step tests, meaning that the FWER of such tests becomes largest under #. Furthermore, it turns out that concepts of positive dependence are applicable to the involved test statistics in many practically relevant cases, in particular, for multivariate normal and chi-squared distributions. Altogether, this allows for a relaxation of the adjustment for multiplicity by making use of the intrinsic correlation structure in the data. We represent product-type bounds for the FWER in terms of a relaxed ?SidÂ´ak-type correction of the overall significance level and compute Â”effective numbers of testsÂ”. Our methodology can be applied to a variety of simultaneous location parameter problems, as in analysis of variance models or in the context of simultaneous categorical data analysis. For example, simultaneous chisquare tests for association of categorical features are ubiquitous in genomewide association studies. In this type of model, Moskvina and Schmidt (2008) gave a formula for an effective number of tests utilizing PearsonÂ’s haplotypic correlation coefficient as a linkage disequilibrium measure. Their result follows as a corollary from our general theory and will be generalized."
"559";559;"2012-042";"Generated Covariates in Nonparametric Estimation: A Short Review.";"Enno Mammen, Christoph Rothe and  Melanie Schienle";"B11";"2012-06-26";"C14,  C31";" In many applications, covariates are not observed but have to be estimated from data. We outline some regression-type models where such a situation occurs and discuss estimation of the regression function in this context.We review theoretical results on how asymptotic properties of nonparametric estimators differ in the presence of generated covariates from the standard case where all covariates are observed. These results also extend to settings where the focus of interest is on average functionals of the regression function."
"560";560;"2012-043";"The Signal of Volatility";"Till Strohsal and  Enzo Weber";"C14";"2012-06-26";"G15,  C32";" The present study addresses the economic interpretation of stock market volatility. We argue that its character is inherently ambivalent, being considered as an indicator of either information flow or uncertainty.We discriminate between these views by measuring the fraction of price changes that feeds into other markets depending on the prevailing level of volatility. This exploits the revealed reaction of investors to gauge the degree of information and uncertainty ascribed to volatility. We estimate simultaneous timevarying coefficient models, using data of US and further stock markets. We find the signal of volatility to depend crucially on the combination of its Â”senderÂ” and Â”receiverÂ”."
"561";561;"2012-044";"Copula-Based Dynamic Conditional Correlation Multiplicative Error Processes";"Taras Bodnar and  Nikolaus Hautsch";"B8";"2012-07-12";"C32,  C58,  C46";"  We introduce a copula-based dynamic model for multivariate processes of (non-negative) high-frequency trading variables revealing time-varying conditional variances and correlations. Modeling the variablesÂ’ conditional mean processes using a multiplicative error model we map the resulting residuals into a Gaussian domain using a Gaussian copula. Based on high-frequency volatility, cumulative trading volumes, trade counts and market depth of various stocks traded at the NYSE, we show that the proposed copula-based transformation is supported by the data and allows disentangling (multivariate) dynamics in higher order moments. To capture the latter, we propose a DCC-GARCH specification. We suggest estimating the model by composite maximum likelihood which is sufficiently flexible to be applicable in high dimensions. Strong empirical evidence for time-varying conditional (co-)variances in trading processes supports the usefulness of the approach. Taking these higher-order dynamics explicitly into account significantly improves the goodness-of-fit of the multiplicative error model and allows capturing time-varying liquidity risks."
"562";562;"2012-045";"Additive Models: Extensions and Related Models.";"Enno Mammen, Byeong U. Park and  Melanie Schienle";"B11";"2012-08-09";"C14,   C30";" We give an overview over smooth backtting type estimators in additive models. Moreover we il- lustrate their wide applicability in models closely related to additive models such as nonparametric regression with dependent error variables where the errors can be transformed to white noise by a linear transformation, nonparametric regression with repeatedly measured data, nonparametric panels with xed eects, simultaneous nonparametric equation models, and non- and semiparamet- ric autoregression and GARCH-models. We also discuss extensions to varying coecient models, additive models with missing observations, and the case of nonstationary covariates."
"563";563;"2012-046";"A uniform central limit theorem and efficiency for deconvolution estimators";"Jakob Söhl and  Mathias Trabs";"C12";"2012-08-09";"C14";" We estimate linear functionals in the classical deconvolution problem by kernel estimators. We obtain a uniform central limit theorem with square root n rate on the assumption that the smoothness of the functionals is larger than the ill-posedness of the problem, which is given by the polynomial decay rate of the characteristic function of the error. The limit distribution is a generalized Brownian bridge with a covariance structure that depends on the characteristic function of the error and on the functionals. The proposed estimators are optimal in the sense of semiparametric efficiency. The class of linear functionals is wide enough to incorporate the estimation of distribution functions. The proofs are based on smoothed empirical processes and mapping properties of the deconvolution operator."
"564";564;"2012-047";"Nonparametric Kernel Density Estimation Near the Boundary";"Peter Malec and  Melanie Schienle";"B11";"2012-08-17";"C14,  C51";" Standard fixed symmetric kernel type density estimators are known to encounter problems for positive random variables with a large probability mass close to zero. We show that in such settings, alternatives of asymmetric gamma kernel estimators are superior but also differ in asymptotic and finite sample performance conditional on the shape of the density near zero and the exact form of the chosen kernel. We therefore suggest a refined version of the gamma kernel with an additional tuning parameter according to the shape of the density close to the boundary. We also provide a data-driven method for the appropriate choice of the modified gamma kernel estimator. In an extensive simulation study we compare the performance of this refined estimator to standard gamma kernel estimates and standard boundary corrected and adjusted fixed kernels. We find that the finite sample performance of the proposed new estimator is superior in all settings. Two empirical applications based on high-frequency stock trading volumes and realized volatility forecasts demonstrate the usefulness of the proposed methodology in practice."
"565";565;"2012-048";"Yield Curve Modeling and Forecasting using Semiparametric Factor Dynamics";"Wolfgang Karl Härdle and  Piotr Majer";"B1";"2012-08-17";"C14,  C51";" Standard fixed symmetric kernel type density estimators are known to encounter problems for positive random variables with a large probability mass close to zero. We show that in such settings, alternatives of asymmetric gamma kernel estimators are superior but also differ in asymptotic and finite sample performance conditional on the shape of the density near zero and the exact form of the chosen kernel. We therefore suggest a refined version of the gamma kernel with an additional tuning parameter according to the shape of the density close to the boundary. We also provide a data-driven method for the appropriate choice of the modified gamma kernel estimator. In an extensive simulation study we compare the performance of this refined estimator to standard gamma kernel estimates and standard boundary corrected and adjusted fixed kernels. We find that the finite sample performance of the proposed new estimator is superior in all settings. Two empirical applications based on high-frequency stock trading volumes and realized volatility forecasts demonstrate the usefulness of the proposed methodology in practice."
"566";566;"2012-049";"Simultaneous test procedures in terms of p-value copulae";"Thorsten Dickhaus and  Jakob Gierl";"A14";"2012-08-21";"C12,  C44";" At least since [1], a broad class of multiple comparison procedures, so-called simultaneous test procedures (STPs), is established in the statistical literature. Elements of an STP are a testing family, consisting of a set of null hypotheses and corresponding test statistics, and a common critical constant. The latter threshold with which each of the test statistics has to be compared is calculated under the (joint) intersection hypothesis of all nulls. Under certain structural assumptions, the so-constructed STP provides strong control of the family-wise error rate. More recently, a general method to construct STPs in the case of asymptotic (joint) normality of the family of test statistics has been developed in [2], and numerical solutions to compute the critical constant in such cases were provided. Here, we propose to look at the problem from a different perspective. We will show that the threshold can equivalently be expressed by a quantile of the copula of the family of pvalues associated with the test statistics, assuming that each of these p-values is marginally uniformly distributed on the unit interval under the corresponding null hypothesis. This offers the opportunity to exploit the rich and growing literature on copula-based modeling of multivariate dependency structures for multiple testing problems and in particular for the construction of STPs in non-Gaussian situations."
"567";567;"2012-050";"Do Natural Resource Sectors Rely Less on External Finance than Manufacturing Sectors?";"Christian Hattendorff";"Z";"2012-08-21";"G20,  G30,  O13,  O14,  O16";" The nding that industrial sectors dier in their dependence on external nance for sector-specic technological reasons and, thus, rely to a dierent degree on nancial development has become a major concept in studies conducted on both growth and trade. Although natural resources might play an important role in each of these elds, research on industries' nancial dependence has been limited so far to manufacturing. By focusing on the natural resource sectors, the present paper aims to close this gap in its analysis. It rejects the common view that the natural resource industry in particular is less dependent on the nancial system, and nds that the results of the analysis depend on the specic measure being applied. Measures relating investment and cash  ow indicate high external dependence, while measures accounting for more short-term liquidity needs demonstrate rather low external dependence of natural resource rms. These results do not change considerably over time or across countries."
"568";568;"2012-051";"Using transfer entropy to measure information flows between financial markets";"Thomas Dimpfl and  Franziska J. Peter";"Z";"2012-08-23";"";" We use transfer entropy to quantify information flows between financial markets and propose a suitable bootstrap procedure for statistical inference. Transfer entropy is a model-free measure designed as the Kullback-Leibler distance of transition probabilities. Our approach allows to determine, measure and test for information transfer without being restricted to linear dynamics. In our empirical application, we examine the importance of the credit default swap market relative to the corporate bond market for the pricing of credit risk. We also analyze the dynamic relation between market risk and credit risk proxied by the VIX and the iTraxx Europe, respectively. We conduct the analyses for pre-crisis, crisis and post-crisis periods."
"569";569;"2012-052";"Rethinking stock market integration: Globalization, valuation and convergence";"Pui Sun Tam and  Pui I Tam";"Z";"2012-08-31";"F36,  G12,  G15";" This paper aims to study the extent of integration among developed and emerg- ing stock markets in the onset of globalization through the formulation of a uni?ed conceptual framework that synthesizes the stock valuation model and the convergence hypothesis. Market integration manifests in the convergence of stock valuation ratios of markets in the long run, where valuation ratios are reflective of stock fundamentals driven by common global factors across markets. The spectrum of transition dynamics of markets towards integration is explored with variants of valuation ratios and diÂ¤er- ent notions of convergence. Results reveal the time-varying nature of the global stock market integration process that is characterized by heterogeneous transition experience of markets at both the total market and disaggregated industrial sector levels."
"570";570;"2012-053";"Financial Network Systemic Risk Contributions";"Nikolaus Hautsch, Julia Schaumburg and  Melanie Schienle";"B8B11";"2012-08-31";"G01,  G18,  G32,  G38,  C21,  ";" We propose the realized systemic risk beta as a measure for financial companiesÂ’ contribution to systemic risk given network interdependence between firmsÂ’ tail risk exposures. Conditional on statistically pre-identified network spillover effects and market and balance sheet information, we define the realized systemic risk beta as the total time-varying marginal effect of a firmÂ’s Value-at-risk (VaR) on the systemÂ’s VaR. Suitable statistical inference reveals a multitude of relevant risk spillover channels and determines companiesÂ’ systemic importance in the U.S. financial system. Our approach can be used to monitor companiesÂ’ systemic importance allowing for a transparent macroprudential regulation."
"571";571;"2012-054";"Modeling Time-Varying Dependencies between Positive-Valued High-Frequency Time Series";"Nikolaus Hautsch, Ostap Okhrin and  Alexander Ristig";"B8B10";"2012-09-12";"C32,  C51";" Multiplicative error models (MEM) became a standard tool for modeling conditional durations of intraday transactions, realized volatilities and trading volumes. The parametric estimation of the corresponding multivariate model, the so-called vector MEM (VMEM), requires a specification of the joint error term distribution, which is due to the lack of multivariate distribution functions on Rd + defined via a copula. Maximum likelihood estimation is based on the assumption of constant copula parameters and therefore, leads to invalid inference, if the dependence exhibits time variations or structural breaks. Hence, we suggest to test for time-varying dependence by calibrating a time-varying copula model and to reestimate the VMEM based on identified intervals of homogenous dependence. This paper summarizes the important aspects of (V)MEM, its estimation and a sequential test for changes in the dependence structure. The techniques are applied in an empirical example."
"572";572;"2012-055";"Consumer Standards as a Strategic Device to Mitigate Ratchet Effects in Dynamic Regulation";"Raffaele Fiocco and  Roland Strausz";"A8";"2012-09-28";"D82,  L51";" Strategic delegation to an independent regulator with a pure consumer standard improves dynamic regulation by mitigating ratchet effects associated with short term contracting. A consumer standard alleviates the regulatorÂ’s myopic temptation to raise output after learning the firm is inefficient. Anticipating this tougher regulatory behavior, efficient firms find cost exaggeration less attractive. This reduces the need for long term rents and mitigates ratchet effects. The regulatorÂ’s welfare standard biased towards consumers comes, however, at the cost of some allocative distortion from the genuine social welfare perspective. Hence, a trade-off results which favors strategic delegation when efficient firms are relatively likely."
"573";573;"2012-056";"Strategic Delegation Improves Cartel Stability";"Martijn A. Han";"A8";"2012-10-11";"D43,  L13,  L20,  L41";" Fershtman and Judd (1987) and Sklivas (1987) show that strategic delegation reduces firm profits in the one-shot Cournot game. Allowing for infinitely repeated interaction, strategic delegation can increase firm profits as it improves cartel stability."
"574";574;"2012-057";"Short-Term Managerial Contracts and Cartels";"Martijn A. Han";"A8";"2012-10-11";"L13,  L22,  L41";" This paper shows how a series of commonly observed short-term CEO employment contracts can improve cartel stability compared to a long-term employment contract. When a managerÂ’s short-term appointment is renewed if and only if the firm hits a certain profit target, then (i) defection from collusion results in superior firm performance, thus reducing the chance of being fired, while (ii) future punishment results in inferior firm performance, thus increasing the chance of being fired in the future. The introduction of this re-employment tradeoff intertwines with the usual monetary tradeoff and can improve cartel stability. Studying the impact of fixed versus variable salary components, I find that fixed components can facilitate collusion with a short-term contract, while not affecting cartel stability with a long-term contract. Moreover, an extension of the model shows that short-term, renewable contracts can be a source of cyclical collusive pricing. Finally, interpreting the results in light of firm financing suggests that debt-financed firms can form more-stable cartels than equity-financed firms."
"575";575;"2012-058";"Private and Public Control of Management";"Charles Angelucci and  Martijn A. Han";"A8";"2012-10-12";"K21,  K42,  L40";" This paper investigates the design of a leniency policy to fight corporate crime. We explicitly take into account the agency problem within the firm. We model this through a three-tier hierarchy: authority, shareholder, and manager. The manager may breach the law and report evidence to the authority. The shareholder writes the managerÂ’s incentive scheme, monitors him, and possibly reports evidence to the authority. Finally, the authority designs a sanctioning/leniency policy that deters corporate crime at the lowest possible cost. The authority designs its policy trying to both (i) exacerbate agency problems within non-compliant firms and (ii) alleviate agency problems within compliant firms. We find that depending on the authorityÂ’s ability to punish the manager, the authority may wish to instigate a Â“within-firm race to the courthouseÂ”. We also provide comparative statics, carry a welfare analysis and discuss policy implications."
"576";576;"2012-059";"Cartelization Through Buyer Groups";"Chris Doyle and  Martijn A. Han";"A8";"2012-10-12";"K21,   L13,   L41,   L42";" Retailers may enjoy stable cartel rents in their output market through the formation of a buyer group in their input market. A buyer group allows retailers to credibly commit to increased input prices, which serve to reduce combined final output to the monopoly level; increased input costs are then refunded from suppliers to retailers through slotting allowances or rebates. The stability of such an Â“implied cartelÂ” depends on the retailersÂ’ incentives to secretly source from a supplier outside of the buyer group arrangement at lower input prices. Cheating is limited if retailers sign exclusive dealing or minimum purchase provisions. We discuss the relevancy of our findings for antitrust policy."
"577";577;"2012-060";"Modelling general dependence between commodity forward curves";"Mikhail Zolotko and  Ostap Okhrin";"B10";"2012-10-12";"C13,  C53,  Q40";" This study proposes a novel framework for the joint modelling of commodity forward curves. Its key contribution is twofold. First, dynamic correlation models are applied in this context as part of the modelling scheme. Second, we introduce a family of dynamic conditional correlation models based on hierarchical Archimedean copulae (HAC DCC), which are flexible, but parsimonious instruments that capture a wide range of dynamic dependencies. The conducted analysis allows us to obtain precise out-of-sample forecasts of the distribution of the returns of various commodity futures portfolios. The Value-at-Risk analysis shows that HAC DCC models outperform other introduced benchmark models on a consistent basis."
"578";578;"2012-061";"Variable selection in Cox regression models with varying coefficients";"Toshio Honda and  Wolfgang Karl Härdle";"B1";"2012-10-12";"C14,  C24";" We deal with two kinds of Cox regression models with varying coefficients. The coefficients vary with time in one model. In the other model, there is an important random variable called an index variable and the coefficients vary with the variable. In both models, we have p-dimensional covariates and p increases moderately. However, it is the case that only a small part of the covariates are relevant in these situations. We carry out variable selection and estimation of the coefficient functions by using the group SCAD-type estimator and the adaptive group Lasso estimator. We examine the theoretical properties of the estimators, especially the L2 convergence rate, the sparsity, and the oracle property. Simulation studies and a real data analysis show the performance of these new techniques."
"579";579;"2012-062";"Brand equity – how is it affected by critical incidents and what moderates the effect";"Sven Tischer and  Lutz Hildebrandt";"B2";"2012-10-22";"M14,  M31,  C12,  C14,  C38,  ";" To explore how occurring critical incidents affect customer-brand relations, this study measures the impact on the basis of an online experiment. For this purpose, 1,122 usable responses are gathered considering the smartphone brands of Apple and Nokia as well as different scenarios. The respective reactions to these negative incidents are evaluated using the concept of customer-based brand equity. More precisely, a structure equation model is specified and differences in latent factor means are estimated taking into account perceived quality, various brand associations, loyalty and overall brand equity. The findings indicate that brand equity dimensions are not equally affected. Moreover, the results demonstrate that both brand equity and the business relationship before crisis moderate the effect of distinct critical incidents."
"580";580;"2012-063";"Common factors in credit defaults swaps markets";"Yi-Hsuan Chen and  Wolfgang Karl Härdle";"B1";"2012-10-25";"C38;G32;E43";" We examine what are common factors that determine systematic credit risk and estimate and interpret the common risk factors. We also compare the contributions of common factors in explaining the changes of credit default swap (CDS) spreads during the pre-crisis, crisis and post-crisis period. Based on the testing result from the common principal components model, this study finds that the eigenstructures across the three subperiods are distinct and the determinants of risk factors differ from three subperiods. Furthermore, we analyze the predictive ability of dynamics in CDS indices changes by dynamic factor models."
"581";581;"2012-064";"Measuring the impact of critical incidents on brand personality";"Sven Tischer";"B2";"2012-10-29";"M14,  M31,  C12,  C14,  C38,  ";" To evaluate how occurring critical incidents change customer perceptions of brand personality, this study measures the impact on the basis of an online experiment. For this purpose, 1,132 usable responses are gathered considering the smartphone brands of Apple and Nokia as well as different critical incidents (corruption vs. product failure). Brand personality perceptions before and after these negative incidents are collected using the measurement model of Geuens, Weijters and De Wulf (2009). The measurement model is examined and the group specific factor scores are estimated. Based on these factor scores, latent means are calculated and hence reactions (personality shifts) are evaluated. The findings indicate that brand personality dimensions are not equally affected. Moreover, the results demonstrate that both brand equity and the business relationship before crisis moderate the effect of distinct critical incidents."
"582";582;"2012-065";"Covered bonds, core markets, and financial stability";"Kartik Anand, James Chapman and  Prasanna Gai";"C10";"2012-10-30";"G01,  G18,  G21";" We examine the financial stability implications of covered bonds. Banks issue covered bonds by encumbering assets on their balance sheet and placing them within a dynamic ring fence. As more assets are encumbered, jittery unsecured creditors may run, leading to a banking crisis. We provide conditions for such a crisis to occur. We examine how different over-the-counter market network structures influence the liquidity of secured funding markets and crisis dynamics. We draw on the framework to consider several policy measures aimed at mitigating systemic risk, including caps on asset encumbrance, global legal entity identifiers, and swaps of good for bad collateral by central banks."
"583";583;"2012-066";"Implied Basket Correlation Dynamics";"Wolfgang Karl Härdle and  Elena Silyakova";"B1";"2012-11-14";"C14,  C32,  G12,  G13,  G15,  ";" Equity basket correlation is an important risk factor. It characterizes the strength of linear dependence between assets and thus measures the degree of portfolio diversification. It can be estimated both under the physical measure from return series, and under the risk neutral measure from option prices. The difference between the two estimates motivates a so called ""dispersion strategy"". We study the performance of this strategy on the German market over the recent 2 years and propose several hedging schemes based on implied correlation (IC) forecasts. Modeling IC is a challenging task both in terms of computational burden and estimation error. First the number of correlation coefficients to be estimated would grow with the size of the basket. Second, since the IC is implied from option prices it is not constant over maturities and strikes. Finally, the IC changes over time. The dimensionality of the problem is reduced by an assumption that the correlation between all pairs of equities is constant (equicorrelation). The IC surface (ICS) is then approximated from implied volatilities of stocks and implied volatility of the basket. To analyze this structure and the dynamics of the ICS we employ a dynamic semiparametric factor model (DSFM)."
"584";584;"2012-067";"Can the market forecast the weather better than meteorologists?";"Matthias Ritter";"C11";"2012-12-10";"G15,   G17,   Q41,   Q47";" Many companies depend on weather conditions, so they require reliable weather forecasts for production planning or risk hedging. In this article, we propose a new way of gaining weather forecasts by exploiting the forward-looking information included in the market prices of weather derivatives traded at the Chicago Mercantile Exchange (CME). For this purpose, the CME futures prices of two monthly temperature indices relevant for the energy sector are compared with index forecasts derived from meteorological temperature forecasts. It turns out that the market prices generally outperform the meteorological forecasts in predicting the outcome of the monthly index. Hence, companies whose prot strongly depends on these indices, such as energy companies, can prot from this additional information source about future weather."
"585";585;"2013-001";"Functional Data Analysis of Generalized Quantile Regressions";"Mengmeng Guo, Lhan Zhou, Jianhua Z. Huang and  Wolfgang Karl Härdle";"B1";"2013-01-02";"C13,  C23,  C38,  Q54";" Generalized quantile regressions, including the conditional quantiles and expectiles as special cases, are useful alternatives to the conditional means for characterizing a conditional distribution, especially when the interest lies in the tails. We develop a functional data analysis approach to jointly estimate a family of generalized quantile regressions. Our approach assumes that the generalized quantile regressions share some common features that can be summarized by a small number of principal component functions. The principal component functions are modeled as splines and are estimated by minimizing a penalized asymmetric loss measure. An iterative least asymmetrically weighted squares algorithm is developed for computation. While separate estimation of individual generalized quantile regressions usually suffers from large variability due to lack of suffcient data, by borrowing strength across data sets, our joint estimation approach signifcantly improves the estimation effciency, which is demonstrated in a simulation study. The proposed method is applied to data from 150 weather stations in China to obtain the generalized quantile curves of the volatility of the temperature at these stations. These curves are needed to adjust temperature risk factors so that gaussianity is achieved. The normal distribution of temperature variations is vital for pricing weather derivatives with tools from mathematical finance."
"586";586;"2013-002";"Statistical properties and stability of ratings in a subset of US firms";"Alexander B. Matthies";"Z";"2013-01-04";"G20,  G24,  G30,  G32";" Standard explanatory variables that determine credit ratings do not achieve significant effects in a sample of 100 US non-financial firms in an ordered probit panel estimation. Sample size and selection as well as the distribution of explanatory variables across rating classes may be the cause this problem. Furthermore, we find evidence to suggest that variable coefficients vary over rating classes when analysed with an unordered loogit model. The sample reproduces well-established macroeconomic effects of credit ratings found by Blume et al. (1998) and highlights the influence of the rating agenciesÂ’ through-the-cycle approach on rating transitions."
"587";587;"2013-003";"Empirical Research on Corporate Credit-Ratings: A Literature Review";"Alexander B. Matthies";"Z";"2013-01-04";"G20,  G24,  G30,  G32,  G34";" We report on the current state and important older findings of empirical studies on corporate credit ratings and their relationship to ratings of other entities. Specifically, we consider the results of three lines of research: The correlation of credit ratings and corporate default, the influence of ratings on capital markets, and the determinants of credit ratings and rating changes. Results from each individual line are important and relevant for the construction and interpretation of studies in the other two fields, e.g. the choice of statistical methods. Moreover, design and construct of credit ratings and the credit rating scale are essential to understand empirical findings."
"588";588;"2013-004";"Preference for Randomization: Empirical and Experimental Evidence";"Nadja Dwenger, Dorothea Kübler and  Georg Weizsäcker";"A6";"2013-01-08";"D03,   D01";" We investigate violations of consequentialism in the form of the stochastic dominance property. The property is shared by many theories of choice and implies that the decision-maker prefers receiving the best outcome for sure over all lotteries that involve multiple outcomes. We run experiments to demonstrate that dominated randomization can be attractive. In treatments where decision-makers are asked to submit multiple decisions without knowing which one is relevant, many participants submit contradictory sets of decisions and thereby induce a dominated lottery between outcomes. Explicit choice of non-consequentialist randomization is observed in a separate treatment. A possible reason for the eect is the desire to avoid having to make the decision. A large data set on (high-stake) university applications in Germany shows patterns that are consistent with a preference for randomization."
"589";589;"2013-005";"Pricing Rainfall Derivatives at the CME";"Brenda López Cabrera, Martin Odening and  Matthias Ritter";"C11";"2013-01-10";"G19,  G29,  G22,  Q59";" Many business people such as farmers and financial investors are affected by indirect losses caused by scarce or abundant rainfall. Because of the high potential of insuring rainfall risk, the Chicago Mercantile Exchange (CME) began trading rainfall derivatives in 2011. Compared to temperature derivatives, however, pricing rainfall derivatives is more difficult. In this article, we propose to model rainfall indices via a flexible type of distribution, namely the normal-inverse Gaussian distribution, which captures asymmetries and heavy-tail behaviour. The prices of rainfall futures are computed by employing the Esscher transform, a wellknown tool in actuarial science. This approach is flexible enough to price any rainfall contract and to adjust theoretical prices to market prices by using the calibrated market price of risk. This empirical analysis is conducted with U.S. precipitation data and CME futures data providing first results on the market price of risk for rainfall derivatives."
"590";590;"2013-006";"Inference for Multi-Dimensional High-Frequency Data: Equivalence of Methods, Central Limit Theorems, and an Application to Conditional Independence Testing";"Markus Bibinger and  Per A. Mykland";"C12";"2013-01-15";"C14,  C32,  C58,  G10";" We find the asymptotic distribution of the multi-dimensional multi-scale and kernel estimators for high-frequency financial data with microstructure. Sampling times are allowed to be asynchronous. The central limit theorem is shown to have a feasible version. In the process, we show that the classes of multi-scale and kernel estimators for smoothing noise perturbation are asymptotically equivalent in the sense of having the same asymptotic distribution for corresponding kernel and weight functions. We also include the analysis for the Hayashi-Yoshida estimator in absence of microstructure. The theory leads to multi-dimensional stable central limit theorems for respective estimators and hence allows to draw statistical inference for a broad class of multivariate models and linear functions of the recorded components. This paves the way to tests and confidence intervals in risk measurement for arbitrary portfolios composed of high-frequently observed assets. As an application, we enhance the approach to cover more complex functions and in order to construct a test for investigating hypotheses that correlated assets are independent conditional on a common factor."
"591";591;"2013-007";"Crossing Network versus Dealer Market: Unique Equilibrium in the Allocation of Order Flow";"Jutta Dönges, Frank Heinemann and  Tijmen R. Daniëls";"C10";"2013-01-16";"C62,  G10,  G20";" The allocation of order  flow to alternative trading systems can be understood as a game with strategic substitutes between buyers on the same side of the market, as well as one of positive network externalities. We consider the allocation of order  flow between a crossing network and a dealer market and show that small differences in traders' preferences generate a unique switching equilibrium, in which patient traders use the crossing network while impatient traders submit orders directly to the dealer market. Our model explains why assets with large turnovers and low price volatility are likely to be traded on crossing networks, while less liquid assets are traded on dealer markets."
"592";592;"2013-008";"Forecasting systemic impact in financial networks";"Nikolaus Hautsch, Julia Schaumburg and  Melanie Schienle";"B8B11";"2013-01-30";"G01,  G18,  G32,  G38,  C21,  ";" We propose a methodology for forecasting the systemic impact of financial institutions in interconnected systems. Utilizing a five-year sample including the 2008/9 financial crisis, we demonstrate how the approach can be used for timely systemic risk monitoring of large European banks and insurance companies. We predict firmsÂ’ systemic relevance as the marginal impact of individual downside risks on systemic distress. The so-called systemic risk betas account for a companyÂ’s position within the network of financial interdependencies in addition to its balance sheet characteristics and its exposure towards general market conditions. Relying only on publicly available daily market data, we determine time-varying systemic risk networks, and forecast systemic relevance on a quarterly basis. Our empirical findings reveal time-varying risk channels and firmsÂ’ specific roles as risk transmitters and/or risk recipients."
"593";593;"2013-009";"‘I'll do it by myself as I knew it all along’: On the failure of hindsight-biased  principals to delegate optimally";"David Danz, Frank Hüber, Dorothea Kübler, Lydia Mechtenberg and  Julia Schmid";"A6";"2013-01-31";"C72,  C91,  D84";" With the help of a simple model, we show that the hindsight bias can lead to ineffcient delegation decisions. This prediction is tested experimentally. In an online experiment that was conducted during the FIFA World Cup 2010 participants were asked to predict a number of outcomes of the ongoing World Cup and had to recall their assessments after the outcomes had been realized. This served as a measure of the hindsight bias for each participant. The participants also had to make choices in a delegation game. Our data confirm that hindsight-biased subjects more frequently fail to delegate optimally than subjects whom we have classified as not hindsight biased."
"594";594;"2013-010";"Composite Quantile Regression for the Single-Index Model";"Yan Fan, Wolfgang Karl Härdle, Weining Wang and   Lixing Zhu";"B1";"2013-02-13";"C00,   C14,   C50,   C58";" Quantile regression is in the focus of many estimation techniques and is an important tool in data analysis. When it comes to nonparametric specifications of the conditional quantile (or more generally tail) curve one faces, as in mean regression, a dimensionality problem. We propose a projection based single index model specifi- cation. For very high dimensional regressors X one faces yet another dimensionality problem and needs to balance precision vs. dimension. Such a balance may be achieved by combining semiparametric ideas with variable selection techniques."
"595";595;"2013-011";"The Real Consequences of Financial Stress";"Stefan Mittnik and  Willi Semmler";"Z";"2013-02-27";"E2,   E6,   C13";" We introduce a dynamic bankingÂ–macro model, which abstains from conventional meanÂ– reversion assumptions and in whichÂ—similar to Brunnermeier and Sannikov (2010)Â—adverse assetÂ–price movements and their impact on risk premia and credit spreads can induce instabilities in the banking sector. To assess such phenomena empirically, we employ a multiÂ–regime vector autoregression (MRVAR) approach rather than conventional linear vector autoregressions. We conduct bivariate empirical analyses, using countryÂ–specific financialÂ–stress indices and industrial production, for the U.S., the UK and the four large euroÂ–area countries. Our MRVARÂ–based impulseÂ–response studies demonstrate that, compared to a linear specification, response profiles are dependent on the current state of the economy as well as the sign and size of shocks. Previous multiÂ–regimeÂ–based studies, focusing solely on the regimeÂ–dependence of responses, conclude that, during a highÂ–stress period, stressÂ–increasing shocks have more dramatic consequences for economic activity than during low stress. Conducting sizeÂ–dependent response analysis, we find that this holds only for small shocks and reverses when shocks become sufficiently large to induce immediate regime switches. Our findings also suggest that, in states of high financial stress, large negative shocks to financialÂ–stress have sizeable positive effects on real activity and support the idea of Â“unconventionalÂ” monetary policy measures in cases of extreme financial stress."
"596";596;"2013-012";"Are There Bubbles in the Sterling-dollar Exchange Rate? New Evidence from Sequential ADF Tests";"Timo Bettendorf and  Wenjuan Chen";"C14";"2013-02-28";"C1,   F3";" There has been mixed evidence regarding the existence of rational bubbles in the foreign exchange markets. This paper introduces recently developed sequential unit root tests into the analysis of exchange rates bubbles. We nd strong evidence of explosive behavior in the nominal Sterling-dollar exchange rate. However, this explosive behavior should not be simply interpreted as evidence of rational bubbles, as we show that it might be driven by the relative prices of traded goods."
"597";597;"2013-013";"A Transfer Mechanism for a Monetary Union";"Philipp Engler and  Simon Voigts";"C7";"2013-03-07";"F41,   F44,   E2,   E3,   E52";" We show in a dynamic stochastic general equilibrium framework that the introduction of a common currency by a group of countries with only partially integrated goods markets, incomplete ?nancial markets and no labor migration across member states, signi?cantly increases volatility of consumption and employment in the face of asymmetric shocks. We propose a simple transfer mechanism between member countries of the union that reduces this volatility. Further- more, we show that this mechanism is more eÂ¢ cient than anticyclical policies at the national level in terms of a better stabilization for the same budgetary eÂ¤ects for households while in the long run deeper integration of goods markets could reduce volatility signi?cantly. Re- garding its implementation, we show that the centralized provision of public goods and services at the level of the monetary union implies cross-country transfers comparable to the scheme under study."
"598";598;"2013-014";"Do High-Frequency Data Improve High-Dimensional Portfolio Allocations?";"Nikolaus Hautsch, Lada M. Kyj and  Peter Malec";"B8B11";"2013-03-08";"G11,   G17,   C58,   C14,   C3";" This paper addresses the open debate about the usefulness of high-frequency (HF) data in large-scale portfolio allocation. We consider the problem of constructing global minimum variance portfolios based on the constituents of the S&P 500 over a four-year period covering the 2008 financial crisis. HF-based covariance matrix predictions are obtained by applying a blocked realized kernel estimator, different smoothing windows, various regularization methods and two forecasting models. We show that HF-based predictions yield a significantly lower portfolio volatility than methods employing daily returns. Particularly during the volatile crisis period, these performance gains hold over longer horizons than previous studies have shown and translate into substantial utility gains from the perspective of  an investor with pronounced risk aversion."
"599";599;"2013-015";"Cyclical Variation in Labor Hours and Productivity Using the ATUS";"Michael C. Burda, Daniel S. Hamermesh and  Jay Stewart";"C7";"2013-03-08";"E23,   J22";" We examine monthly variation in weekly work hours using data for 2003-10 from the Current Population Survey (CPS) on hours/worker, from the Current Employment Survey (CES) on hours/job, and from the American Time Use Survey (ATUS) on both. The ATUS data minimize recall difficulties and constrain hours of work to accord with total available time. The ATUS hours/worker are less cyclical than the CPS series, but the hours/job are more cyclical than the CES series. We present alternative estimates of productivity based on ATUS data and find that it is more pro-cyclical than other productivity measures."
"600";600;"2013-016";"Quantitative forward guidance and the predictability of monetary policy  - A wavelet based jump detection approach -";"Lars Winkelmann";"C14";"2013-04-16";"E58,   C14,   C58";" The publication of a projected path of future policy decisions by central banks is a controversially debated method to improve monetary policy guidance. This paper suggests a new approach to evaluate the impact of the guidance strategy on the predictability of monetary policy. Using the example of Norway, the empirical investigation is based on jump probabilities of interest rates on central bank announcement days before and after the introduction of quantitative guidance. Within the standard semimartingale framework, we propose a new methodology to detect jumps. We derive a representation of the quadratic variation in terms of a wavelet spectrum. An adaptive threshold procedure on wavelet spectrum estimates aims at localizing jumps. Our main empirical result indicates that quantitative guidance significantly improves the predictability of monetary policy."
"601";601;"2013-017";"Estimating the Quadratic Covariation Matrix from Noisy Observations: Local Method of Moments and Efficiency";"Markus Bibinger, Nikolaus Hautsch, Peter Malec and  Markus Reiß";"B8C12";"2013-04-25";"C14,   C32,   C58,   G10";" An efficient estimator is constructed for the quadratic covariation or integrated covolatility matrix of a multivariate continuous martingale based on noisy and non-synchronous observations under high-frequency asymptotics. Our approach relies on an asymptotically equivalent  continuous-time observation model where a local generalised method of moments in the spectral domain turns out to be optimal. Asymptotic semiparametric efficiency is established in the CramÃ©r-Rao sense. Main findings are that non-synchronicity of observation times has no impact on the asymptotics and that major efficiency gains are possible under correlation. Simulations illustrate the finite-sample behaviour."
"602";602;"2013-018";"Fair re-valuation of wine as an investment";"Fabian Y.R.P. Bocart and  Christian M. Hafner";"Z";"2013-04-25";"C14,   C43,   M40,   G12";" The prices of wine is a key topic for market participants interested in valuing their stock, including dealers, restaurants or consumers who may be interested in optimizing their purchases. As a closely related issue, re-valuation is the need to regularly update the value of a stock. This need is especially met by fund managers in the growing industry of wine as an investment. In this case, fair-value measurement is compulsory by law. We briefly review methods available to funds and introduce a new quantitative method aimed at meeting IFRS 13 compliance for fair valuation. Using auction data, we apply our method to compute current fair value of a basket of wines."
"603";603;"2013-019";"The European Debt Crisis: How did we get into this mess? How can we get out of it?";"Michael C. Burda";"C7";"2013-04-25";"F33,   F34,   E42";" By any measure, the European Monetary Union and the European Union are in a deep hole. In the summer of 2011 we came uncomfortably close to an uncontrolled sovereign default of an EU country, a member of the European Monetary Union, hardly ten years after the common currency project was launched. In the months that followed, Greece was brought back from the precipice, but by the time of this writing has accumulated sovereign indebtedness of more than Â€380b or more than 170% of the countryÂ’s gross domestic product. By current estimates, more than half of this debt is held by foreigners, and mostly by foreign official institutions. How could a country with less than 2% of EU output be the source of such great concern? Quite simply, because in the meantime Ireland, Portugal, Spain and Italy (which along with Greece, are known as the GIIPs countries, or the PIIGS in less politically correct circles) have all spent significant time at the financial edge, with borrowing costs rising enough to threaten the integrity of the Eurozone banking system, the mechanism of payments, the European Central Bank and the common currency itself. In my view, we are still not out of the hole, even though most recent events may belie that assessment."
"604";604;"2013-020";"Disaster Risk in a New Keynesian Model";"Maren Brede";"C7";"2013-04-25";"E21,   E31,   E32";" This paper develops a simple New Keynesian model incorporating a small time-varying probability that the economy is struck by a disaster in the future. The model's main prediction is that a small increase in the disaster probability causes a recession in the economy, specically due to limited saving opportunities inasmuch as the model abstracts from capital accumulation. By contrasting its ndings to the ones of a comparable real business cycle model, this paper evaluates how the disaster hypothesis has been used and modelled in the existing literature."
"605";605;"2013-021";"Econometrics of co-jumps in high-frequency data with noise";"Markus Bibinger and  Lars Winkelmann";"C12C14";"2013-05-02";"C14,   G32,   E58";" We establish estimation methods to determine co-jumps in multivariate high-frequency data with nonsynchronous observations and market microstructure noise. The ex-post quadratic covariation of the signal part, which is modeled by an ItÂˆo-semimartingale, is estimated with a locally adaptive spectral approach. Locally adaptive thresholding allows to disentangle the co-jump and continuous part in quadratic covariation. Our estimation procedure implicitly renders spot (co-)variance estimators. We derive a feasible stable limit theorem for a truncated spectral estimator of integrated covariance. A test for common jumps is obtained with a wild bootstrap strategy. We give an explicit guideline how to implement the method and test the algorithm in Monte Carlo simulations. An empirical application to intra-day tick-data demonstrates the practical value of the approach."
"606";606;"2013-022";"Decomposing Risk in Dynamic Stochastic General Equilibrium";"Hong Lan and  Alexander Meyer-Gohde";"C7";"2013-05-02";"C63,   E32,   G12";" We analyze the theoretical moments of a nonlinear approximation to a model of business cycles and asset pricing with stochastic volatility and recursive preferences. We find that heteroskedastic volatility operationalizes a time-varying risk adjustment channel that induces variability in conditional asset pricing measures and assigns a substantial portion of the variance of macroeconomic variables to variations in precautionary behavior, both while leaving its ability to match key macroeconomic and asset pricing facts untouched. Our method decomposes moments into contributions from realized shocks and differing orders of approximation and from shifts in the distribution of future shocks, enabling us to identify the common channel through which stochastic volatility in isolation operates and through which conditional asset pricing measures vary."
"607";607;"2013-023";"Reference Dependent Preferences and the EPK Puzzle";"Maria Grith, Wolfgang Karl Härdle and  Volker Krätschmer";"B1";"2013-05-06";"D04,   D53,   C02,   G13";" Supported by several recent investigations, the empirical pricing kernel (EPK) puzzle might be considered a stylized fact. Based on an economic model with state dependent preferences for the financial investors, we want to emphasize a microeconomic view that succeeds in explaining the puzzle. We retain the expected utility framework in a one period model and illustrate the case when the state is defined with respect to a reference point. We further investigate how the model relates the shape of the EPK to the economic conditions."
"608";608;"2013-024";"Pruning in Perturbation DSGE Models - Guidance from Nonlinear Moving Average Approximations";"Hong Lan and  Alexander Meyer-Gohde";"C7";"2013-05-07";"C52,   C63,   E30";" We derive recursive representations of nonlinear moving average (NLMA) perturbations of DSGE models. As the stability of higher order NLMA representations follows directly from stability at first order, these recursive representations provide rigorous support for the practice of pruning that is becoming widespread. Our recursive representation differs from pruned perturbations in that it centers the approximation and its coefficients at the approximation of the stochastic steady state consistent with the order of approximation. We compare our algorithm with six different pruning algorithms at second and third order, documenting the differences between these six algorithms and standard (non pruned) state space perturbations at first, second, and third order in a unified notation compatible with the popular software package Dynare. While our third order algorithm is the most accurate, the gains over two alternate algorithms are modest, suggesting that this choice is unlikely to be a potential source of error."
"609";609;"2013-025";"The ‘Celtic Crisis’: Guarantees, transparency, and systemic liquidity risk";"Philipp König, Kartik Anand and  Frank Heinemann";"C10";"2013-05-16";"G01,   G28,   D89";" Bank liability guarantee schemes have traditionally been viewed as costless measures to shore up investor confidence and stave off bank runs. However, as the experiences of some European countries, most notably Ireland, have demonstrated, the credibility and effectiveness of these guarantees is crucially intertwined with the sovereignÂ’s funding risks. Employing methods from the literature on global games, we develop a simple model to explore the systemic linkage between the rollover risks of a bank and a government, which are connected through the governmentÂ’s guarantee of bank liabilities. We show the existence and uniqueness of the joint equilibrium and derive its comparative static properties. In solving for the optimal guarantee numerically, we show how its credibility may be improved through policies that promote balance sheet transparency. We explain the asymmetry in risk-transfer between sovereign and banking sector, following the introduction of a guarantee as being attributed to the resolution of strategic uncertainties held by bank depositors and the opacity of the banksÂ’ balance sheets."
"610";610;"2013-026";"State Price Densities implied from weather derivatives";"Wolfgang Karl Härdle, Brenda López-Cabrera and  Huei-Wen Teng";"B1C11";"2013-05-16";"C11,   C22,   C58,   G12,   G1";" A State Price Density (SPD) is the density function of a risk neutral equivalent martingale measure for option pricing, and is indispensible for exotic option pricing and portfolio risk management. Many approaches have been proposed in the last two decades to calibrate a SPD using financial options from the bond and equity markets. Among these, non and semi parametric methods were preferred because they can avoid model mis-specification of the underlying and thus give insight into complex portfolio propelling. However, these methods usually require a large data set to achieve desired convergence properties. Despite recent innovations in finan- cial and insurance markets, many markets remain incomplete and there exists an illiquidity issue. One faces the problem in estimation by e.g. kernel techniques that there are not enough observations locally available. For this situation, we employ a Bayesian quadrature method because it allows us to incorporate prior assumptions on the model parameters and hence avoids problems with data sparsity. It is able to compute the SPD of both call and put options simultaneously, and is particularly robust when the market faces the illiquidity issue. By comparing our approach with other approaches, we show that the traditional way of estimating the SPD by differ- entiating an interpolation of option prices does not hold in practice. As illustration, we calibrate the SPD for weather derivatives, a classical example of incomplete mar- kets with financial contracts payoffs linked to non-tradable assets, namely, weather indices. Finally, we study the dynamics of the implied SPD's and related to weather data."
"611";611;"2013-027";"Bank Lending Relationships and the Use of Performance-Sensitive Debt";"Tim R. Adam and  Daniel Streitz";"A13";"2013-05-21";"G21,   G31,   G32";" We show that performance-sensitive debt (PSD) is used to reduce hold-up problems in repeated lending relationships. Using a large sample of bank loans, we find a more frequent use of PSD if hold-up is more likely, e.g. if a longterm lending relationship exists and the borrower has fewer outside financing alternatives. The use of PSD is less likely in syndicated relationship loans, as hold-up is less important in this market. Finally, we find a substitution effect between the use of PSD and the tightness of financial covenants, which is consistent with PSD reducing hold-up problems caused by the use of covenants."
"612";612;"2013-028";"Analysis of Deviance in Generalized Partial Linear Models";"Wolfgang Karl Härdle and  Li-Shan Huang";"B1";"2013-05-28";"C00,   C14,   C50,   C58";" We develop analysis of deviance tools for generalized partial linear models based on local polynomial fitting. Assuming a canonical link, we propose expressions for both local and global analysis of deviance, which admit an additivity property that reduces to ANOVA decompositions in the Gaussian case. Chi-square tests based on integrated likelihood functions are proposed to formally test whether the nonparametric term is significant. Simulation results are shown to illustrate the proposed chi-square tests. The methodology is applied to German Bundesbank Federal Reserve data."
"613";613;"2013-029";"Estimating the quadratic covariation of an asynchronously observed semimartingale with jumps";"Markus Bibinger and  Mathias Vetter";"C12";"2013-05-28";"G10,   C14";" We consider estimation of the quadratic (co)variation of a semimartingale from discrete observations which are irregularly spaced under high-frequency asymptotics. In the univariate setting, results from Jacod (2008) are generalized to the case of irregular observations. In the two-dimensional setup under non-synchronous observations, we derive a stable central limit theorem for the estimator by Hayashi and Yoshida (2005) in the presence of jumps. We reveal how idiosyncratic and simultaneous jumps affect the asymptotic distribution. Observation times generated by Poisson processes are explicitly discussed."
"614";614;"2013-030";"Can expert knowledge compensate for data scarcity in crop insurance pricing?";"Zhiwei Shen, Martin Odening and  Ostap Okhrin";"B10C11";"2013-05-31";"C14,   Q19";" Although there is an increasing interest in index-based insurances in many developing countries, crop data scarcity hinders its implementation by forcing insurers to charge higher premiums. Expert knowledge has been considered a valuable information source to augment limited data in insurance pricing. This article investigates whether the use of expert knowledge can mitigate model risk which arises from insufficient statistical data. We adopt the Bayesian framework that allows for the combination of scarce data and expert knowledge, to estimate the risk parameter and buffer load. In addition, a benchmark for the evaluation of expert information is created by using a richer dataset generated from resampling. We find that expert knowledge reduces the parameter uncertainty and changes the insurance premium in the correct direction, but that the effect of the correction is sensitive to different strike levels of insurance indemnity."
"615";615;"2013-031";"Comparison of Methods for Constructing Joint Confidence Bands for Impulse Response Functions";"Helmut Lütkepohl, Anna Staszewska-Bystrova and  Peter Winker";"C15";"2013-05-31";"C32";" In vector autoregressive analysis confidence intervals for individual impulse responses are typically reported to indicate the sampling uncertainty in the estimation results. A range of methods are reviewed and a new proposal is made for constructing joint confidence bands, given a prespecifed coverage level, for the impulse responses at all horizons considered simultaneously. The methods are compared in a simulation experiment and recommendations for empirical work are provided."
"616";616;"2013-032";"CDO Surfaces Dynamics";"Barbara Choros-Tomczyk, Wolfgang Karl Härdle and  Ostap Okhrin";"B1B10";"2013-07-09";"C14,   C51,   G11,   G17";" Modelling the dynamics of credit derivatives is a challenging task in finance and economics. The recent crisis has shown that the standard market models fail to measure and forecast financial risks and their characteristics. This work studies risk of collateralized debt obligations (CDOs) by investigating the evolution of tranche spread surfaces and base correlation surfaces using a dynamic semiparametric factor model (DSFM). The DSFM offers a combination of flexible functional data analysis and dimension reduction methods, where the change in time is linear but the shape is nonparametric. The study provides an empirical analysis based on iTraxx Europe tranches and proposes an application to curve trading strategies. The DSFM allows us to describe the dynamics of all the tranches for all available maturities and series simultaneously which yields better understanding of the risk associated with trading CDOs and other structured products."
"617";617;"2013-033";"Estimation and Inference for Varying-coeffcient Models with Nonstationary Regressors using Penalized Splines";"Haiqiang Chen, Ying Fang and  Yingxing Li";"B1";"2013-07-17";"C12,   C14,   C22";" This paper considers estimation and inference for varying-coefficient models with nonstationary regressors. We propose a nonparametric estimation method using penalized splines, which achieves the same optimal convergence rate as kernel-based methods, but enjoys computation advantages. Utilizing the mixed model representation of penalized splines, we develop a likelihood ratio test statistic for checking the stability of the regression coefficients. We derive both the exact and the asymptotic null distributions of this test statistic. We also demonstrate its optimality by examining its local power performance. These theoretical fundings are well supported by simulation studies."
"618";618;"2013-034";"Robust Estimation and Inference for Threshold Models with Integrated Regressors";"Haiqiang Chen";"B1";"2013-07-17";"C12,   C22,   C52";" This paper studies the robust estimation and inference of threshold models with integrated regres- sors. We derive the asymptotic distribution of the profiled least squares (LS) estimator under the diminishing threshold effect assumption that the size of the threshold effect converges to zero. Depending on how rapidly this sequence converges, the model may be identified or only weakly identified and asymptotic theorems are developed for both cases. As the convergence rate is unknown in practice, a model-selection procedure is applied to determine the model identification strength and to construct robust confidence intervals, which have the correct asymptotic size irrespective of the magnitude of the threshold effect. The model is then generalized to incorporate endogeneity and serial correlation in error terms, under which, we design a Cochrane-Orcutt feasible generalized least squares (FGLS) estimator which enjoys efficiency gains and robustness against different error specifications, including both I(0) and I(1) errors. Based on this FGLS estimator, we further develop a sup-Wald statistic to test for the existence of the threshold effect. Monte Carlo simulations show that our estimators and test statistics perform well."
"619";619;"2013-035";"A new perspective on the economic valuation of informal care: The well-being approach revisited";"Konstantin Kehl and  Stephan Stahlschmidt";"B1";"2013-08-07";"D61,   I11,   I31";" Informal care has drawn much attention among scholars and policymakers as it concerns an essential but hard to evaluate resource of welfare. Albeit several studies addressed the monetary value of informal care, dierences in the relationship between caregivers and recipients have often been ignored. We report on a profound and formerly unobserved distinction between care in the household and non-household care for a family member or in a voluntary framework. According to our results caregivers within the household perceive care as a burden and a positive shadow price arises. By contrast in the family but non-household context { and especially in the voluntary case { care is (at least partly) understood as an enriching experience which extends well-being and leads to negative shadow prices. This distinction calls a marketized view of informal care into question and may contribute to explaining the limitations of monetary incentive policies to encourage informal care."
"620";620;"2013-036";"Herding in financial markets: Bridging the gap between theory and evidence";"Christopher Boortz, Simon Jurkatis, Stephanie Kremer and  Dieter Nautz";"C14";"2013-08-07";"G11,   G24";" Due to data limitations and the absence of testable, model-based predictions, theory and evidence on herd behavior are only loosely connected. This paper attempts to close this gap in the herding literature. From a theoretical perspective, we use numerical simulations of a herd model to derive new, theory-based predictions for aggregate herding intensity. From an empirical perspective, we employ high-frequency, investor-specific trading data to test the theory-implied impact of information risk and market stress on herding. Confirming model predictions, our results show that herding intensity increases with information risk. In contrast, herding measures estimated for the financial crisis period cannot be explained by the herd model. This suggests that the correlation of trades observed during the crisis is mainly due to the common reaction of investors to new public information and should not be misinterpreted as herd behavior."
"621";621;"2013-037";"Default Risk Calculation based on Predictor Selection for the Southeast Asian Industry";"Wolfgang Karl Härdle and  Dedy Dwi Prastyo";"B1";"2013-08-13";"C13,   C61,   G33";" Probability of default prediction is one of the important tasks of rating agencies as well as of banks and other financial companies to measure the default risk of their counterparties. Knowing predictors that significantly contribute to default prediction provides a better insight into fundamentals of credit risk analysis. Default prediction and default predictor selection are two related issues, but many existing approaches address them separately. We employed a unified procedure, a regularization approach with logit as an underlying model, which simultaneously selects the default predictors and optimizes all the parameters within the model. We employ Lasso and elastic-net penalty functions as regularization approach. The methods are applied to predict default of companies from industry sector in Southeast Asian countries. The empirical result exhibits that the proposed method has a very high accuracy prediction particularly for companies operating Indonesia, Singapore, and Thailand. The relevant default predictors over the countries reveal that credit risk analysis is sample specific. A few number of predictors result in counter intuitive sign estimates."
"622";622;"2013-038";"ECB monetary policy surprises: identification through cojumps in interest rates";"Lars Winkelmann, Markus Bibinger and  Tobias Linzert";"C12C14";"2013-08-29";"E58,   C14,   C58";" This paper proposes a new econometric approach to disentangle two distinct response patterns of the yield curve to monetary policy announcements. Based on cojumps in intraday tick-data of a short and long term interest rate, we develop a day-wise test that detects the occurrence of a significant policy surprise and identifies the market perceived source of the surprise. The new test is applied to 133 policy announcements of the European Central Bank (ECB) in the period from 2001-2012. Our main findings indicate a good predictability of ECB policy decisions and remarkably stable perceptions about the ECBÂ’s policy preferences."
"623";623;"2013-039";"Limited higher order beliefs and the welfare effects of public information";"Camille Cornand and   Frank Heinemann";"C10";"2013-08-29";"D82,   D83,   E52,   E58";" In games with strategic complementarities, public information about the state of the world has a larger impact on equilibrium actions than private information of the same precision, because the former is more informative about the likely behavior of others. This may lead to welfare-reducing Â‘overreactionsÂ’ to public signals as shown by Morris and Shin (2002). Recent experiments on games with strategic complementarities show that subjects attach a lower weight to public signals than theoretically predicted. Aggregate behavior can be better explained by a cognitive hierarchy model where subjects employ limited levels of reasoning. This paper analyzes the welfare effects of public information under limited levels of reasoning and argues that for strategies according with experimental evidence, public information that is more precise than private information cannot reduce welfare, unless the policy maker has instruments that are perfect substitutes to private actions."
"624";624;"2013-040";"Privacy Concerns, Voluntary Disclosure of Information, and Unraveling: An Experiment";"Volker Benndorf, Dorothea Kübler and  Hans-Theo Normann";"A6";"2013-09-05";"C72,   C90,   C91";" We study the voluntary revelation of private, personal information in a labor-market experiment with a lemons structure where workers can reveal their productivity at a cost. While rational revelation improves a worker's payo, it imposes a negative externality on others and may trigger further unraveling. Our data suggest that subjects reveal their productivity less frequently than predicted in equilibrium. A loaded frame emphasizing personal information about workers' health leads to even less revelation. We show that three canonical behavioral models all predict too little rather than too much revelation: level-k reasoning, quantal-response equilibrium, and to a lesser extent inequality aversion."
"625";625;"2013-041";"Goodness-of-fit Test for Specification of Semiparametric Copula Dependence Models";"Shulin Zhang, Ostap Okhrin, Qian M. Zhou and  Peter X.-K. Song";"B10";"2013-09-05";"C12,   C22,   C32,   C52,   G1";" This paper concerns goodness-of-fit test for semiparametric copula models. Our contribution is two-fold: we first propose a new test constructed via the comparison between ""in-sample"" and ""out-of-sample"" pseudolikelihoods, which avoids the use of any probability integral transformations. Under the null hypothesis that the copula model is correctly specified, we show that the proposed test statistic converges in probability to a constant equal to the dimension of the parameter space and establish the asymptotic normality for the test. Second, we introduce a hybrid mechanism to combine several test statistics, so that the resulting test will make a desirable test power among the involved tests. This hybrid method is particularly appealing when there exists no single dominant optimal test. We conduct comprehensive simulation experiments to compare the proposed new test and hybrid approach with the best ""blank test"" shown in Genest et al. (2009). For illustration, we apply the proposed tests to analyze three real datasets."
"626";626;"2013-042";"Volatility linkages between energy and agricultural commodity prices";"Brenda López Cabrera and  Franziska Schulz";"C11";"2013-09-11";"G19,   G29,   G22,   Q14,   Q4";" In this paper we investigate price and volatility risk originating in link- ages between energy and agricultural commodity prices in Germany and study their dynamics over time. We propose an econometric approach to quantify the volatility and correlation risk structure, which has a large impact for in- vestment and hedging strategies of market participants as well as for policy makers. Volatilities and their short and long run linkages (spillovers) are an- alyzed using a dynamic conditional correlation GARCH model as well as a multivariate multiplicative volatility model. Our approach provides a exible and accurate fitting procedure for volatility and correlation risk."
"627";627;"2013-043";"Testing the Preferred-Habitat Theory: The Role of Time-Varying Risk Aversion";"Till Strohsal";"C14C15";"2013-09-25";"E43,   C22";" Testing the Preferred-Habitat Theory: The Role ofTime-Varying Risk Aversion Abstract: This paper examines the preferred-habitat theory under time-varying risk aversion. The predicted positive relation between the term spread and relative supply of longer-term debt is stronger when risk aversion is high. To capture this effect, a time-varying coefficient model is introduced and applied to German bond data. The results support the theoretical predictions and indicate substantial time variation: under high risk aversion, yield spreads react about three times more strongly than when risk aversion is low. The accumulated response of term spreads to a one standard deviation change in debt supply ranges between 5 and 33 basis points."
"628";628;"2013-044";"Assortative matching through signals";"Friedrich Poeschel";"C7";"2013-09-26";"J64,   D83,   C78";" When agents do not know where to find a match, they search. However, agents could direct their search to agents who strategically choose a certain signal. Introducing cheap talk to a model of sequential search with bargaining, we find that signals will be truthful if there are mild complementarities in match production: supermodularity of the match production function is a necessary and sufficient condition. It simultaneously ensures perfect positive assortative matching, so that single-crossing property and sorting condition coincide. As the information from signals allows agents to avoid all unnecessary search, this search model exhibits nearly unconstrained efficiency."
"629";629;"2013-045";"Intertemporal Consumption and Debt Aversion: An Experimental Study";"Thomas Meissner";"C10";"2013-10-11";"C91,   D81,   E21";" This paper tests how subjects behave in an intertemporal consumption/saving experiment when borrowing is allowed and whether subjects treat debt differently than savings. Two treatments create environments where either saving or borrowing is required for optimal consumption. Since both treatments share the same optimal consumption levels, actual consumption choices can be directly compared across treatments. The experimental findings imply that deviations from optimal behavior are higher when subjects have to borrow than when they have to save in order to consume optimally, suggesting debt-aversion. Signifiant underconsumption is observed when subjects have to borrow in order to reach optimal consumption. Only weak evidence is found suggesting that subjects over-consume when saving is necessary for optimal consumption."
"630";630;"2013-046";"Automated Valuation Modelling: A Specification Exercise";"Rainer Schulz,  Martin Wersing and   Axel Werwatz";"B3";"2013-10-11";"R32,   C52,   C53";" Market value predictions for residential properties are important for investment decisions and the risk management of households, banks, and real estate developers. The increased access to market data has spurred the development and application of Automated Valuation Models (AVMs), which can provide appraisals at low cost. We discuss the stages involved when developing an AVM. By reflecting on our experience with md*immo, an AVM from Berlin, Germany, our paper contributes to an area that has not received much attention in the academic literature. In addition to discussing the main stages of AVM development, we examine empirically the statistical model development and validation step. We nd that automated outlier removal is important and that a log model performs best, but only if it accounts for the retransformation problem and heteroscedasticity."
"631";631;"2013-047";"Tie the straps: uniform bootstrap confidence bands for bounded influence curve estimators";"Wolfgang Karl Härdle,  Ya'acov Ritov and   Weining Wang";"B1Z";"2013-10-28";"C00,   C14";" We consider theoretical bootstrap ""coupling"" techniques for nonparametric robust smoothers and quantile regression, and verify the bootstrap improvement. To cope with curse of dimensionality, a variant of ""coupling"" bootstrap techniques are developed for additive models with both symmetric error distributions and further extension to the quantile regression framework. Our bootstrap method can be used in many situations like constructing condence intervals and bands. We demonstrate the bootstrap improvement over the asymptotic band theoretically, and also in simulations and in applications to firm expenditures and the interaction of economic sectors and the stock market."
"632";632;"2014-001";"Principal Component Analysis in an Asymmetric Norm";"Ngoc Mai Tran, Maria Osipenko and  Wolfgang Karl Härdle";"B1";"2014-01-02";"C38,   C61,   C63";" Principal component analysis (PCA) is a widely used dimension reduction tool in the analysis of many kind of high-dimensional data. It is used in signal processing, mechanical ingeneering, psychometrics, and other fields under different names. It still bears the same mathematical idea: the decomposition of variation of a high dimensional object into uncorrelated factors or components. However, in many of the above applications, one is interested in capturing the tail variables of the data rather than variation around the mean. Such applications include weather related event curves, expected shortfalls, and speeding analysis among others. These are all high dimensional tail objects which one would like to study in a PCA fashion. The tail character though requires to do the dimension reduction in an asymmetric norm rather than the classical L2-type orthogonal projection. We develop an analogue of PCA in an asymmetric norm. These norms cover both quantiles and expectiles, another tail event measure. The difficulty is that there is no natural basis, no 'principal components', to the k-dimensional subspace found. We propose two denitions of principal components and provide algorithms based on iterative least squares. We prove upper bounds on their convergence times, and compare their performances in a simulation study. We apply the algorithms to a Chinese weather dataset with a view to weather derivative pricing."
"633";633;"2014-002";"A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data";"Lijie Gu, Li Wang, Wolfgang Karl Härdle and  Lijian Yang";"B1";"2014-01-03";"C14,   C23";" We consider a varying coefficient regression model for sparse functional data, with time varying response variable depending linearly on some time independent covariates with coefficients as functions of time dependent covariates. Based on spline smoothing, we propose data driven simultaneous confidence corridors for the coefficient functions with asymptotically correct confidence level. Such confidence corridors are useful benchmarks for statistical inference on the global shapes of coefficient functions under any hypotheses. Simulation experiments corroborate with the theoretical results. An example in CD4/HIV study is used to illustrate how inference is made with computable p-values on the effects of smoking, preinfection CD4 cell percentage and age on the CD4 cell percentage of HIV infected patients under treatment."
"634";634;"2014-003";"An Extended Single Index Model with Missing Response at Random";"Qihua Wang, Tao Zhang and  Wolfgang Karl Härdle";"B1";"2014-01-03";"62J99,   62E20";" An extended single-index model is considered when responses are missing at random. A three-step estimation procedure is developed to define an estimator for the single index parameter vector by a joint estimating equation. The proposed estimator is shown to be asymptotically normal. An iterative scheme for computing this estimator is proposed. This algorithm only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high model dimensionality. Some simulation study is conducted to investigate the finite sample performances of the pro- posed estimators."
"635";635;"2014-004";"Structural Vector Autoregressive Analysis in a Data Rich Environment: A Survey";"Helmut Lütkepohl";"C15";"2014-01-09";"C32";" Large panels of variables are used by policy makers in deciding on policy actions. Therefore it is desirable to include large information sets in models for economic analysis. In this survey methods are reviewed for accounting for the information in large sets of variables in vector autoregressive (VAR) models. This can be done by aggregating the variables or by reducing the parameter space to a manageable dimension. Factor models reduce the space of variables whereas large Bayesian VAR models and panel VARs reduce the parameter space. Global VARs use a mixed approach. They aggregate the variables and use a parsimonious parametrisation. All these methods are discussed in this survey although the main emphasize is on factor models."
"636";636;"2014-005";"Functional stable limit theorems for efficient spectral covolatility estimators";"Randolf Altmeyer and  Markus Bibinger";"C12";"2014-01-13";"C14,   C32";" We consider noisy non-synchronous discrete observations of a continuous semimartingale. Functional stable central limit theorems are established under high-frequency asymptotics in three setups: onedimensional for the spectral estimator of integrated volatility, from two-dimensional asynchronous observations for a bivariate spectral covolatility estimator and multivariate for a local method of moments. The results demonstrate that local adaptivity and smoothing noise dilution in the Fourier domain facilitate substantial efficiency gains compared to previous approaches. In particular, the derived asymptotic variances coincide with the benchmarks of semiparametric CramÂ´er-Rao lower bounds and the considered estimators are thus asymptotically efficient in idealized sub-experiments. Feasible central limit theorems allowing for confidence are provided."
"637";637;"2014-006";"A consistent two-factor model for pricing temperature derivatives";"Andreas Groll, Brenda López-Cabrera and  Thilo Meyer-Brandis";"C11";"2014-01-13";"G19,   G29,   G22,   N23,   N5";" We analyze a consistent two-factor model for pricing temperature derivatives that incorporates the forward looking information available in the market by specifying a model for the dynamics of the complete meteorological forecast curve. The two-factor model is a generalization of the Nelson-Siegel curve model by allowing factors with mean-reversion to a stochastic mean for structural changes and seasonality for periodic patterns. Based on the outcomes of a statistical analysis of forecast data we conclude that the two-factor model captures well the stylized features of temperature forecast curves. In particular, a functional principal component analysis reveals that the model re ects reasonably well the dynamical structure of forecast curves by decomposing their shapes into a tilting and a bending factor. We continue by developing an estimation procedure for the model, before we derive explicit prices for temperature derivatives and calibrate the market price of risk (MPR) from temperature futures derivatives (CAT, HDD, CDD) traded at the Chicago Mercantile Exchange (CME). The factor model shows that the behavior of the implied MPR for futures traded in and out of the measurement period is more stable than other estimates obtained in the literature. This conrms that at least parts of the irregularity of the MPR is not due to irregular risk perception but rather due to information misspecification. Similar to temperature derivatives, this approach can be used for pricing other non-tradable assets."
"638";638;"2014-007";"Confidence Bands for Impulse Responses: Bonferroni versus Wald";"Helmut Lütkepohl, Anna Staszewska-Bystrova and  Peter Winker";"C15";"2014-01-16";"C32";" In impulse response analysis estimation uncertainty is typically displayed by constructing bands around estimated impulse response functions. These bands may be based on frequentist or Bayesian methods. If they are based on the joint distribution in the Bayesian framework or the joint asymptotic distribution possibly constructed with bootstrap methods in the frequentist framework often individual condence intervals or credibility sets are simply connected to obtain the bands. Such bands are known to be too narrow and have a joint condence content lower than the desired one. If instead the joint distribution of the impulse response coecients is taken into account and mapped into the band it is shown that such a band is typically rather conservative. It is argued that a smaller band can often be obtained by using the Bonferroni method. While these considerations are equally important for constructing forecast bands, we focus on the case of impulse responses in this study."
"639";639;"2014-008";"Simultaneous Confidence Corridors and Variable Selection for Generalized Additive Models";"Shuzhuan Zheng, Rong Liu, Lijian Yang and  Wolfgang Karl Härdle";"B1";"2014-01-16";"C35,   C52,   C53,   G33";" In spite of the widespread use of generalized additive models (GAMs), there is no well established methodology for simultaneous inference and variable selection for the components of GAM. There is no doubt that both, inference on the marginal component functions and their selection, are essential in this additive statistical models. To this end, we establish simultaneous confidence corridors (SCCs) and a variable selection criteria through the spline-backfitted kernel smoothing techniques. To characterize the global features of each component, SCCs are constructed for testing their shapes. By extending the BIC to additive models with identity/trivial link, an asymptotically consistent BIC approach for variable selection is proposed. Our procedures are examined in simulations for its theoretical accuracy and performance, and used to forecast the default probability of listed Japanese companies."
"640";640;"2014-009";"Structural Vector Autoregressions: Checking Identifying Long-run Restrictions via Heteroskedasticity";"Helmut Lütkepohl and  Anton Velinov";"C15";"2014-01-20";"C32";" Long-run restrictions have been used extensively for identifying structural shocks in vector autoregressive (VAR) analysis. Such restrictions are typically just-identifying but can be checked by utilizing changes in volatility. This paper reviews and contrasts the volatility models that have been used for this purpose. Three main approaches have been used, exogenously generated changes in the unconditional residual covariance matrix, changing volatility modelled by a Markov switching mechanism and multivariate generalized autoregressive conditional heteroskedasticity (GARCH) models. Using changes in volatility for checking long-run identifying restrictions in structural VAR analysis is illustrated by reconsidering models for identifying fundamental components of stock prices."
"641";641;"2014-010";"Efficient Iterative Maximum Likelihood Estimation of High-Parameterized Time Series Models";"Nikolaus Hautsch, Ostap Okhrin and  Alexander Ristig";"B8B10";"2014-01-27";"C13,  C32,  C50";" We propose an iterative procedure to efficiently estimate models with complex log-likelihood functions and the number of parameters relative to the observations being potentially high. Given consistent but inefficient estimates of sub-vectors of the parameter vector, the procedure yields computationally tractable, consistent and asymptotic efficient estimates of all parameters. We show the asymptotic normality and derive the estimator's asymptotic covariance in dependence of the number of iteration steps. To mitigate the curse of dimensionality in high-parameterized models, we combine the procedure with a penalization approach yielding sparsity and reducing model complexity. Small sample properties of the estimator are illustrated for two time series models in a simulation study. In an empirical application, we use the proposed method to estimate the connectedness between companies by extending the approach by Diebold and Yilmaz (2014) to a high-dimensional non-Gaussian setting."
"642";642;"2014-011";"Fiscal Devaluation in a Monetary Union";"Philipp Engler, Giovanni Ganelli, Juha Tervala and  Simon Voigts";"C7";"2014-01-27";"E32,  E62,  F32,  F41";" Between 1999 and the onset of the economic crisis in 2008 real ex-change rates in Greece, Ireland, Italy, Portugal and Spain appreciated relative to the rest of the euro area. This divergence in competitiveness was reflected in the emergence of current account imbalances. Given that exchange rate devaluations are no longer available in a monetary union, one potential way to address such imbalances is through a fiscal devaluation. We use a DSGE model calibrated to the euro area to investigate the impact of a fiscal devaluation, modeled as a revenue-neutral shift from employers' social contributions to the Value Added Tax. We find that a fiscal devaluation carried out in `Southern European countries' has a strong positive eect on output, but a mild eect on the trade balance of these countries. In addition, the negative effect on `Central-Northern countries' output is weak."
"643";643;"2014-012";"Nonparametric Estimates for Conditional Quantiles of Time Series";"Jürgen Franke, Peter Mwita and  Weining Wang";"Z";"2014-01-31";"C00,   C14,   C50,   C58";" We consider the problem of estimating the conditional quantile of a time series fYtg at time t given covariates Xt, where Xt can ei- ther exogenous variables or lagged variables of Yt . The conditional quantile is estimated by inverting a kernel estimate of the conditional distribution function, and we prove its asymptotic normality and uni- form strong consistency. The performance of the estimate for light and heavy-tailed distributions of the innovations are evaluated by a simulation study. Finally, the technique is applied to estimate VaR of stocks in DAX, and its performance is compared with the existing standard methods using backtesting."
"644";644;"2014-013";"Product Market Deregulation and Employment Outcomes: Evidence from the German Retail Sector";"Charlotte Senftleben-König";"A9";"2014-01-31";"J21,   L51,   L81";" This paper investigates the short- and medium-term effects of the deregulation of shopopening hours legislation on retail employment in Germany. In 2006, the legislative competence was shifted from the federal to the state level, leading to a gradual deregulation of shop opening restrictions in most of GermanyÂ’s sixteen federal states. The paper exploits regional variation in the legislation in order to identify the effect product market deregulation has on retail employment. We find robust evidence that the deregulation of shop closing legislation had negative effects on retail employment, with considerable heterogeneity in terms of the type of employment as well as establishment size. That is, the employment losses are most pronounced for small retail stores and are almost exclusively borne by full-time employees."
"645";645;"2014-014";"Estimation procedures for exchangeable Marshall copulas with hydrological application";"Fabrizio Durante and  Ostap Okhrin";"B10";"2014-01-31";"C13,   C14";" Complex phenomena in environmental sciences can be conveniently represented by several inter-dependent random variables. In order to describe such situations, copula-based models have been studied during the last year. In this paper, we consider a novel family of bivariate copulas, called exchangeable Marshall copulas. Such copulas describe both positive and (upper) tail association between random variables. Specically, inference procedures for the family of exchangeable Marshall copulas are introduced, based on the estimation of their (univariate) generator. Moreover, the performance of the proposed methodologies is shown in a simulation study. Finally, an illustration describes how the proposed procedures can be useful in a hydrological application."
"646";646;"2014-015";"Ladislaus von Bortkiewicz - statistician, economist, and a European intellectual";"Wolfgang Karl Härdle and  Annette B. Vogt";"B1";"2014-02-14";"N01,   D02,   B14,   B16";" Ladislaus von Bortkiewicz (1868 - 1931) was a European statistician. His scientific work covered theoretical economics, stochastics, mathematical statistics and radiology, today we would call him a cross disciplinary scientist. With his clear views on mathematical principles with their applications in these fields he stood in conflict with the mainstream economic schools in Germany at the dawn of the 20th century. He had many prominent students (Gumbel, Leontief, Freudenberg among them) and he carved out the path of modern statistical thinking. He was a true European intellectual with a career path from St. Petersburg via Gottingen to StraÃŸburg and finally the Berliner UniversitÃ¤t, now Humboldt-UniversitÃ¤t zu Berlin. He is known for the precise calibration of insurance claims applying the - at that time hardly known - Poisson distribution to Prussian horse kick and child suicide data. He proposed a simple solution to the Marxian transformation problem and wrote numerous articles and books on the mathematical treatment of statistical (including radiological physical) data. In this article we sketch his life and work and point out the prominent role that he has in today's statistical thinking."
"647";647;"2014-016";"An Application of Principal Component Analysis on Multivariate Time-Stationary Spatio-Temporal Data";"Stephan Stahlschmidt, Wolfgang Karl Härdle and  Helmut Thome";"B1";"2014-02-19";"C31,   C33,   R11";" Principal component analysis denotes a popular algorithmic technique to dimension reduction and factor extraction. Spatial variants have been proposed to account for the particularities of spatial data, namely spatial heterogeneity and spatial autocorrelation, and we present a novel approach which transfers principal component analysis into the spatio-temporal realm. Our approach, named stPCA, allows for dimension reduction in the attribute space while striving to preserve much of the data's variance and maintaining the data's original structure in the spatio-temporal domain. Additionally to spatial autocorrelation stPCA exploits any serial correlation present in the data and consequently takes advantage of all particular features of spatial-temporal data. A simulation study underlines the superior performance of stPCA if compared to the original PCA or its spatial variants and an application on indicators of economic deprivation and urbanism demonstrates its suitability for practical use."
"648";648;"2014-017";"The composition of government spending and the multiplier at the Zero Lower Bound";"Julien Albertini, Arthur Poirier and  Jordan Roulleau-Pasdeloup";"C7";"2014-02-19";"E31,   E32,   E52,   E62";" We investigate the size of the multiplier at the ZLB in a New keynesian model. It ranges from around -0.25 to +1.5, depending on the extent to which government spending is productive, substitutable or not for private consumption."
"649";649;"2014-018";"Interacting Product and Labor Market Regulation and the Impact of Immigration on Native Wages";"Susanne Prantl and  Alexandra Spitz-Oener";"A9";"2014-02-19";"J61,   L50,   J3";" Does interacting product and labor market regulation alter the impact of immigration on wages of competing native workers? Focusing on the large, sudden and unanticipated wave of migration from East to West Germany after German reunification and allowing for endogenous immigration, we compare native wage reactions across dierent segments of the West German labor market: one segment without product and labor market regulation, to which standard immigration models best apply, one segment in which product and labor market regulation interact, and one segment covering intermediate groups of workers. We nd that the wages of competing native West Germans respond negatively to the large influx of similar East German workers in the segment with almost free firm entry into product markets and weak worker influence on the decision-making of firms. Competing native workers are insulated from such pressure if firm entry regulation interacts with labor market institutions, implying a strong influence of workers on the decision-making of profit-making firms."
"650";650;"2014-019";"Unemployment benefits extensions at the zero lower bound on nominal interest rate";"Julien Albertini and  Arthur Poirier";"C7";"2014-02-19";"E24,   E31,   E32,   E43,   E5";" In this paper we investigate the impact of the recent US unemployment benefits extension on the labor market dynamic when the nominal interest rate is held at the zero lower bound (ZLB). Using a New Keynesian model, our quantitative experiments suggest that, in contrast to the existing literature that ignores the liquidity trap situation, unemployment benefits expansions cause a wage and inflationary pressure which curb the increase in real interest rate and slightly reduce unemployment at the ZLB. Outside the ZLB, it has adverse effects, meaning that unemployment insurance benefits should be adjusted according to the macroeconomic conditions if the scope is to reduce unemployment. Furthermore, the ZLB amplifies the labor market downturn. An unconstrained monetary policy rule, i.e. negative interest rate, could have reduced the unemployment rate by around 1 percentage point in the trough of the recession."
"651";651;"2014-020";"Modelling spatiotemporal variability of temperature";"Xiaofeng Cao, Ostap Okhrin, Martin Odening and  Matthias Ritter";"B10C11";"2014-02-25";"C14,  C53,  G32";" Forecasting temperature in time and space is an important precondition  for both the design of weather derivatives and the assessment of the hedging effectiveness  of index based weather insur-ance. In this article, we show how this task can be accomplished by  means of Kriging techniques. Moreover, we compare Kriging with a dynamic semiparametric factor  model (DSFM) that has been recently developed for the analysis of high dimensional financial data.  We apply both methods to comprehensive temperature data covering a large area of China and assess  their performance in terms of predicting a temperature index at an unobserved location.  The results show that the DSFM performs worse than standard Kriging techniques. Moreover, we show how geographic  basis risk inherent to weather derivatives can be mitigated by regional diversification."
"652";652;"2014-021";"Do Maternal Health Problems Influence Child's Worrying Status? Evidence from British Cohort Study";"Xianhua Dai, Wolfgang Karl Härdle and  Keming Yu";"B1";"2014-03-10";"C11,   C38,   C63";" The influence of maternal health problems on child's worrying status is important in practice in terms of the intervention of maternal health problems early for the influence on child's worrying status. Conventional methods apply symmetric prior distributions such as a normal distribution or a Laplace distribituion for regression coefficients, which may be suitable for median regression and exhibit no robustness to outliers. This paper develops a quantile regression on linear panel data model without heterogeneity from a Bayesian point of view, and examines the influence of maternal health problems on child's worrying status. Upon a location-scale mixture representation of the asymmetric Laplace error distribution, this paper provides how the posterior distribution can be sampled and summarized by Markov chain Monte Carlo method. Applying for the 1970 British Cohort Study data, we find and that a different maternal health problem has different influence on child's worrying status at different quantiles. In addition, applying stochastic search variable selection for maternal health problems in the 1970 British Cohort Study data, we find that maternal nervous breakdown, in our work, among the 25 maternal health problems, contributes most to influence the child's worrying status."
"653";653;"2014-022";"Nonparametric Test for a Constant Beta over a Fixed Time Interval";"Markus Reiß, Viktor Todorov and  George Tauchen";"C12";"2014-03-10";"C14,   C32,   C58,   G10";" We derive a nonparametric test for constant (continuous) beta over a fixed interval of time. Continuous beta is defined as the ratio of the continuous covariation between an asset and observable risk factor (e.g., the market return) and the continuous variation of the latter. Our test is based on discrete observations of a bivariate It^o semimartingale with mesh of the observation grid shrinking to zero. We rst form a consistent and asymptotically mixed normal estimate of beta using all the observations within the time interval under the null hypothesis that beta is constant. Using it we form an estimate of the residual component of the asset returns that is orthogonal (in martingale sense) to the risk factor. Our test is then based on the distinctive asymptotic behavior, under the null and alternative hypothesis, of the sample covariation between the risk factor and the estimated residual component of the asset returns over blocks with asymptotically shrinking time span. Optimality of the test is considered as well. We document satisfactory finite sample properties of the test on simulated data. In an empirical application based on 10-minute data we analyze the time variation in market betas of four assets over the period 2006-2012. The results suggest that (for likely structural reasons) for one of the assets there is statistically nontrivial variation in market beta even for a period as short as a week. On the other hand, for the rest of the assets in our analysis we find evidence that a window of constant beta of one week to one month is statistically plausible."
"654";654;"2014-023";"Inflation Expectations Spillovers between the United States and Euro Area";"Aleksei Netsunajev and  Lars Winkelmann";"C14C15";"2014-03-18";"E31,  F42,  E52";" We quantify spillovers of inflation expectations between the United States (US) and Euro Area (EA) based on break-even inflation (BEI) rates. In contrast to previous studies, we model US and EA BEI rates jointly in a structural vector autoregressive (SVAR) model. The SVAR approach allows to identify US and EA specific inflation expectations shocks. By modeling the heteroscedasticity of the data, we are able to test the identifying restrictions of structural shocks and analyze time-varying spillovers. Adjusted for BEI risk premia, our main result suggests that spillovers of inflation expectations increase during times of macroeconomic stress. We document a significant impact of the European sovereign debt crisis on US expectations. The finding contributes to the discussion about a weakening of inflation control by national central banks and speaks in favor of internationally coordinated policy actions, especially during crisis times."
"655";655;"2014-024";"Peer Effects and Students’ Self-Control";"Berno Buechel, Lydia Mechtenberg and  Julia Petersen";"A8";"2014-04-01";"C93,  D85,  I21,  J24";" We conducted a multi-wave field experiment to study the interaction of peer effects and selfcontrol among undergraduate students. We use a behavioral measure of self-control based on whether students achieve study related goals they have set for themselves. We find that both self-control and the number of talented friends increase studentsÂ’ performance. We then set out to test the theoretical prediction of Battaglini, BÃ©nabou and Tirole (2005) that (only) sufficiently self-controlled individuals profit from interactions with peers. We find that peers with high self-control are more likely to connect to others, have a higher overall number of friends and have a higher number of talented friends. Moreover, positive news about self-controlled behavior of their peers increases studentsÂ’ own perseverance. Hence, our findings are consistent with the model of Battaglini, BÃ©nabou and Tirole. In addition, we find that female students are more likely to have high self-control, but do not outperform male students. One reason for this is that female students have a lower number of talented friends than their male counterparts, thereby profiting less from positive peer effects. and analyze time-varying spillovers. Adjusted for BEI risk premia, our main result suggests that spillovers of inflation expectations increase during times of macroeconomic stress. We document a significant impact of the European sovereign debt crisis on US expectations. The finding contributes to the discussion about a weakening of inflation control by national central banks and speaks in favor of internationally coordinated policy actions, especially during crisis times."
"656";656;"2014-025";"Is there a demand for multi-year crop insurance?";"Maria Osipenko, Zhiwei Shen and  Martin Odening";"B1C11";"2014-04-15";"D81,  G22,  Q14";" In this paper we adapt a dynamic discrete choice model to examine the aggregated demand for single- and multi-year crop insurance contracts. We show that in a competitive insurance market with heterogeneous risk averse farmers, there is simultaneous demand for both insurance contracts. Moreover, the introduction of multi-year contracts enhances the market penetration of insurance products. Using U.S. corn yield data, we empirically assess the potential of multi-year crop insurance."
"657";657;"2014-026";"Credit Risk Calibration based on CDS Spreads";"Shih-Kang Chao, Wolfgang Karl Härdle and  Hien Pham-Thu";"B1";"2014-05-09";"G12,   G13,   G23";" As observed in the financial crisis, CDS spreads tend to increase simutaneously as a reaction to common shocks. Focusing on the spillover effects triggered by extreme events, we propose a credit risk analysis tool by applying credit default swap spread returns to the concept of 4CoVaR suggested by Adrian and Brunnermeier (2011). The interconnection and mutual impact on credit spreads are investigated based on CDS spreads of the biggest derivative dealers in the market. By including factors identified as determinants of CDS spreads to the set of explanatory variables such as equity return and equity volatility and implementing the variable selection technique least absolute shrinkage and selection operator (LASSO), the results demonstrate an improved performance in CDS spread VaR calculation. The enhancement is more significant in pre-crisis period but both methodologies tend to overestimate risk in turbulent period. Further, non-linear effects between CDS spreads in extreme events are captured by the introduction of a partial linear model in the CoVaR calculation."
"658";658;"2014-027";"Stale Forward Guidance";"Gunda-Alexandra Detmers and  Dieter Nautz";"C14";"2014-05-13";"E52,   E58";" An increasing number of central banks manage market expectations via interest rate projections. Typically, those projections are updated only quarterly and thus, may become stale when new information enters the market. We use data from New Zealand to investigate the time-varying and state-dependent effects of interest rate projections on market expectations and interest rate uncertainty. Confirming the stabilizing effect of fresh central bank announcements, we show that interest rate uncertainty rises between two projection releases. Moreover, rate uncertainty and the importance of macroeconomic news increase if expectations deviate from the rate projected by the central bank. Counterfactual analysis suggests that the efficiency of projections would improve if the central bank updated its projection whenever it becomes stale."
"659";659;"2014-028";"Confidence Corridors for Multivariate Generalized Quantile Regression";"Shih-Kang Chao, Katharina Proksch, Holger Dette and  Wolfgang Härdle";"B1";"2014-05-16";"C2,   C12,   C14";" We focus on the construction of confidence corridors for multivariate nonparametric generalized quantile regression functions. This construction is based on asymptotic results for the maximal deviation between a suitable nonparametric estimator and the true function of interest which follow after a series of approximation steps including a Bahadur representation, a new strong approximation theorem and exponential tail inequalities for Gaussian random fields. As a byproduct we also obtain confidence corridors for the regression function in the classical mean regression. In order to deal with the problem of slowly decreasing error in coverage probability of the asymptotic confidence corridors, which results in meager coverage for small sample sizes, a simple bootstrap procedure is designed based on the leading term of the Bahadur representation. The finite sample properties of both procedures are investigated by means of a simulation study and it is demonstrated that the bootstrap procedure considerably outperforms the asymptotic bands in terms of coverage accuracy. Finally, the bootstrap confidence corridors are used to study the efficacy of the National Supported Work Demonstration, which is a randomized employment enhancement program launched in the 1970s. This article has supplementary materials online."
"660";660;"2014-029";"Information Risk, Market Stress and Institutional Herding in Financial Markets: New Evidence Through the Lens of a Simulated Model";"Christopher Boortz, Stephanie Kremer, Simon Jurkatis and  Dieter Nautz";"C14";"2014-05-20";"D81,   D82,   G14";" This paper employs numerical simulations of the Park and Sabourian (2011) herd model to derive new theory-based predictions for how information risk and market stress influence aggregate herding intensity. We test these predictions empirically using a comprehensive data set of highfrequency and investor-specic trading data from the German stock market. Exploiting intra-day patterns of institutional trading behavior, we confirm that higher information risk increases both buy and sell herding. The model also explains why buy, not sell, herding is more pronounced during the financial crisis."
"661";661;"2014-030";"Forecasting Generalized Quantiles of Electricity Demand: A Functional Data Approach";"Brenda López Cabrera and  Franziska Schulz";"C11";"2014-05-30";"G19,   G29,   G22,   Q14,   Q4";" Electricity load forecasts are an integral part of many decision-making pro- cesses in the electricity market. However, most literature on electricity load forecasting concentrates on deterministic forecasts, neglecting possibly impor- tant information about uncertainty. A more complete picture of future demand can be obtained by using distributional forecasts, allowing for a more ecient decision-making. A predictive density can be fully characterized by tail mea- sures such as quantiles and expectiles. Furthermore, interest often lies in the accurate estimation of tail events rather than in the mean or median. We pro- pose a new methodology to obtain probabilistic forecasts of electricity load, that is based on functional data analysis of generalized quantile curves. The core of the methodology is dimension reduction based on functional principal components of tail curves with dependence structure. The approach has sev- eral advantages, such as exible inclusion of explanatory variables including meteorological forecasts and no distributional assumptions. The methodol- ogy is applied to load data from a transmission system operator (TSO) and a balancing unit in Germany. Our forecast method is evaluated against other models including the TSO forecast model. It outperforms them in terms of mean absolute percentage error (MAPE) and achieves a MAPE of 2:7% for the TSO."
"662";662;"2014-031";"Structural Vector Autoregressions with Smooth Transition in Variances - The Interaction Between U.S. Monetary Policy and the Stock Market";"Helmut Lütkepohl and  Aleksei Netsunajev";"C15";"2014-06-17";"C32";" In structural vector autoregressive analysis identifying the shocks of interest via heteroskedasticity has become a standard tool. Unfortunately, the approaches currently used for modelling heteroskedasticity all have drawbacks. For instance, assuming known dates for variance changes is often unrealistic while more  exible models based on GARCH or Markov switching residuals are diffcult to handle from a statistical and computational point of view. Therefore we propose a model based on a smooth change in variance that is  exible as well as relatively easy to estimate. The model is applied to a five-dimensional system of U.S. variables to explore the interaction between monetary policy and the stock market. It is found that previously used conventional identification schemes in this context are rejected by the data if heteroskedasticity is allowed for. Shocks identified via heteroskedasticity have a different economic interpretation than the shocks identified using conventional methods."
"663";663;"2014-032";"TEDAS - Tail Event Driven ASset Allocation";"Wolfgang Karl Härdle, Sergey Nasekin, David Lee Kuo Chuen and  Phoon Kok Fai";"B1";"2014-06-13";"C00,   C14,   C50,   C58";" Portfolio selection and risk management are very actively studied topics in quantitative finance and applied statistics. They are closely related to the dependency structure of portfolio assets or risk factors. The correlation structure across assets and opposite tail movements are essential to the asset allocation problem, since they determine the level of risk in a position. Correlation alone is not informative on the distributional details of the assets. By introducing TEDAS -Tail Event Driven ASset allocation, one studies the dependence between assets at different quantiles. In a hedging exercise, TEDAS uses adaptive Lasso based quantile regression in order to determine an active set of negative non-zero coefficients. Based on these active risk factors, an adjustment for intertemporal correlation is made. Finally, the asset allocation weights are determined via a Cornish-Fisher Value-at-Risk optimization. TEDAS is studied in simulation and a practical utility-based example using hedge fund indices."
"664";664;"2014-033";"Discount Factor Shocks and Labor Market Dynamics";"Julien Albertini and  Arthur Poirier";"C7";"2014-07-01";"E3,   J6";" In this paper we investigate the labor market dynamics in a matching model where fluctuations are driven by movements in the discount factor. A comparison with the standard productivity shock is provided. Movements in the discount factor can be used as a proxy for variations in financial risks, especially the expected payoff from hiring workers. It is shown that the canonical matching model under a very standard calibration is able to generate an important volatility of unemployment and vacancies with respect to output. We estimate the structural model with the two shocks and using the Bayesian methodology. The bulk of variations in unemployment and vacancies is mainly explained by disturbances pertaining to the discount factor. Productivity shocks account for most of the historical output variations but the discount factor plays a more important role over the last two decades."
"665";665;"2014-034";"Risky Linear Approximations";"Alexander Meyer-Gohde";"C7";"2014-07-03";"C61,   C63,   E17";" I construct risk-corrected approximations of the policy functions of DSGEmodels around the stochastic steady state and ergodic mean that are linear in the state variables. The resulting approximations are uniformly more accurate than standard linear approximations and capture the dynamics of asset pricing variables such as the expected risk premium missed by standard linear approximations. The algorithm is fast and reliable, requiring only the solution of linear equations using standard perturbation output. I examine the joint macroeconomic and asset pricing implications of a real business cycle model with stochastic trends and recursive preferences. The method is able to estimate risk aversion under these preferences using the Kalman filter, where a standard linear approximation provides no information and alternative methods require computationally intensive particle filters subject to sampling variation."
"666";666;"2014-035";"Adaptive Order Flow Forecasting with Multiplicative Error Models";"Wolfgang K. Härdle, Andrija Mihoci and  Christopher Hian-Ann Ting";"B1";"2014-07-08";"C41,   C51,   C53,   G12,   G1";" A flexible statistical approach for the analysis of time-varying dynamics of transaction data on financial markets is here applied to intra-day trading strategies. A local adaptive technique is used to successfully predict financial time series, i.e., the buyer and the seller-initiated trading volumes and the order flow dynamics. Analysing order flow series and its information content of mini Nikkei 225 index futures traded at the Osaka Securities Exchange in 2012 and 2013, a data-driven optimal length of local windows up to approximately 1-2 hours is reasonable to capture parameter variations and is suitable for short-term prediction. Our proposed trading strategies achieve statistical arbitrage opportunities and are therefore beneficial for quantitative finance practice."
"667";667;"2014-036";"Portfolio Decisions and Brain Reactions via the CEAD method";"Piotr Majer, Peter N.C. Mohr, Hauke R. Heekeren and  Wolfgang K. Härdle";"B1";"2014-07-17";"C3,   C6,   C9,   C14,   D8";" Decision making can be a complex process requiring the integration of several attributes of choice options. Understanding the neural processes underlying (uncertain) investment decisions is an important topic in neuroeconomics. We analyzed functional magnetic resonance imaging (fMRI) data from an investment decision (ID) study for ID-related effects. We propose a new technique for identifying activated brain regions: Cluster, Estimation, Activation and Decision (CEAD) method. Our analysis is focused on clusters of voxels rather than voxel units. Thus, we achieve a higher signal to noise ratio within the unit tested and a smaller number of hypothesis tests compared with the often used General Linear Model (GLM). We propose to first conduct the brain parcellation by applying spatially constrained NCUT spectral clustering. The information within each cluster can then be extracted by the flexible DSFM dimension reduction technique and finally be tested for differences in activation between conditions. This sequence of Cluster, Estimation, Activation and Decision admits a model-free analysis of the local BOLD signal. Applying a GLM on the DSFM-based time series resulted in a significant correlation between the risk of choice options and changes in fMRI signal in the anterior insula (aINS) and DMPFC. Additionally, individual differences in decision-related reactions within the DSFM time series predicted individual differences in risk attitudes as modeled with the framework of the mean-variance model. "
"668";668;"2014-037";"Common price and volatility jumps in noisy high-frequency data";"Markus Bibinger and  Lars Winkelmann";"C12C14";"2014-07-16";"E58,   C14";" We introduce a statistical test for simultaneous jumps in the price of a financial asset and its volatility process. The proposed test is based on high-frequency tick-data and is robust to market microstructure frictions. To localize volatility jumps, we design and analyze a nonparametric spectral estimator of the spot volatility process. A simulation study and an empirical example with NASDAQ order book data demonstrate the practicability of the proposed methods and highlight the important role played by price volatility co-jumps."
"669";669;"2014-038";"Spatial Wage Inequality and Technological Change";"Charlotte Senftleben-Koenig and  Hanna Wielandt";"A9";"2014-08-11";"J31,   O33,   R23";" During the last decades, wage inequality in Germany has considerably increased both within and across regions. Building on concepts of the task-based approach, this paper studies whether and to what extent these developments are driven by technological change. We present novel evidence that technological change is positively related to intra-regional wage inequality. This is driven by increases in the compensation for non-routine cognitive tasks that are prevalent at upper percentiles of the wage distribution combined with decreases in the compensation for non-routine manual tasks, which are located at lower percentiles. Because there exists substantial variation in the degree of technology exposure across German regions, technological change can also explain part of the rise in inter-regional wage inequality."
"670";670;"2014-039";"The integration of credit default swap markets in the pre and post-subprime crisis in common stochastic trends";"Cathy Yi-Hsuan Chen, Wolfgang K. Härdle and  Hien Pham-Thu";"B1";"2014-08-29";"C38,   G32,   E43";" It was evident that credit default swap (CDS) spreads have been highly correlated during the recent financial crisis. Motivated by this evidence, this study attempts to investigate the extent to which CDS markets across regions, maturities and credit ratings have integrated more in crisis. By applying the Panel Analysis of Non-stationarity in Idiosyncratic and Common components method (PANIC) developed by Bai and Ng (2004), we observe a potential shift in CDS integration between the pre- and post-Lehman collapse period, indicating that the system of CDS spreads is tied to a long-run equilibrium path. This finding contributes to a credit risk management task and also coincides with the missions of Basel III since the more integrated CDS markets could result in correlated default, credit contagion and simultaneous downgrading in the future."
"671";671;"2014-040";"Localising Forward Intensities for Multiperiod Corporate Default";"Dedy Dwi Prastyo and   Wolfgang Karl Härdle";"B1";"2014-09-03";"C41,   C53,   C58,   G33";" Using a local adaptive Forward Intensities Approach (FIA) we investigate multiperiod corporate defaults and other delisting schemes. The proposed approach is fully datadriven and is based on local adaptive estimation and the selection of optimal estimation windows. Time-dependent model parameters are derived by a sequential testing procedure that yields adapted predictions at every time point. Applying the proposed method to monthly data on 2000 U.S. public rms over a sample period from 1991 to 2011, we estimate default probabilities over various prediction horizons. The prediction performance is evaluated against the global FIA that employs all past observations. For the six months prediction horizon, the local adaptive FIA performs with the same accuracy as the benchmark. The default prediction power is improved for the longer horizon (one to three years). Our local adaptive method can be applied to any other specications of forward intensities."
"672";672;"2014-041";"Certification and Market Transparency";"Konrad Stahl and   Roland Strausz";"A8";"2014-09-03";"D82,   G24,   L15";" We provide elementary insights into the effectiveness of certification to increase market transparency. In a market with opaque product quality, sellers use certification as a signaling device, while buyers use it as an inspection device. This difference alone implies that seller-certification yields more transparency and higher social welfare. Under buyer-certification profit maximizing certifiers further limit transparency, but because seller-certification yields larger profits, active regulation concerning the mode of certification is not needed. These findings are robust and widely applicable to, for instance, patents, automotive parts, and financial products."
"673";673;"2014-042";"Beyond dimension two: A test for higher-order tail risk";"Carsten Bormann,  Melanie Schienle and   Julia Schaumburg";"B1";"2014-09-03";"C01,   C46,   C58";" In practice, multivariate dependencies between extreme risks are often only assessed in a pairwise way. We propose a test to detect when tail dependence is truly high{dimensional and bivariate simplications would produce misleading results. This occurs when a signicant portion of the multivariate dependence structure in the tails is of higher dimension than two. Our test statistic is based on a decomposition of the stable tail dependence function, which is standard in extreme value theory for describing multivariate tail dependence. The asymptotic properties of the test are provided and a bootstrap based nite sample version of the test is suggested. A simulation study documents the good performance of the test for standard sample sizes. In an application to international government bonds, we detect a high tail{risk and low return situation during the last decade which can essentially be attributed to increased higher{order tail risk. We also illustrate the empirical consequences from ignoring higher-dimensional tail risk."
"674";674;"2014-043";"Semiparametric Estimation with Generated Covariates";"Enno Mammen,  Christoph Rothe and   Melanie Schienle";"B1";"2014-09-03";"C14,   C31";" We study a general class of semiparametric estimators when the innite-dimensional nuisance parameters include a conditional expectation function that has been estimated nonparametri- cally using generated covariates. Such estimators are used frequently to e.g. estimate nonlinear models with endogenous covariates when identication is achieved using control variable tech- niques. We study the asymptotic properties of estimators in this class, which is a non-standard problem due to the presence of generated covariates. We give conditions under which estimators are root-n consistent and asymptotically normal, derive a general formula for the asymptotic variance, and show how to establish validity of the bootstrap."
"675";675;"2014-044";"On the Timing of Climate Agreements";"Robert C. Schmidt and   Roland Strausz";"A8";"2014-09-05";"D62,   F53,   H23,   Q55";" A central issue in climate policy is the question whether long-term targets for green- house gas emissions should be adopted. This paper analyzes strategic effects related to the timing of such commitments. Using a two-country model, we identify a redistributive effect that undermines long-term cooperation when countries are asymmetric and side payments are unavailable. The effect enables countries to shift rents strategically via their R&D efforts under delayed cooperation. In contrast, a complementarity effect stabi- lizes long-term cooperation, because early commitments in abatement induce countries to invest more in low-carbon technologies, and create additional knowledge spillovers. Con- trasting both effects, we endogenize the timing of climate agreements."
"676";676;"2014-045";"Optimal Sales Contracts with Withdrawal Rights";"Daniel Krähmer and   Roland Strausz";"A8";"2014-09-05";"D82,   H57";" We introduce ex post participation constraints in the standard sequential screening model. This captures the presence of consumer withdrawal rights as, for instance, mandated by EU regulation of Â“distance sales contractsÂ”. With such additional constraints, the optimal contract is static and, unlike with only ex ante participation constraints, does not elicit the agentÂ’s information sequentially. With ex post participation constraints it is insufficient to consider only local incentive constraints. We develop a novel technique to identify the relevant global constraints."
"677";677;"2014-046";"Ex post information rents in sequential screening";"Daniel Krähmer and   Roland Strausz";"A8";"2014-09-05";"D82,   H57";" We study ex post information rents in sequential screening models where the agent receives private ex ante and ex post information. The principal has to pay ex post information rents for preventing the agent to coordinate lies about his ex ante and ex post information. When the agentÂ’s ex ante information is discrete, these rents are positive, whereas they are zero in continuous models. Consequently, full disclosure of ex post information is generally suboptimal. Optimal disclosure rules trade off the benefits from adapting the allocation to better information against the effect that more information aggravates truth-telling."
"678";678;"2014-047";"Similarities and Differences between U.S. and German Regulation of the Use of Derivatives and Leverage by Mutual Funds – What Can Regulators Learn from Each Other?";"Dominika Paula Galkiewicz";"A13";"2014-09-05";"G15,   G18";" This study analyzes current regulation with respect to the use of derivatives and leverage by mutual funds in the U.S. and Germany. After presenting a detailed overview of U.S. and German regulations, this study thoroughly compares the level of flexibility funds have in both countries. I find that funds in the U.S. and Germany face limits on direct leverage (amount of bank borrowing) of up to 33% and 10% of their net assets, respectively. Funds can extend these limits indirectly by using derivatives beyond their net assets (e.g., by selling credit default swaps protection with a notional amount equal to their net assets). Additionally, issuer-oriented rules in the U.S. and Germany account for issuer risk differently: U.S. funds have greater discretion to undervalue derivative exposure compared to German funds. All analyses of this study reveal that under existing derivative and leverage regulation, funds in both countries are able to increase risk by using derivatives up to the point at which it is possible for them to default solely due to investments in derivatives. The results of this study are highly relevant for the public and regulators."
"679";679;"2014-048";"That's how we roll: an experiment on rollover risk";"Ciril Bosch-Rosa";"C10";"2014-09-09";"C92,   C91,   G01,   GO2,   G2";" There is consensus that the recent nancial crisis revolved around a crash of the short-term credit market. Yet there is no agreement around the necessary policies to prevent another credit freeze. In this experiment we test the eects that contract length (i.e. maturity mismatch) has on the market-wide supply of short-term credit. Our main result is that, while credit markets with shorter maturities are less prone to freezes, the optimal policy should be state-dependent, favoring long contracts and lower maturity mismatch when the economy is in good shape, and allowing for short-term contracts when the economy is in a recession. We also report the possibility of credit runs on rms with strong fundamentals, something that cannot be observed in the canonical static models of nancial panics. Finally, we show that our experimental design produces rich learning dynamics, with a text-book bubble and crash pattern in the market for short-term credit."
"680";680;"2014-049";"Comparing Solution Methods for DSGE Models with Labor Market Search";"Hong Lan";"C7";"2014-09-15";"C63,   C68,   E32";" I compare the performance of solution methods in solving a standard real business cycle model with labor market search frictions. Under the conventional calibration, the model is solved by the projection method using the Chebyshev polynomials as its basis, and the perturbation methods up to third order in both levels and logs. Evaluated by two accuracy tests, the projection approximation achieves the highest degree of accuracy, closely followed by the third order perturbation in levels. Although different in accuracy, all the approximated solutions produce simulated moments similar in value."
"681";681;"2014-050";"Volatility Modelling of CO2 Emission Allowance Spot Prices with Regime-Switching GARCH Models";"Thijs Benschop and   Brenda López Cabrera";"C11";"2014-09-19";"C53,   G17,   Q49,   Q53,   Q5";" We analyse the short-term spot price of European Union Allowances (EUAs), which is of particular importance in the transition of energy markets and for the development of new risk management strategies. Due to the characteristics of the price process, such as volatility persistence, breaks in the volatility process and heavy-tailed distributions, we investigate the use of Markov switching GARCH (MS-GARCH) models on daily spot market data from the second trading period of the EU ETS. Emphasis is given to short-term forecasting of prices and volatility. We find that MS-GARCH models distinguish well between two states and that the volatility processes in the states are clearly different. This finding can be explained by the EU ETS design. Our results support the use of MS-GARCH models for risk management, especially because their forecasting ability is better than other Markov switching or simple GARCH models."
"682";682;"2014-051";"Corporate Cash Hoarding in a Model with Liquidity Constraints";"Falk Mazelis";"C7";"2014-09-19";"C63,   E21,   E41,   D81";" This paper studies the role of uncertainty in the corporate cash hoarding puzzle. The baseline model is a stochastic neoclassical growth model featuring idiosyncratic and uninsurable productivity shocks and a cash-in-advance constraint on new in- vestments on the individual rm level. Individual agents' choices regarding cash holdings are analyzed. After a wealth threshold is reached, the cash-in-advance con- straint ceases to have an eect on the agent's behavior. The resulting aggregate cash holdings of households increases with uncertainty. Aggregate consumption is also higher, but the added volatility of consumption decreases lifetime utility. Al- lowing rms to borrow and lend available unused cash increases average variables. An exogenous increase in the interest rate at which they intermediate funds leads to increased intermediation activity, corresponding to the lending channel of monetary policy transmission."
"683";683;"2014-052";"Designing an Index for Assessing Wind Energy Potential";"Matthias Ritter,  Zhiwei Shen,  Brenda López Cabrera,  Martin Odening and   Lars Deckert";"C11";"2014-09-24";"Q42,   Q47";" To meet the increasing global demand for renewable energy such as wind energy, more and more new wind parks are installed worldwide. Finding a suitable location, however, requires a detailed and often costly analysis of the local wind conditions. Plain average wind speed maps cannot provide a precise forecast of wind power because of the non-linear relationship between wind speed and production. In this paper, we suggest a new approach of assessing the local wind energy potential: Meteorological reanalysis data are applied to obtain long-term low-scale wind speed data at turbine location and hub height; then, with actual high-frequency production data, the relation between wind data and energy production is determined via a five parameter logistic function. The resulting wind energy index allows for a turbine-specific estimation of the expected wind power at an unobserved location. A map of wind power potential for whole Germany exemplifies the approach."
"684";684;"2014-053";"Improved volatility estimation based on limit order books";"Markus Bibinger,  Moritz Jirak and   Markus Reiss";"C12";"2014-09-26";"C22,   C58";" For a semi-martingale Xt, which forms a stochastic boundary, a rate-optimal estimator for its quadratic variation hX;Xit is con- structed based on observations in the vicinity of Xt. The problem is embedded in a Poisson point process framework, which reveals an interesting connection to the theory of Brownian excursion ar- eas. A major application is the estimation of the integrated squared volatility of an ecient price process Xt from intra-day order book quotes. We derive n????1=3 as optimal convergence rate of integrated squared volatility estimation in a high-frequency framework with n observations (in mean). This considerably improves upon the classi- cal n????1=4-rate obtained from transaction prices under microstructure noise."
"685";685;"2014-054";"Strategic Complementarities and Nominal Rigidities";"Philipp König and   Alexander Meyer-Gohde";"C7";"2014-10-06";"E31,   C70,   D82";" We reconsider the canonical model of price setting with menu costs by Ball and Romer (1990). Their original model exhibits multiple equilibria for nominal aggregate demand shocks of intermediate size. By abandoning Ball and RomerÂ’s (1990) assumption that demand shocks are common knowledge among price setters, we derive a unique symmetric threshold equilibrium where agents adjust prices whenever the demand shock falls outside the thresholds. The comparative statics of this threshold may differ from the one that gives rise to maximal nominal rigidity examined by Ball and Romer (1990). In contrast to their analysis, we find that a decrease in real rigidities can be associated with an increase in nominal rigidities due to the endogenous adjustment of agentsÂ’ beliefs regarding the aggregate price level."
"686";686;"2014-055";"Estimating the Spot Covariation of Asset Prices – Statistical Theory and Empirical Evidence";"Markus Bibinger,  Markus Reiss,  Nikolaus Hautsch and   Peter Malec";"B8B11C12";"2014-10-08";"C58,   C14,   C32";" We propose a new estimator for the spot covariance matrix of a multi-dimensional continuous semi-martingale log asset price process which is subject to noise and non-synchronous observations. The estimator is constructed based on a local average of block-wise parametric spectral covariance estimates. The latter originate from a local method of moments (LMM) which recently has been introduced by Bibinger et al. (2014). We extend the LMM estimator to allow for autocorrelated noise and propose a method to adaptively infer the autocorrelations from the data. We prove the consistency and asymptotic normality of the proposed spot covariance estimator. Based on extensive simulations we provide empirical guidance on the optimal implementation of the estimator and apply it to high-frequency data of a cross-section of NASDAQ blue chip stocks. Employing the estimator to estimate spot covariances, correlations and betas in normal but also extreme-event periods yields novel insights into intraday covariance and correlation dynamics. We show that intraday (co-)variations (i) follow underlying periodicity patterns, (ii) reveal substantial intraday variability associated with (co-)variation risk, (iii) are strongly serially correlated, and (iv) can increase strongly and nearly instantaneously if new information arrives."
"687";687;"2014-056";"Monetary Policy Effects on Financial Intermediation via the Regulated and the Shadow Banking Systems";"Falk Mazelis";"C7";"2014-10-10";"E32,   E44,   E51,   G20";" We extend the monetary DSGE model by Gertler and Karadi (2011) with a non-bank financial intermediary to investigate the impact of monetary policy shocks on aggregate loan supply. We distinguish between bank and non-bank intermediaries based on the liquidity of their credit claims. While banks can endogenously create deposits to fund rm loans, non-banks have to raise deposits on the funding market to function as intermediaries. The funding market is modeled via search and matching by non-banks for available deposits of households. Because deposit creation responds to economy-wide productivity automatically, bank reaction to shocks corresponds to the balance sheet channel. Non-banks are constrained by the available deposits and their behavior is better explained by the lending channel. The two credit channels are affcted differently following a monetary policy shock. As a result of these counteracting effects, an increasing non-bank sector leads to a reduced reaction of aggregate loan supply following a monetary policy shock, which is consistent with the data. An extension to deposit like-issuance by the non-bank sector will allow further studies of re-regulating the non-bank sector."
"688";688;"2014-057";"A Tale of Two Tails: Preferences of neutral third-parties in three-player ultimatum games";"Ciril Bosch-Rosa";"C10";"2014-10-12";"C92,   D71,   D63,   D31";" We present a three-player game in which a proposer makes a suggestion on how to split $10 with a passive responder. The oer is accepted or rejected depending on the strategy prole of a neutral third-party whose payos are independent from his decisions. If the oer is accepted the split takes place as suggested, if rejected, then both proposer and receiver get $0. Our results show a decision-maker whose main concern is to reduce the inequality between proposer and responder and who, in order to do so, is willing to reject both selsh and generous oers.This pattern of rejections is robust through a series of treatments which include changing the at-fee payo of the decision-maker, introducing a monetary cost for the decision-maker in case the oer ends up in a rejection, or letting a computer replace the proposer to randomly make the splitting suggestion between proposer and responder. Further, through these dierent treatments we are able to show that decision- makers ignore the intentions behind the proposers suggestions, as well as ignoring their own relative payos, two surprising results given the existing literature."
"689";689;"2014-058";"Boiling the frog optimally: nan experiment on survivor curve shapes and internet revenue";"Christina Aperjis,  Ciril Bosch-Rosa,  Daniel Friedman and   Bernardo A. Huberman";"C10";"2014-10-12";"C91,   D40,   L11";" When should a necessary inconvenience be introduced gradually, and when should it be imposed all at once? The question is crucial to web content providers, who in order to generate revenue must sooner or later introduce advertisements, subscription fees, or other inconveniences. Assuming that eventually people fully adapt to changes, the answer depends only on the shape of the survivor curve S(x), which represents the fraction of a user population willing to tolerate inconveniences of size x (Aperjis and Huberman 2011). We report a new laboratory experiment that, for the rst time, estimates the shape of survivor curves in several dierent settings. We engage laboratory subjects in a series of six desirable activities, e.g., playing a video game, viewing a chosen video clip, or earning money by answering questions. For each activity we introduce a chosen level x 2 [xmin; xmax] of a particular inconvenience, and each subject chooses whether to tolerate the inconvenience or to switch to a bland activity for the remaining time. Our key nding is that, in general, the survivor curve is log-convex. Theory suggests therefore that introducing inconveniences all at once will generally be more protable for web content providers."
"690";690;"2014-059";"Expectile Treatment Effects: An efficient alternative to compute the distribution of treatment effects";"Stephan Stahlschmidt,  Matthias Eckardt and   Wolfgang K. Härdle";"B1";"2014-10-16";"C21,   C31,   C54,   J64";" The distribution of treatment eects extends the prevailing focus on average treatment eects to the tails of the outcome variable and quantile treatment eects denote the predominant technique to compute those eects in the presence of a confounding mechanism. The underlying quantile regression is based on a L1{loss function and we propose the technique of expectile treatment eects, which relies on expectile regression with its L2{loss function. It is shown, that apart from the extreme tail ends expectile treatment eects provide more ecient estimates and these theoretical results are broadened by a simulation and subsequent analysis of the classic LaLonde data. Whereas quantile and expectile treatment eects perform comparably on extreme tail locations, the variance of the expectile variant amounts in our simulation on all other locations to less than 80% of its quantile equivalent and under favourable conditions to less than 2=3. In the LaLonde data expectile treatment eects reduce the variance by more than a quarter, while at the same time smoothing the treatment eects considerably."
"691";691;"2014-060";"Are US Inflation Expectations Re-Anchored?";"Dieter Nautz and   Till Strohsal";"C14";"2014-10-20";"E31,   E52,   E58,   C22";" Anchored inflation expectations are of key importance for monetary policy. If long-terminflation expectations arewell-anchored, they should be unaffected by short-termeconomic news. This letter introduces newsregressions with multiple endogenous breaks to investigate the de- and re-anchoring of US inflation expectations. We confirm earlier evidence on the de-anchoring of expectations driven by the outbreak of the crisis. Our results indicate that expectations have not been re-anchored ever since."
"692";692;"2014-061";"The statutory breakdown of payroll taxes between firms and workers and the business";"Simon Voigts";"C7";"2014-10-20";"H55,   H20,   E30,   E60";" According to conventional wisdom, the statutory split of payroll taxes between rms and workers is irrelevant for the real allocation in the long run, as tax incidence is fully determined by the market structure when prices and wages adjust. This paper breaks with this view, by showing that if payroll taxes are levied on workers, business cycle uctuations of prices and wages are smaller than under the formal taxation of rms. Lower nominal volatility mitigates price and wage dispersion, and thereby the proclivity loss from business cycles. In a standard DSGE model cal- ibrated to a typical European country, a full shift of contributions from rms to workers reduces the welfare costs of the business cycle 11.25%."
"693";693;"2014-062";"Do Tax Cuts Increase Consumption? An Experimental Test of Ricardian Equivalence";"Thomas Meissner and   Davud Rostam-Afschar";"C10";"2014-10-23";"D91,   E21,   H24,   C91";" This paper tests whether the Ricardian Equivalence proposition holds in a life cycle consumption laboratory experiment. This proposition is a fundamental assumption underlying numerous studies on intertemporal choice and has important implications for tax policy. Using nonparametric and panel data methods, we nd that the Ricardian Equivalence proposition does not hold in general. Our results suggest that taxation has a signicant and strong impact on consumption choice. Over the life cycle, a tax relief increases consumption on average by about 22% of the tax rebate. A tax increase causes consumption to decrease by about 30% of the tax increase. These results are robust with respect to variations in the diculty to smooth consumption. In our experiment, we nd the behavior of about 62% of our subjects to be inconsistent with the Ricardian proposition. Our results show dynamic eects; taxation in uences consumption beyond the current period."
"694";694;"2014-063";"The Influence of Oil Price Shocks on China’s Macro-economy : A Perspective of International Trade";"Shiyi Chen,  Dengke Chen and   Wolfgang K. Härdle";"B1";"2014-10-29";"F41,   Q43,   Q48";" International trade has been playing an extremely significant role in China over the last 20 years. This paper is aimed at investigating and understanding the relationship between ChinaÂ’smacro-economy and oil price fromthis newperspective. We find strong evidence to suggest that the increase of ChinaÂ’s price level, resulting fromoil price shocks, is statistically less than that of its main trade partnersÂ’. This helps us to understand the confused empirical results estimated within the SVAR framework and sheds light on recent data. More specifically, as for the empirical results, we find ChinaÂ’s output level is positively correlated with the oil price, and oil price shocks slightly appreciate the RMB against the US dollar. Positive correlation between ChinaÂ’s output and oil price shocks presumably results from the drop in ChinaÂ’s relative price induced by oil price shocks, which is inclined to stimulate ChinaÂ’s goods and service exports. The slight appreciation of the RMB could be justified by the drop in ChinaÂ’s relative price, which is indicated by economic theory. Moreover, constructing a simple model, our new perspective also helps us to understand the recent fact that together with the dramatic surge of the world oil price, while the oil imports of the other major countries (especially the largest oil import country US) in the world steadily decline or remain stable, ChinaÂ’s oil imports, in contrast, have kept rising steeply since the year 2004."
"695";695;"2014-064";"Whom are you talking with? An experiment on credibility and communication structure";"Gilles Grandjean,  Marco Mantovani,  Ana Mauleon and   Vincent Vannetelbosch";"C10";"2014-10-29";"C72,   C91,   D03,   D83";" The paper analyzes the role of the structure of communication - i.e. who is talking with whom - on the choice of messages, on their credibility and on actual play. We run an experiment in a three-player coordination game with Pareto ranked equilibria, where a pair of agents has a profitable joint deviation from the Pareto-dominant equilibrium. According to our analysis of credibility, the subjects should communicate and play the Pareto optimal equilibrium only when communication is public. When pairs of agents exchange messages privately, the players should play the Pareto dominated equilibrium and disregard communication. The experimental data conform to our predictions: the agents reach the Pareto-dominant equilibrium only when announcing to play it is credible. When private communication is allowed, lying is prevalent, and players converge to the Pareto-dominated equilibrium. Nevertheless, at the individual level, playersÂ’ beliefs and choices tend to react to messages even when these are non-credible."
"696";696;"2014-065";"A Theory of Price Adjustment under Loss Aversion";"Steffen Ahrens,  Inske Pirschel and   Dennis J. Snower";"C10";"2014-11-03";"D03,   D21,   E31,   E50";" We present a new partial equilibrium theory of price adjustment, based on consumer loss aversion. In line with prospect theory, the consumersÂ’ perceived utility losses from price increases are weighted more heavily than the perceived utility gains from price decreases of equal magnitude. Price changes are evaluated relative to an endogenous reference price, which depends on the consumersÂ’ rational price expectations from the recent past. By implication, demand responses are more elastic for price increases than for price decreases and thus firms face a downward-sloping demand curve that is kinked at the consumersÂ’ reference price. Firms adjust their prices flexibly in response to variations in this demand curve, in the context of an otherwise standard dynamic neoclassical model of monopolistic competition. The resulting theory of price adjustment is starkly at variance with past theories. We find that - in line with the empirical evidence - prices are more sluggish upwards than downwards in response to temporary demand shocks, while they are more sluggish downwards than upwards in response to permanent demand shocks."
"697";697;"2014-066";"TENET: Tail-Event driven NETwork risk";"Wolfgang Karl Härdle,  Weining Wang and   Lining Yu";"B1";"2014-11-03";"G01,  G18,  G32,  G38,   C21, ";" We propose a semiparametric measure to estimate systemic interconnectedness across financial institutions based on tail-driven spill-over effects in a ultra-high dimensional framework. Methodologically, we employ a variable selection technique in a time series setting in the context of a single-index model for a generalized quantile regression framework. We can thus include more financial institutions into the analysis, to measure their interdependencies in tails and, at the same time, to take into account non-linear relationships between them. A empirical application on a set of 200 publicly traded U. S. nancial institutions provides useful rankings of systemic exposure and systemic contribution at various stages of financial crisis. Network analysis, its behaviour and dynamics, allows us to characterize a role of each sector in the financial crisis and yields a new perspective of the nancial markets at the U. S. financial market 2007 - 2012."
"698";698;"2014-067";"Bootstrap confidence sets under model misspecification";"Vladimir Spokoiny and   Mayya Zhilova";"B5";"2014-11-17";"C13,   C15";" A multiplier bootstrap procedure for construction of likelihood-based condence sets is considered for nite samples and a possible model misspecication. Theoretical results justify the bootstrap consistency for a small or moderate sample size and allow to control the impact of the parameter dimension p: the bootstrap approximation works if p3=n is small. The main result about bootstrap consistency continues to apply even if the underlying parametric model is misspecied under the so called Small Modeling Bias condition. In the case when the true model deviates signicantly from the considered parametric family, the bootstrap procedure is still applicable but it becomes a bit conservative: the size of the constructed condence sets is increased by the modeling bias. We illustrate the results with numerical examples for misspecied constant and logistic regressions."
"699";699;"2014-068";"Estimation and Determinants of Chinese Banks’ Total Factor Efficiency: A New Vision Based on Unbalanced Development of Chinese Banks and Their Overall Risk.";"Shiyi Chen,  Wolfgang K. Härdle and   Li Wang";"B1";"2014-11-20";"C14,   C33,   G21";" The development of shadow banking system in China catalyzes the expansion of banksÂ’ off-balance-sheet activities, resulting in a distortion of ChinaÂ’s traditional credit expansion and underestimation of its commercial banksÂ’ overall risk. This paper is the first to incorporate banksÂ’ overall risk, endogenously into bankÂ’s production process as undesirable by-product for the estimation of banksÂ’ total factor efficiency (TFE) as well as TFE of each production factor. A unique data sample of 171 Chinese commercial banks, which is the largest data sample concerning with Chinese banking efficiency issues until now as far as we know, making our results more convincing and meaningful. Our results show that, compared with a model incorporated with banksÂ’ overall risk, a model considering on-balance-sheet lending activities only may over-estimate the overall average TFE and under-estimate TFE volatility as a whole. Higher overall risk taking of banks tends to decrease bank TFE through Â‘diverting effectÂ’. However, significant heterogeneities of bank integrated TFE (TFIE) and TFE of each production factor exist among banks of different types or located in different regions, as a result of still prominent unbalanced development of Chinese commercial banks today. Based on newly estimated TFIE, the paper also investigates the determinants of bank efficiency, and finds that a model with risk-weighted assets as undesirable outputs can better capture the impact of shadow banking involvement."
"700";700;"2014-069";"When the Taylor principle is insufficient - A benchmark for the fiscal theory of the price level in a monetary union";"Maren Brede";"C7";"2014-11-20";"E31,   E52,   E62,   E63";" This paper derives restrictions on monetary and scal policies for determinate equilibria in a two-country monetary union with autarkic members. It nds that a central bank following the Taylor principle may not be sucient for determinacy unless accompanied by one 'active' scal authority in the sense of Leeper (1991). Alternatively, both scal authorities can be 'active' while the central bank abandons the Taylor principle. The two determinate equilibria have signicantly dierent implications for the transmission of scal and monetary shocks and for the scal theory of the price level in a monetary union."
"701";701;"2014-070";"The Politics of Related Lending";"Michael Halling,  Pegaret Pichler and   Alex Stomper";"A15";"2014-11-24";"G21,   G38,   L32";" We analyze the profitability of government-owned banksÂ’ lending to their owners, using a unique data set of relatively homogeneous government-owned banks; the banks are all owned by similarly structured local governments in a single country. Making use of a natural experiment that altered the regulatory and competitive environment, we find evidence that such lending was used to transfer revenues from the banks to the governments. Some of the evidence is particularly pronounced in localities where the incumbent politicians face significant competition for reelection."
"702";702;"2015-001";"Pricing Kernel Modeling";"Denis Belomestny,  Shujie Ma and   Wolfgang Karl Härdle";"B1";"2015-01-08";"C00,   C14,   G12";" We propose a new method to estimate the empirical pricing kernel based on option data. We estimate the pricing kernel nonparametrically by using the ratio of the risk-neutral density estimator and the subjective density estimator. The risk-neutral density is approximated by a weighted kernel density estimator with varying unknown weights for dierent observations, and the subjective density is approximated by a kernel density estimator with equal weights. We represent the European call option price function by the second order integration of the risk-neutral density, so that the unknown weights are obtained through one-step penalized least squares estimation with the Kullback-Leibler divergence as the penalty function. Asymptotic results of the resulting estimators are established. The performance of the proposed method is illustrated empirically by simulation and real data application studies."
"703";703;"2015-002";"Estimating the Value of Urban Green Space: A hedonic Pricing Analysis of the Housing Market in Cologne, Germany";"Jens Kolbe and   Henry Wüstemann";"B3";"2015-01-08";"R31,   C14,   Q50";" Urban Green Space (UGS) such as parks and forests provide a wide range of environmental and recreational benefits. One objective in the conservation efforts of UGS is to analyse the benefits associated with UGS in order to make them more visible and to provide support for landscape planning. This paper examines the effects of UGS on house prices applying a Hedonic Pricing Method (HPM). The data set contains over 85,046 geo-coded apartment transactions for the years 1995- 2012 and contains information on three intrinsic variables of the real estate (e.g. transaction price, floor area and age). In order to examine the capitalization of UGS in real estate prices we further incorporate crosssection geo-coded data for the different types of UGS: forests, parks, farmland and fallow land drawn from the European Urban Atlas (EUA) of the European Environment Agency for the year 2006. In order to control for additional open space categories we further incorporated geo-coded data on water bodies and fallow land. Using a Geographical Information System (GIS) we calculated the coverage of UGS in pre-defined buffers around households as well as the distance in a continuous fashion (Euclidian distance) between UGS and the households. Our results show a capitalization of UGS in real estate prices but the effect of the structural variables is higher. We found a positive price effect of parks, forests and water and an inverse relation between the price variable and the presence of fallow land and farmland."
"704";704;"2015-003";"Identifying Berlin's land value map using Adaptive Weights Smoothing";"Jens Kolbe,  Rainer Schulz,  Martin Wersing,  and   Axel Werwatz";"B3";"2015-01-08";"C14,   R14,   R15";" We use Adaptive Weights Smoothing (AWS) of Polzehl and Spokoiny (2000, 2003, 2006) to estimate a map of land values for Berlin, Germany. Our data are prices of undeveloped land that was transacted between 1996-2009. Even though the observed land price is an indicator of the respective land value, it is in uenced by transaction noise. The iterative AWS applies piecewise constant regression to reduce this noise and tests at each location for constancy at the margin. If not rejected, further observations are included in the local regression. The estimated land value map conforms overall well with expert-based land values. Our application suggests that the transparent AWS could prove a useful tool for researchers and real estate practitioners alike."
"705";705;"2015-004";"Efficiency of Wind Power Production and its Determinants";"Simone Pieralli,  Matthias Ritter and   Martin Odening";"C11";"2015-01-15";"D20,   D21,   Q42";" This article examines the efficiency of wind energy production. We quantify production losses in four wind parks across Germany for 19 wind turbines with non-convex efficiency analysis. In a second stage regression, we adapt the linear regression results of Kneip, Simar, Wilson (2014) to explain electricity losses by means of a bias-corrected truncated regression. Our results show that electricity losses amount to 27% of the maximal producible electricity. These losses can be mainly traced back to changing wind conditions while only 6 % are caused by turbine errors."
"706";706;"2015-005";"Distillation of News Flow into Analysis of Stock Reactions";"Junni L. Zhang,  Wolfgang K. Härdle,  Cathy Y. Chen and   Elisabeth Bommes";"B1";"2015-01-30";"C81,   G14,   G17";" News carry information of market moves. The gargantuan plethora of opinions, facts and tweets on nancial business oers the opportunity to test and analyze the influence of such text sources on future directions of stocks. It also creates though the necessity to distill via statistical technology the informative elements of this prodigious and indeed colossal data source. Using mixed text sources from professional platforms, blog fora and stock message boards we distill via dierent lexica sentiment variables. These are employed for an analysis of stock reactions: volatility, volume and returns. An increased (negative) sentiment will in uence volatility as well as volume. This influuence is contingent on the lexical projection and dierent across GICS sectors. Based on review articles on 100 S&P 500 constituents for the period of October 20, 2009 to October 13, 2014 we project into BL, MPQA, LM lexica and use the distilled sentiment variables to forecast individual stock indicators in a panel context. Exploiting dierent lexical projections, and using dierent stock reaction indicators we aim at answering the following research questions: (i) Are the lexica consistent in their analytic ability to produce stock reaction indicators, including volatility, detrended log trading volume and return? (ii) To which degree is there an asymmetric response given the sentiment scales (positive v.s. negative)? (iii) Are the news of high attention rms diusing faster and result in more timely and ecient stock reaction? (iv) Is there a sector specic reaction from the distilled sentiment measures? We nd there is signicant incremental information in the distilled news  ow. The three lexica though are not consistent in their analytic ability. Based on condence bands an asymmetric, attention-specic and sector-specic response of stock reactions is diagnosed."
"707";707;"2015-006";"Cognitive Bubbles";"Ciril Bosch-Rosay,  Thomas Meissnerz and   Antoni Bosch-Domènech";"C10";"2015-02-05";"C91,   D12,   D84,   G11";" Smith et al. (1988) reported large bubbles and crashes in experimental asset markets, a result that has been replicated by a large literature. Here we test whether the occurrence of bubbles depends on the experimental subjects' cognitive sophistication. In a two-part experiment, we rst run a battery of tests to assess the subjects' cognitive sophistication and classify them into low or high levels of cognitive sophistication. We then invite them separately to two asset market experiments populated only by subjects with either low or high cognitive sophistication. We observe classic bubble- crash patterns in the sessions populated by subjects with low levels of cognitive sophistication. Yet, no bubbles or crashes are observed with our sophisticated subjects. This result lends strong support to the view that the usual bubbles and crashes in experimental asset markets are caused by subjects' confusion and,"
"708";708;"2015-007";"Stochastic Population Analysis: A Functional Data Approach";"Lei Fang and   Wolfgang K. Härdle";"B1";"2015-02-12";"C14,   C32,   C38,   J11,   J1";" Based on the Lee-Carter (LC) model, the benchmark in population forecasting, a variety of extensions and modifications are proposed in this paper. We investigate one of the extensions, the Hyndman-Ullah (HU) method and apply it to Asian demographic data sets: China, Japan and Taiwan. It combines ideas of functional principal component analysis (fPCA), nonparametric smoothing and time series analysis. Based on this stochastic approach, the demographic characteristics and trends in different Asian regions are calculated and compared. We illustrate that China and Japan exhibited a similar demographic trend in the past decade. We also compared the HU method with the LC model. The HU method can explain more variation of the demographic dynamics when we have data of high quality, however, it also encounters problems and performs similarly as the LC model when we deal with limited and scarce data sets, such as Chinese data sets due to the substandard quality of the data and the population policy."
"709";709;"2015-008";"Nonparametric change-point analysis of volatility";"Markus Bibinger, Moritz Jirak and  Mathias Vetter";"C12";"2015-02-12";"C12,   C14";" This work develops change-point methods for statistics of high-frequency data. The main interest is the volatility of an ItÂˆo semi-martingale, which is discretely observed over a fixed time horizon. We construct a minimax-optimal test to discriminate different smoothness classes of the underlying stochastic volatility process. In a high-frequency framework we prove weak convergence of the test statistic under the hypothesis to an extreme value distribution. As a key example, under extremely mild smoothness assumptions on the stochastic volatility we thereby derive a consistent test for volatility jumps. A simulation study demonstrates the practical value in finite-sample applications."
"710";710;"2015-009";"From Galloping Inflation to Price Stability in Steps: Israel 1985–2013";"Rafi Melnick and  Till Strohsal";"C14";"2015-02-12";"E31,   E52,   E58,   C22";" After the introduction of a stabilization program Israeli inflation decreased from 400% in 1985 to 2% in 2013. This paper analyzes how the remarkable transition process of IsraelÂ’s disinflation took place. We reinforce the existing hypothesis that inflationmoved in distinct steps characterized by constant levels with short-lived fluctuations around them. Multiple endogenous breakpoint tests provide strong empirical evidence in favor of our claim. We find that the disinflation process is defined by three clear steps of high, medium and low inflation. The break dates are in line with major economic events that constitute the end and the beginning of each disinflation step."
"711";711;"2015-010";"Estimation of NAIRU with In ation Expectation Data";"Wei Cui, Wolfgang K. Härdle and  Weining Wang";"B1";"2015-02-12";"C32; E23; E24";" Estimating natural rate of unemployment (NAIRU) is important for understanding the joint dynamics of unemployment, inflation, and inflation expectation. However, existing literature falls short in endogenizing inflation expectation together with NAIRU in a model consistent way. We develop and estimate a structural model with forward and backward looking Phillips curve. Inflation expectation is treated as a function of state variables and we use survey data as its observations. We find out that the estimated NAIRU using our methodology tracks the unemployment process closely except for the high inflation period around 1970. Moreover, the estimated Bayesian credible sets are narrower and our model leads to better inflation and unemployment forecasts. These results suggest that monetary policy was very effective during the sample periods and there was not much room for policy improvement."
"712";712;"2015-011";"Competitors In Merger Control: Shall They Be Merely Heard Or Also Listened To?";"Thomas Giebe and   Miyu Lee";"A6";"2015-02-27";"G34,   K21,   L4,   C73,   L2";" There are legal grounds to hear competitors in merger control proceedings, and competitor involvement has gained significance. To what extent this is economically sensible is our question. The competition authority applies some welfare standard while the competitor cares about its own profit. In general, but not always, this implies a conflict of interest. We formally model this setting with cheap talk signaling games, where hearing the competitor might convey valuable information to the authority, but also serve the competitorÂ’s own interests. We find that the authority will mostly have to ignore the competitor but, depending on the authorityÂ’s own prior information, strictly following the competitorÂ’s selfish recommendation will improve the authorityÂ’s decision. Complementary to our analysis, we provide empirical data of competitor involvement in EU merger cases and give an overview of the legal discussion in the EU and US."
"713";713;"2015-012";"The Impact of Credit Default Swap Trading on Loan Syndication";"Daniel Streitz";"A13";"2015-03-09";"G21,   G32";" We analyze the impact of CDS trading on bank syndication activity. Theoretically, the effect of CDS trading is ambiguous: on the one hand, CDS can improve risksharing and hence be a more flexible risk management tool than loan syndication; on the other hand, CDS trading can reduce bank monitoring incentives. We document that banks are less likely to syndicate loans and retain a larger loan fraction once CDS are actively traded on the borrowerÂ’s debt. We then discern the risk management and the moral hazard channel. We find no evidence that the reduced likelihood to syndicate loans is a result of increased moral hazard problems."
"714";714;"2015-013";"Pitfalls and Perils of Financial Innovation: The Use of CDS by Corporate Bond Funds";"Tim Adam and   Andre Guettler";"A13";"2015-03-12";"G11,   G15,   G23";" We use the financial crisis of 2007Â–2009 as a laboratory to examine the costs and benefits of teams versus single managers in asset management. We find that when a fund uses complex trading strategies involving the use of CDS team-managed funds outperform solo-managed funds. This may be due to the greater diversity of expertise, experience and skill of teams relative to single managers. During the financial crisis, however, the performance premium of teams becomes negative, which may be because of the slower decision times of teams, which are especially costly during times of rapidly changing market conditions."
"715";715;"2015-014";"Generalized Exogenous Processes in DSGE: A Bayesian Approach";"Alexander Meyer-Gohde and   Daniel Neuhoff";"C7";"2015-03-23";"C11,   C32,   C51,   C52";" The Reversible Jump Markov Chain Monte Carlo (RJMCMC) method can enhance Bayesian DSGE estimation by sampling from a posterior distribution spanning potentially nonnested models with parameter spaces of different dimensionality. We use the method to jointly sample from an ARMA process of unknown order along with the associated parameters. We apply the method to the technology process in a canonical neoclassical growth model using post war US GDP data and find that the posterior decisively rejects the standard AR(1) assumption in favor of higher order processes. While the posterior contains significant uncertainty regarding the exact order, it concentrates posterior density on hump-shaped impulse responses. A negative response of hours to a positive technology shock is within the posterior credible set when noninvertible MA representations are admitted."
"716";716;"2015-015";"Structural Vector Autoregressions with Heteroskedasticity: A Comparison of Different Volatility Models";"Helmut Lütkepohl and   Aleksei Netšunajev";"C15";"2015-03-23";"C32";" A growing literature uses changes in residual volatility for identifying structural shocks in vector autoregressive (VAR) analysis. A number of dierent models for heteroskedasticity or conditional heteroskedasticity are proposed and used in applications in this context. This study reviews the dierent volatility models and points out their advantages and drawbacks. It thereby enables researchers wishing to use identication of structural VAR models via heteroskedasticity to make a more informed choice of a suitable model for a specic empirical analysis. An application investigating the interaction between U.S. monetary policy and the stock market is used to illustrate the related issues."
"717";717;"2015-016";"Testing Missing at Random using Instrumental Variables";"Christoph Breunig";"B1";"2015-03-30";"C12,   C14";" This paper proposes a test for missing at random (MAR). The MAR assumption is shown to be testable given instrumental variables which are independent of response given potential outcomes. A nonparametric testing procedure based on integrated squared distance is proposed. The statisticÂ’s asymptotic distribu- tion under the MAR hypothesis is derived. We demonstrate that our results can be easily extended to a test of missing completely at random (MCAR) and miss- ing completely at random conditional on covariates X (MCAR(X)). A Monte Carlo study examines finite sample performance of our test statistic. An empirical illustration concerns pocket prescription drug spending with missing values; we reject MCAR but fail to reject MAR."
"718";718;"2015-017";"Loss Potential and Disclosures Related to Credit Derivatives – A Cross-Country Comparison of Corporate Bond Funds under U.S. and German Regulation";"Dominika Paula Galkiewicz";"A13";"2015-03-30";"G11,   G15,   G23,   G28";" This study analyzes the loss potential arising from investments into CDS for a sample of large U.S. and German mutual funds. Further, it investigates whether the comments funds make on CDS use in periodic fund reports are consistent with the disclosed CDS holdings. For several funds in the U.S., the potential losses arising from selling CDS protection are almost as high as net assets, while in Germany, this potential can be even higher. Regarding the information funds provide to investors about their use of CDS, the results of the study suggest that comments on CDS contained in periodic reports are often unspecific and sometimes misleading. Thus, investors might have to analyze portfolio holdings in order to learn about the true investment behavior of funds. For instance, in Germany, funds that use more short than long CDS often state that they only use long CDS for hedging purposes. Based on the results, it seems advisable that regulators in both countries tighten rules restricting the speculative use of derivatives by funds to a reasonable level, as well as implement more standardized disclosure policies."
"719";719;"2015-018";"Manager Characteristics and Credit Derivative Use by U.S. Corporate Bond Funds";"Dominika Paula Galkiewicz";"A13";"2015-03-30";"G23,   G28";" This study provides a comprehensive overview of the use of credit default swaps by U.S. corporate bond funds and analyzes in detail whether certain characteristics of managers, in addition to the fundamentals of a fund, determine how their use these credit derivatives. Results suggest that a managerÂ’s education, age, experience, and skill are positively correlated with a fundÂ’s CDS holdings. In particular, managers holding a masterÂ’s degree or educated at prestigious universities prefer using CDS. However, funds with older, more experienced managers or these keeping higher assets under their management are more likely to take on credit risk via selling CDS protection. Younger managers or managers that were educated at prestigious universities rather tend to buy CDS protection possibly due to differing concerns about their careers. If considering the Heckman correction for self-selection of funds into CDS use, the aforementioned findings remain stable."
"720";720;"2015-019";"Measuring Connectedness of Euro Area Sovereign Risk";"Rebekka Gatjen and   Melanie Schienle";"B1";"2015-04-07";"C32,   C58,   F34,   G01,   G1";" We introduce a methodology for measuring default risk connectedness that is based on an out-of-sample variance decomposition of model forecast errors. The out-of-sample nature of the procedure leads to \realized"" measures which, in practice, respond more quickly to crisis occurrences than those based on in-sample methods. The resulting relative and absolute connectedness measures nd distinct and complementary information from CDS and bond yield data on European area sovereign risk. The detection and use of these second moment dierences of CDS and bond data is new to the literature and allows to identify countries that impose risk on the system from those which sustain risk."
"721";721;"2015-020";"Is There an Asymmetric Impact of Housing on Output?";"Tsung-Hsien Michael Lee and   Wenjuan Chen";"C14";"2015-04-13";"C32,   C58,   F34,   G01,   G1";" Numerous papers have tried to understand housingÂ’s role in the economy and have not reached an agreement. In this paper we turn to the asymmetric relationship between housing and the overall economic activity. We find that the relation between building permits and GDP is regime-dependent. Causality analysis suggests that the housing variable leads output only in the regime associated with periods when the housing and business cycles are experiencing contractions. Our findings not only echo the argument that housing leads the business cycles, but also show that it has time-varying effect on the overall economic activity."
"722";722;"2015-021";"Characterizing the Financial Cycle: Evidence from a Frequency Domain Analysis";"Till Strohsal,  Christian R. Proaño and   Jürgen Wolters";"C14";"2015-04-14";"C22,   E32,   E44";" A growing body of literature argues that the financial cycle is considerably longer in duration and larger in amplitude than the business cycle and that its distinguishing features became more pronounced over time. This paper proposes an empirical approach suitable to test these hypothe- ses. We parametrically estimate the whole spectrum of financial and real variables to obtain a complete picture of their cyclical properties. We provide strong statistical evidence for the US and slightly weaker evidence for the UK validating the hypothesized features of the financial cycle. In Germany, however, the financial cycle is, if at all, much less visible."
"723";723;"2015-022";"Risk Related Brain Regions Detected with 3D Image FPCA";"Ying Chen; Wolfgang K. Härdle; Qiang He; Piotr Majer";"B1";"2015-04-17";"C3,   C6,   C9,   C14,   D8";" Abstract: Risk attitude and perception is reflected in brain reactions during RPID experiments. Given the fMRI data, an important research question is how to detect risk related regions and to investigate the relation between risk preferences and brain activity. Conventional methods are often insensitive to or misrepresent the original spatial patterns and interdependence of the fMRI data. In order to cope with this fact we propose a 3D Image Functional Principal Component Analysis (3D Image FPCA) method that directly converts the brain signals to fundamental spatial common factors and subject-specific temporal factor loadings via proper orthogonal decomposition. Simulation study and real data analysis show that the 3D Image FPCA method improves the quality of spatial representations and guarantees the contiguity of risk related regions. The selected regions provide signature scores and carry explanatory power for subjects' risk attitudes. For in-sample analysis, the 3D Image method perfectly classifies both strongly and weakly risk averse subjects. In out-of-sample, it achieves 73-88% overall accuracy, with 90-100% rate for strongly risk averse subjects, and 49-71% for weakly risk averse subjects."
"724";724;"2015-023";"An Adaptive Approach to Forecasting Three Key Macroeconomic Variables for Transitional China";"Linlin Niu,  Xiu Xu and   Ying Chen";"B1";"2015-04-22";"E43,   E47";" Abstract We propose the use of a local autoregressive (LAR) model for adaptive estimation and forecasting of three of ChinaÂ’s key macroeconomic variables: GDP growth, inflation and the 7-day interbank lending rate. The approach takes into account possible structural changes in the data-generating process to select a local homogeneous interval for model estimation, and is particularly well-suited to a transition economy experiencing ongoing shifts in policy and structural adjustment. Our results indicate that the proposed method outperforms alternative models and forecast methods, especially for forecast horizons of 3 to 12 months. Our 1-quarter ahead adaptive forecasts even match the performance of the well-known CMRC Langrun survey forecast. The selected homogeneous intervals indicate gradual changes in growth of industrial production driven by constant evolution of the real economy in China, as well as abrupt changes in interestrate and inflation dynamics that capture monetary policy shifts."
"725";725;"2015-024";"How Do Financial Cycles Interact? Evidence from the US and the UK";"Till Strohsal,  Christian R. Proaño and   Jürgen Wolters";"C14";"2015-04-24";"C22,   E32,   E44";" Abstract Are financial cycles an international phenomenon, and, if so, how do financial cycles interact? This letter provides new evidence for the US and the UK. Considering the properties of the data in both the time and the frequency domains, we find a strong relation between the financial cycles of the US and the UK. US financial cycles have a significant impact on the UK, but not the other way around. The relation is clearly most pronounced for cycles between 8 and 30 years, which is also the frequency range that explains almost all variation of the data."
"726";726;"2015-025";"Employment Polarization and Immigrant Employment Opportunities";"Hanna Wielandt";"A9";"2015-04-24";"J24,   J31,   J62,   O33,   R2";" Abstract: Building on the task-based approach of technological change, this paper discusses the interaction between occupational polarization (e.g. a gradual increase of native employment in the lowest and highest-paying jobs) and employment opportunities of immigrant workers. Using high quality administrative data for Germany, I first show that technological change is positively related to employment growth of natives in low-paying occupations that are also typically held by immigrant workers. In a second step, I show that labor markets in which native employment in those low-paying occupations grew more also experienced a larger decline in immigrant employment rates. The findings are consistent with the idea that the reallocation of natives towards low paying occupations induces stronger competition in the low-skill labor market, a segment in which foreign workers are typically employed. The results suggest that this relationship is more relevant for recent immigrants who have been in Germany for less than 5 years, and that approximately one third of the decline in employment rates could be associated with occupational polarization of native employment."
"727";727;"2015-026";"Forecasting volatility of wind power production";"Zhiwei Shen and   Matthias Ritter";"C11";"2015-05-18";"C22,   Q42,   Q47";" Abstract:  The increasing share of wind energy in the portfolio of energy sources highlights its uncertainties due to changing weather conditions. To account for the uncertainty in predicting wind power production, this article examines the volatility forecasting abilities of different GARCH-type models for wind power production. Moreover, due to characteristic features of the wind power process, such as heteroscedasticity and nonlinearity, we also investigate the use of a Markov regime-switching GARCH (MRS-GARCH) model on forecasting volatility of wind power. The realized volatility, which is derived from lower-scale data, serves as a benchmark for the latent volatility. We find that the MRS-GARCH model significantly outperforms traditional GARCH models in predicting the volatility of wind power, while the exponential GARCH model is superior among traditional GARCH models. "
"728";728;"2015-027";"The Information Content of Monetary Statistics for the Great Recession: Evidence from Germany";"Wenjuan Chen and   Dieter Nautz";"C14";"2015-05-21";"E27,   E32,   E51,   C43";" Abstract:  This paper introduces a Divisia monetary aggregate for Germany and explores its information content for the Great Recession. Divisia money and the corresponding simple sum aggregate are highly correlated in normal times but begin to diverge before the crisis. Out of sample forecast analysis and a conditional forecast exercise show that the predictive content of this divergence for the Great Recession is not only statistically significant, but also economically important.   "
"729";729;"2015-028";"The Time-Varying Degree of Inflation Expectations Anchoring";"Till Strohsal,  Rafi Melnick and   Dieter Nautz";"C14";"2015-05-27";"E31,   E52,   E58,   C22";" Abstract:  Well-anchored inflation expectations have become a key indicator for the credibility of a central bankÂ’s inflation target. Since the outbreak of the recent financial crisis, the existence and the degree of de-anchoring of U.S. inflation expectations have been under debate. This paper introduces an encompassing time-varying parameter model to analyze the changing degree of U.S. inflation expectations anchoring. We confirm that inflation expectations have been partially de-anchored during the financial crisis. Yet, our results suggest that inflation expectations have been successfully re-anchored ever since.   "
"730";730;"2015-029";"Change point and trend analyses of annual expectile curves of tropical storms";"P. Burdejova, 	W. K. Härdle,  	P. Kokoszka and   Q. Xiong";"B1";"2015-05-27";"C12,   C15,   C32,   Q54";" Abstract:  Motivated by the conjectured existence of trends in the intensity of tropical storms, this paper proposes new inferential methodology to detect a trend in the annual pattern of environmental data. The new methodology can be applied to data which can be represented as annual curves which evolve from year to year. Other examples include annual temperature or logÂ–precipitation curves at specific locations. Within a framework of a functional regression model, we derive two tests of significance of the slope function, which can be viewed as the slope coefficient in the regression of the annual curves on year. One of the tests relies on a Monte Carlo distribution to compute the critical values, the other is pivotal with the chiÂ– square limit distribution. Full asymptotic justification of both tests is provided. Their finite sample properties are investigated by a simulation study. Applied to tropical storm data, these tests show that there is a significant trend in the shape of the annual pattern of upper wind speed levels of hurricanes.   "
"731";731;"2015-030";"Testing for Identification in SVAR-GARCH Models";"Helmut Luetkepohl and   George Milunovich";"C15";"2015-06-10";"C32";" Abstract Changes in residual volatility in vector autoregressive (VAR) models can be used for identifying structural shocks in a structural VAR analysis. Testable conditions are given for full identification for the case where the volatility changes can be modelled by a multivariate GARCH process. Formal statistical tests are presented for identification and their small sample properties are investigated via a Monte Carlo study. The tests are applied to investigate the validity of the identification conditions in a study of the effects of U.S. monetary policy on exchange rates. It is found that the data do not support full identification  in most of the models considered, and the implied problems for the interpretation  of the results are discussed.    "
"732";732;"2015-031";"Simultaneous likelihood-based bootstrap confidence sets for a large number of models";"Mayya Zhilova";"B5";"2015-06-19";"C13,   C15";" AbstractThe paper studies a problem of constructing simultaneous likelihood-based confidence sets. We consider a simultaneous multiplier bootstrap procedure for estimating the quantiles of the joint distribution of the likelihood ratio statistics, and for adjusting the confidence level for multiplicity. Theoretical results state the bootstrap validity in the following setting: the sample size n is fixed, the maximal parameter dimension p_max and the number of considered parametric models K are s.t. (log?K )^12 p_max^3/n is small. We also consider the situation when the parametric models are misspecified. If the models' misspecification is significant, then the bootstrap critical values exceed the true ones and the simultaneous bootstrap confidence set becomes conservative. Numerical experiments for local constant and local quadratic regressions illustrate the theoretical results.   "
"733";733;"2015-032";"Government Bond Liquidity and Sovereign-Bank Interlinkages";"Sören Radde,  Cristina Checherita-Westphal and   Wei Cui";"C10";"2015-07-08";"G12,   E41,   E44,   E63";" Banks in the euro area typically hold a large amount of government debt in their bond portfolios, which are valued both for their low credit risk and high liquidity. During the sovereign debt crisis, these characteristics of government debt were severely impaired in stressed euro area countries. In order to understand the transmission channels of stress from government debt markets to the real economy, we augment a standard dynamic macroeconomic model with a banking sector and a market for government debt characterized by search frictions. A sovereign solvency shock modelled as a haircut on government bonds is introduced to study the interaction of sovereign credit and liquidity risk. As banks react to this shock by rebalancing towards highly liquid short-run assets, such as central bank deposits, demand for government bonds collapses, which endogenously worsens their market liquidity. Thus, a sovereign liquidity risk channel from government bond markets to the real sector emerges. Endogenous government bond liquidity negatively affects the funding conditions of the fiscal sector, tightens financing constraints in the banking sector and lowers investment and output. The model is able to match a number of stylised facts regarding the behaviour of sovereign debt markets during the euro area sovereign debt crisis, such as depressed turnover rates and rising bid-ask spreads.   "
"734";734;"2015-033";"Not Working at Work: Loafing, Unemployment and Labor Productivity";"Michael C. Burda,  Katie Genadek and   Daniel S. Hamermesh";"C7";"2015-07-08";"J22,   E24";" Using the American Time Use Survey (ATUS) 2003-12, we estimate time spent by workers in non-work while on the job. Non-work time is substantial and varies positively with the local unemployment rate. While the average time spent by workers in non-work conditional on any positive non-work rises with the unemployment rate, the fraction of workers who report time in non-work varies pro-cyclically, declining in recessions. These results are consistent with a model in which heterogeneous workers are paid efficiency wages to refrain from loafing on the job. That model correctly predicts relationships of the incidence and conditional amounts of non-work with wage rates and measures of unemployment benefits in state data linked to the ATUS, and it is consistent with observed occupational differences in non-work.   "
"735";735;"2015-034";"Factorisable Sparse Tail Event Curves";"Shih-Kang Chao,  Wolfgang K. Härdle and   Ming Yuan";"B1";"2015-07-20";"C38,   C55,   C63,   G17,   G2";" In this paper, we propose a multivariate quantile regression method which enables localized analysis on conditional quantiles and global comovement analysis on conditional ranges for high-dimensional data. The proposed method, hereafter referred to as FActorisable Sparse Tail Event Curves, or FASTEC for short, exploits the potential factor structure of multivariate conditional quantiles through nuclear norm regularization and is particularly suitable for dealing with extreme quantiles. We study both theoretical properties and computational aspects of the estimating procedure for FASTEC. In particular, we derive nonasymptotic oracle bounds for the estimation error, and develope an ecient proximal gradient algorithm for the non-smooth optimization problem incurred in our estimating procedure. Merits of the proposed methodology are further demonstrated through applications to Conditional Autoregressive Value-at-Risk (CAViaR) (Engle and Manganelli; 2004), and a Chinese temperature dataset.  "
"736";736;"2015-035";"Price discovery in the markets for credit risk: A Markov switching ap-proach";"Thomas Dimpfl and   Franziska J. Peter";"B1";"2015-07-20";"C14,   G15";" We examine price discovery in the Credit Default Swap and cor- porate bond market. By using a Markov switching framework we are able to analyze the dynamic behavior of the information shares dur- ing tranquil and crisis periods. The results show that price discovery takes place mostly on the CDS market. The importance of the CDS market even increases during the more volatile crisis periods. Accord- ing to a cross sectional analysis liquidity is the main determinant of a market's contribution to price discovery. During the crisis period, however, we also nd a positive link between leverage and CDS market information shares. Overall the results indicate that price discovery measures and their determinants change during tranquil and crisis pe- riods, which emphasizes the importance of more  exible frameworks, such as Markov switching models.  "
"737";737;"2015-036";"Crowdfunding, demand uncertainty, and moral hazard - a mechanism design approach";"Roland Strausz";"A8";"2015-07-28";"D82,   G32,   L11,   M31";" Crowdfunding challenges the traditional separation between nance and marketing. It creates economic value by reducing demand uncertainty, which enables a better screening of positive NPV projects. Entrepreneurial moral hazard threatens this eect. Using mechanism design, mechanisms are characterized that induce ecient screening, while preventing moral hazard. \All-ornothing"" reward-crowdfunding platforms re ect salient features of these mechanisms. Eciency is sustainable only if expected gross returns exceed twice expected investment costs. Constrained ecient mechanisms exhibit underinvestment. With limited consumer reach, crowdfunders become actual investors. Crowdfunding complements rather than substitutes traditional entrepreneurial nancing, because each nancing mode displays a dierent strength.  "
"738";738;"2015-037";"""Buy-It-Now"" or ""Sell-It-Now"" auctions : Effects of changing bargaining power in sequential trading mechanism";"Tim Grebe,  Radosveta Ivanova-Stenzel and   Sabine Kröger";"A6";"2015-08-03";"C72,   C91,   D44,   D82";" We study experimentally the eect of bargaining power in sequential trading mechanisms that oer the possibility to trade at a xed price before an auction. In the \Buy-It-Now"" format, the seller oers a price prior to the auction; whereas in the \Sell-It-Now"" format, it is the buyer. Both formats are extensively used in online and oine markets. Despite very dierent strategic implications for buyers and sellers, results from our experiment suggest no eects of bargaining power on aggregate outcomes. There is, however, substantial heterogeneity within sellers. Sellers who neglect the adverse selection eect of their own price oer in the BIN format could benet from giving up bargaining power by using the \Sell-It-Now"" format.  "
"739";739;"2015-038";"Conditional Systemic Risk with Penalized Copula";"Ostap Okhrin,  Alexander Ristig,  Jeffrey Sheen and   Stefan Trück";"B10";"2015-08-03";"C40,   C46,   C51,   G1,   G2";" Financial contagion and systemic risk measures are commonly derived from conditional quantiles by using imposed model assumptions such as a linear parametrization. In this paper, we provide model free measures for contagion and systemic risk which are independent of the speci- cation of conditional quantiles and simple to interpret. The proposed systemic risk measure relies on the contagion measure, whose tail behavior is theoretically studied. To emphasize contagion from extreme events, conditional quantiles are specied via hierarchical Archimedean copula. The parameters and structure of this copula are simultaneously estimated by imposing a non-concave penalty on the structure. Asymptotic properties of this sparse estimator are derived and small sample properties illustrated using simulations. We apply the proposed framework to investigate the interconnectedness between American, European and Australasian stock market indices, providing new and interesting insights into the relationship between systemic risk and contagion. In particular, our ndings suggest that the systemic risk contribution from contagion in tail areas is typically lower during times of nancial turmoil, while it can be signicantly higher during periods of low volatility.  "
"740";740;"2015-039";"Dynamics of Real Per Capita GDP";"Daniel Neuhoff";"C7";"2015-08-11";"C51,   C52,   E32";" This study investigates the dynamics of quarterly real GDP per capita growth rates across four countries, the US, UK, Canada and France. I obtain estimates for ARIMA(p,q) processes for first differences of log quarterly real GDP per capita using Reversible Jump Markov Chain Monte Carlo, allowing me to account for model uncertainty when comparing the implied impulse responses across countries. The results are checked for robustness with respect to the detrending device. The estimated impulse response functions are different in shape. The persistence estimates for the US, France, Canada and Italy are clustered together, while the UK and Japan are clear outliers. Significant posterior uncertainty remains regarding the persistence estimates and the appropriate ARMA models. The results for the UK is sensitive to the time period. An analysis of the components of GDP for the US suggests that the dynamics are mainly driven by consumption.  "
"741";741;"2015-040";"The Role of Shadow Banking in the Monetary Transmission Mechanism and the Business Cycle";"Falk Mazelis";"C7";"2015-08-12";"E32,   E44,   E51,   G20";" This paper investigates the heterogeneous impact of monetary policy shocks on nancial in- termediaries. I distinguish between banks and shadow banks based on their funding constraints. Because credit creation by banks responds to economy-wide productivity endogenously, bank reaction to shocks corresponds to the balance sheet channel. Shadow banks are constrained by their available funding and their behavior is better explained by the lending channel. In line with empirical observations, shadow bank lending moves in the opposite direction to bank lending following monetary policy shocks, which mitigates aggregate credit responses. The propagation of real and nancial shocks is likewise altered when shadow banks are identied as a distinct sector among nancial intermediaries. Following estimation of the model using Bayesian methods, a historical shock decomposition highlights the roles of banks and shadow banks in the run-up to the 2007 - 08 nancial crisis.  "
"742";742;"2015-041";"Forecasting the oil price using house prices";"Rainer Schulz and   Martin Wersing";"B1";"2015-08-12";"C53,   E32,   Q47,   R31";" We show that house prices from Aberdeen in the UK improve in- and out-of-sample oil price forecasts. The improvements are of a similar magnitude to those attained using macroeconomic indicators. We ex- plain these forecast improvements with the dominant role of the oil industry in Aberdeen. House prices aggregate the dispersed knowl- edge of the future oil price that exists in the city. We obtain similar empirical evidence for Houston, another city dominated by the oil in- dustry. Consistent with our explanation, we nd that house prices from economically more diversied areas in the UK and the US do not improve oil price forecasts.  "
"743";743;"2015-042";"Copula-Based Factor Model for Credit Risk Analysis";"Meng-Jou Lu,  Cathy Yi-Hsuan Chen and   Karl Wolfgang Härdle";"B1";"2015-08-24";"C38,   C53,   F34,   G11,   G1";" A standard quantitative method to access credit risk employs a factor model based on joint multi- variate normal distribution properties. By extending a one-factor Gaussian copula model to make a more accurate default forecast, this paper proposes to incorporate a state-dependent recovery rate into the con- ditional factor loading, and model them by sharing a unique common factor. The common factor governs the default rate and recovery rate simultaneously and creates their association implicitly. In accordance with Basel III, this paper shows that the tendency of default is more governed by systematic risk rather than idiosyncratic risk during a hectic period. Among the models considered, the one with random fac- tor loading and a state-dependent recovery rate turns out to be the most superior on the default prediction.  "
"744";744;"2015-043";"On the Long-run Neutrality of Demand Shocks";"Wenjuan Chen and   Aleksei Netsunajev";"C14C15";"2015-09-04";"C32";" Abstract:  Long run neutrality restrictions have been widely used to identify structural shocks in VAR models. This paper revisits the seminal paper by Blanchard and Quah (1989), and investigates their identification scheme. We use structural VAR models with smoothly changing covariances for identification of shocks. The resulted impulse responses are economically meaningful. Formal test results reject the long-run neutrality of demand shocks.  "
"745";745;"2015-044";"The (De-)Anchoring of Inflation Expectations: New Evidence from the Euro Area";"Laura Pagenhardt,  Dieter Nautz and   Till Strohsal";"C14";"2015-09-04";"E31,   E52,   E58,   C22";" Abstract:   Well-anchored inflation expectations are a key factor for achieving economic stability. This paper provides new empirical results on the anchoring of long-term inflation expectations in the euro area. In line with earlier evidence, we find that euro area inflation expectations have been anchored until fall 2011. Since then, however, they respond significantly to macroeconomic news. Our results obtained from multiple endogenous break point tests suggest that euro area inflation expectations have remained de-anchored ever since. "
"746";746;"2015-045";"Tail Event Driven ASset allocation: evidence from equity and mutual funds’ markets";"Wolfgang Karl Härdle,  David Lee Kuo Chuen,  Sergey Nasekin,  Xinwen Ni and   Alla Petukhina";"B1";"2015-09-11";"C00,   C14,   C50,   C58 2";" Abstract:   Classical asset allocation methods have assumed that the distribution of asset returns is smooth, well behaved with stable statistical moments over time. The distribution is assumed to have constant moments with e.g., Gaussian distribution that can be conveniently parameterised by the first two moments. However, with market volatility increasing over time and after recent crises, asset allocators have cast doubts on the usefulness of such static methods that registered large drawdown of the portfolio. Others have suggested dynamic or synthetic strategies as alternatives, which have proven to be costly to implement. The authors propose and apply a method that focuses on the left tail of the distribution and does not require the knowledge of the entire distribution, and may be less costly to implement. The recently introduced TEDAS -Tail Event Driven ASset allocation approach determines the dependence between assets at tail measures. TEDAS uses adaptive Lasso based quantile regression in order to determine an active set of portfolio elements with negative non-zero coefficients. Based on these active risk factors, an adjustment for intertemporal dependency is made. The authors extend TEDAS methodology to three gestalts differing in allocation weightsÂ’ determination: a Cornish-Fisher Value-at-Risk minimization,  Markowitz diversification rule and naive equal weighting. TEDAS strategies significantly outperform other widely used allocation approaches on two asset markets: German equity and Global mutual funds. "
"747";747;"2015-046";"Site assessment, turbine selection, and local feed-in tariffs through the wind energy index";"Matthias Ritter and   Lars Deckert";"C11";"2015-09-11";"Q42,   Q47";" Abstract:   Since wind energy is rapidly growing, new wind farms are installed worldwide and a discussion is going on concerning the optimal political framework to promote this development. In this paper, we present a wind energy index, which is supportive for wind park planners, operators, and policy-makers. Based on long-term and low-scale reanalysis wind speed data from MERRA and true production data, it can predict the expected wind energy production for every location and turbine type. After an in-sample and out-of-sample evaluation of the index performance, it is applied to assess the wind energy potential of locations in Germany, to compare different turbine types and to derive the required compensation in terms of locally different feed-in tariffs. We show that in many parts of South Germany, profitability of new wind parks cannot be achieved given the current legal situation. "
"748";748;"2015-047";"TERES - Tail Event Risk Expectile based Shortfall";"Philipp Gschöpf,  Wolfgang Karl Härdle and   Andrija Mihoci";"B1";"2015-09-15";"C13,   C16,   G20,   G28";" Abstract:   A flexible framework for the analysis of tail events is proposed. The framework contains tail moment measures that allow for Expected Shortfall (ES) estimation. Connecting the implied tail thickness of a family of distributions with the quantile and expectile estimation, a platform for risk assessment is provided. ES and implications for tail events under different distributional scenarios are investigated, particularly we discuss the implications of increased tail risk for mixture distributions. Empirical results from the US, German and UK stock markets, as well as for the selected currencies indicate that ES can be successfully estimated on a daily basis using a one-year time horizon across different risk levels. "
"749";749;"2015-048";"CRIX or evaluating Blockchain based currencies";"Simon Trimborn and   Wolfgang Karl Härdle";"B1";"2015-10-15";"C51,   C52,   G10";" Abstract:   More and more companies start offering digital payment systems. Smartphones evolve to a digital wallet such that it seems like we are about to enter the era of digital finance. In fact we are already inside an digital economy. The market of e-x (x = ""finance"", ""money"", ""book"", you name it . . . ) has not only picked up enormous momentum but has become standard for driving innovative activities of the global economy. A few clicks at y and payment at z brings our purchase to location w. Own currencies for the digital market were therefore just a matter of time. The idea of the Nobel Laureate Hayek, see [1], to let companies offer concurrent currencies seemed for a long time scarcely probabilistic, but the invention of the Blockchain made it possible to fill his vision with life. Cryptocurrencies (abbr. cryptos) came up and widened the angle towards this new level of economic interaction. Since bitcoinsÂ’ appearance a bunch of new cryptos spread the web and offered new ways of proliferation. The crypto market then fanned out and showed clear signs of acceptance and deep liquidity so that one has to look closer at the general moves and dynamics. "
"750";750;"2015-049";"Inflation Co-movement across Countries in Multi-maturity Term Structure: An Arbitrage-Free Approach";"Shi Chen,  Wolfgang Karl Härdle and   Weining Wang";"B1";"2015-11-02";"G12,   E43,   E31";" Abstract:   Inflation expectation is acknowledged to be an important indicator for policy makers and financial investors. To capture a more accurate real-time estimate of inflation expectation on the basis of financial markets, we propose an arbitrage-free model across different countries in a multi-maturity term structure, where we first estimate inflation expectation by modelling the nominal and inflation-indexed bond yields jointly for each country. The Nelson-Siegel model is popular in fitting the term structure of government bond yields, the arbitrage-free model we proposed is the extension of the arbitrage-free dynamic Nelson-Siegel model proposed by Christensen, Diebold and Rudebusch (2011). We discover that the extracted common trend for inflation expectation is an important driver for each country of interest. Moreover, the model will lead to an improved forecast in a benchmark level of inflation and will provide good implications for financial markets. "
"751";751;"2015-050";"Nonparametric Estimation in case of Endogenous Selection";"Christoph Breunig,  Enno Mammen and   Anna Simoni";"Z";"2015-11-12";"C14,   C26";" Abstract:   This paper addresses the problem of estimation of a nonparametric regression function from selectively observed data when selection is endogenous. Our approach relies on independence between covariates and selection conditionally on potential outcomes. Endogeneity of regressors is also allowed for. In both cases, consistent two-step estimation procedures are proposed and their rates of convergence are derived. Also pointwise asymptotic distribution of the estimators is established. In addition, we propose a nonparametric specification test to check the validity of our independence assumption. Finite sample properties are illustrated in a Monte Carlo simulation study and an empirical illustration. "
"752";752;"2015-051";"Frictions or deadlocks? Job polarization with search and matching frictions";"Julien Albertini,  Jean Olivier Hairault,  François Langot and   Thepthida Sopraseuth";"C7";"2015-11-23";"E24,   J62,   J64,   O33";" Abstract:   This paper extends Pissarides (1990)Â’s matching model by considering two sectors (routine and manual) and workersÂ’ occupational choices, in the context of skill-biased demand shifts, to the detriment of routine jobs and in favour of manual jobs because of technological changes. The theoretical challenge is to investigate the reallocation process from the middle towards the bottom of the wage distribution. By using this framework, we shed light on the way in which labour market institutions affect the job polarization observed in the United States and Europe. The results of our quantitative experiments suggest that search frictions have non-trivial effects on the reallocation process and transitional dynamics of aggregate employment."
"753";753;"2015-052";"lCARE - localizing Conditional AutoRegressive Expectiles";"Xiu Xu,  Andrija Mihoci and   Wolfgang Karl Härdle";"B1";"2015-12-03";"C32,   C51,   G17";" Abstract:   We account for time-varying parameters in the conditional expectile based value at risk (EVaR) model. EVaR appears more sensitive to the magnitude of portfolio losses compared to the quantile-based Value at Risk (QVaR), nevertheless, by fitting the models over relatively long ad-hoc fixed time intervals, research ignores the potential time-varying parameter properties. Our work focuses on this issue by exploiting the local parametric approach in quantifying tail risk dynamics. By achieving a balance between parameter variability and modelling bias, one can safely fit a parametric expectile model over a stable interval of homogeneity. Empirical evidence at three stock markets from 2005- 2014 shows that the parameter homogeneity interval lengths account for approximately 1-6 months of daily observations. Our method outperforms models with one-year fixed intervals, as well as quantile based candidates while employing a time invariant portfolio protection (TIPP) strategy for the DAX portfolio. The tail risk measure implied by our model finally provides valuable insights for asset allocation and portfolio insurance."
"754";754;"2015-053";"Specification Testing in Random Coefficient Models";"Christoph Breunig and   Stefan Hoderlein";"B1";"2015-12-30";"C12,   C14";" Abstract:   In this paper, we suggest and analyze a new class of specification tests for random coefficient models. These tests allow to assess the validity of central structural features of the model, in particular linearity in coefficients and generalizations of this notion like a known nonlinear functional relationship. They also allow to test for degeneracy of the distribution of a random coefficient, i.e., whether a coefficient is fixed or random, including whether an associated variable can be omitted altogether. Our tests are nonparametric in nature, and use sieve estimators of the characteristic function. We analyze their power against both global and local alternatives in large samples and through a Monte Carlo simulation study. Finally, we apply our framework to analyze the specification in a heterogeneous random coefficients consumer demand model. "
"755";755;"2015-054";"TFP Convergence in German States since Reunification: Evidence and Explanations";"Michael C. Burda and   Battista Severgnini";"C7";"2015-12-30";"D24,   E01,   E22,   O33,   O4";" Abstract:   A quarter-century after reunication, labor productivity in eastern Germany continues to lag systematically behind the West. Denison-Hall-Jones point-in-time estimates point to large gaps in total factor productivity as the proximate cause, and auxiliary measurements which do not rely on capital stock data conrm a slowdown in TFP growth after 2000. Strikingly, capital intensity in eastern Germany, especially in industry, has overshot values in the West, casting doubt on the embodied technology hypothesis. Indeed, TFP growth is negatively associated with rates of expenditures on both total investment and plant and equipment. The best candidates for explaining the stubborn East-West TFP gap are the low concentration of managers in the East and the insucient R&D expenditure, rather than the concentration of rm headquarters and R&D personnel. "
"756";756;"2016-001";"Downside risk and stock returns: An empirical analysis of the long-run and short-run dynamics from the G-7 Countries";"Cathy Yi-Hsuan Chen,  Thomas C. Chiang and   Wolfgang Karl Härdle";"B1";"2016-01-07";"G11,   G12,   G15,   C24,   F3";" Abstract:   This paper presents presents presents a fractionally cointegrated vector autoregression (FCVAR) (FCVAR) (FCVAR) (FCVAR) model to examine to examine to examine to examine to examine to examine to examine various relations between stock returns and downside risk.  Evidence from major advanced markets markets markets markets supports the supports the notion that notion that notion that downside risk measured by value value value-at -risk ( risk (VaRVaRVaR) has significant information  content content that reflects that reflects that reflects that reflects that reflects lagged long-run variance and higher moments of risk for for predict redict ing stock returns. stock returns. stock returns. stock returns. The e The e vidence vidence vidence supports the positive tradeoff hypothesis and and the leverage effect leverage effect leverage  in the long in the long in the long run and and for  markets in the short run. We find that US downside risk accounts for 54.36% of price discovery, whereas the whereas the whereas the whereas the own effect from own effect from the country itself only 27.06%. "
"757";757;"2016-002";"Uncertainty and Employment Dynamics in the Euro Area and the US";"Aleksei Netsunajev and    Katharina Glass";"C15";"2016-01-13";"D80,   C32,   C11,   E24";" Abstract:   In this paper we investigate transmission and spillovers of local and foreign economic policy uncertainty shocks to unemployment in two largest economic regions in the world - the United States (US) and the Euro area (EA). For this purpose we deploy Bayesian Markov-switching structural vector autoregressive (MS-SVAR) model identified via heteroskedasticity. In addition to local effects we find foreign uncertainty shocks influence the Euro area but not the US unemployment. Moreover we document weaker spillovers of both local and foreign uncertainty shocks in the more volatile times.   "
"758";758;"2016-003";"College Admissions with Entrance Exams: Centralized versus Decentralized";"Isa E. Hafalir,  Rustamdjan Hakimov,  Dorothea Kübler and   Morimitsu Kurino";"A6";"2016-01-19";"C78; D47; D78; I21";" Abstract:   We study a college admissions problem in which colleges accept students by ranking studentsÂ’ efforts in entrance exams. StudentsÂ’ ability levels affect the cost of their efforts. We solve and compare the equilibria of Â“centralized college admissionsÂ” (CCA) where students apply to all colleges and Â“decentralized college admissionsÂ” (DCA) where students only apply to one college. We show that lower ability students prefer DCA whereas higher ability students prefer CCA. Many predictions of the theory are supported by a lab experiment designed to test the theory, yet we find a number of differences that render DCA less attractive than CCA compared to the equilibrium benchmark.    "
"759";759;"2016-004";"Leveraged ETF options implied volatility paradox: a statistical study";"Wolfgang Karl Härdle,  Sergey Nasekin and   Zhiwu Hong";"B1";"2016-02-02";"C00,   C14,   C50,   C58";" Abstract:   In this paper, we study the statistical properties of the moneyness scaling transformation by Leung and Sircar (2015). This transformation adjusts the moneyness coordinate of the implied volatility smile in an attempt to remove the discrepancy between the IV smiles for levered and unlevered ETF options. We construct bootstrap uniform confidence bands which indicate that in a statistical sense there remains a possibility that the implied volatility smiles are still not the same, even after moneyness scaling has been performed. This presents possible arbitrage opportunities on the (L)ETF market which can be exploited by traders. We build possible arbitrage strategies by constructing portfolios with LETF shares and options which possibly have a positive value at the point of creation and non-negative value at the expiration time. An empirical data application shows that there are indeed such opportunities in the market which result in risk-free gains for the investor. A dynamic ""trade-with-the-smile"" strategy based on a dynamic semiparametric factor model is presented. This strategy utilizes the dynamic structure of implied volatility surface allowing out-of-sample forecasting and information on unleveraged ETF options to construct theoretical one-step-ahead implied volatility surfaces. The codes used to obtain the results in this paper, are available on www.quantlet.de.  "
"760";760;"2016-005";"The German Labor Market Miracle, 2003 -2015: An Assessment";"Michael C. Burda";"C7";"2016-02-16";"E24,   J21";" Abstract:   This paper reviews the dramatic and widely noted developments in the German labor market in the past decade and surveys the most plausible reasons for these changes. Alternative hypotheses are compared and contrasted. I argue that the labor market reforms associated with the Agenda 2010 Â– the Hartz reforms Â– played a role at least as great as that of increasing flexibility of wage determination and the allocation of hours across workers. Until 2010, the German economic miracle could be accounted for by an expansion of part-time work, which has since been supplanted by a sustained expansion of full-time employment. Supported by wage flexibility in this segment, part-time employment represents an important new margin of flexibility in the German labor market.  "
"761";761;"2016-006";"What Derives the Bond Portfolio Value-at-Risk: Information Roles of Macroeconomic and Financial Stress Factors";"Anthony H. Tu and   Cathy Yi-Hsuan Chen";"B1";"2016-02-18";"G11,   G16";" Abstract:   This paper first develops a new approach, which is based on the Nelson-Siegel term structure factor-augmented model, to compute the VaR of bond portfolios. We then applied the model to examine whether information contained on macroeconomic variables and financial shocks can help to explain the variations of VaR. A principal component analysis is used to incorporate the information contained in different variables. The empirical result shows that, including macroeconomic variables and financial shocks in the Nelson-Siegel term structure factor model, we can observe an obvious tendency towards better VaR forecasting performance. Moreover, the impact of incorporating financial shocks seems to be stronger than that of incorporating macroeconomic variables.  "
"762";762;"2016-007";"Budget-neutral fiscal rules targeting inflation differentials";"Maren Brede";"C7";"2016-02-22";"E62,   E63,   F41,   F45";" Abstract:   In light of persistent in ation dispersion and rising debt levels in the EMU, this paper investigates the welfare implications of budget-neutral scal policies that counteract in ation dierentials. In a two-country DSGE model of a monetary union with traded and non-traded goods a national scal authority is able to reduce welfare losses arising from asymmetric shocks by following a Taylor-type rule for consumption taxes while using labour income taxes to balance its budget. Under technology and government spending shocks welfare losses can be reduced by up to 15%.  "
"763";763;"2016-008";"Measuring the benefit from reducing income inequality in terms of GDP";"Simon Voigts";"C7";"2016-02-24";"D31,   D63";" Abstract:   Given that well-being is a concave function of income, inequality is inecient from a utilitarian perspective. This paper proposes a way to express the utilitarian benet from redistributive reforms in terms of out- put, i.e. as a share of GDP. Three applications are presented: First, in nine European countries under study, a mild increase in government redis- tribution allows for gains in well-being equivalent to 8.9%-20.2% of higher GDP, and 55.8% for the US. Second, in the US, redistributing income in excess of the level at the 99th percentile is as benecial as a 39.5% GDP-increment. Third, revoking government redistribution in Germany reduces welfare by the same amount as a 25.4% decline in output. "
"764";764;"2016-009";"Solving DSGE Portfolio Choice Models with Asymmetric Countries";"Grzegorz R. Dlugoszek";"C7";"2016-02-29";"E44,   F41,   G11";" Abstract:   This paper proposes a combination of bifurcation methods and nonlinear moving average as a tool to solve asymmetric DSGE models with portfolio choice. Its performance is compared to the workhorse routine developed by Devereux and Sutherland (2010, 2011). The proposed technique has two advantages. First, it captures the direct effect of uncertainty on portfolio holdings. Second, it reflects the presence of asymmetries by yielding risk adjusted asset positions that lie close to the ergodic mean of the global solution. In terms of Euler equation errors, the proposed method is shown to be on average at least as good as the standard approach. "
"765";765;"2016-010";"No Role for the Hartz Reforms? Demand and Supply Factors in the German Labor Market, 1993-2014";"Michael C. Burda and   Stefanie Seele";"C7";"2016-02-29";"E24,   J21";" Abstract:   The supply and demand framework of Katz and Murphy (1992) provides new evidence on the source of changes in socially insured full-time and part-time employment in years preceding and following the implementation of the landmark Hartz reforms in Germany. Our findings are consistent with a stable demand for labor, especially in western Germany, implying that supply factors were decisive for the evolution of the labor market after 2003. The correlation of changes in wages and labor force participation is also consistent with a positive labor supply shock at a given working-age population. We also show that part-time employment played a decisive role in the post-2003 improvement of the German labor market. "
"766";766;"2016-011";"Cognitive Load Increases Risk Aversion";"Holger Gerhardt,  Guido P. Biele,  Hauke R. Heekeren and  Harald Uhlig";"A12";"2016-03-03";"C91,   D03,   D81,   D87";" Abstract:   We investigate how stable individualsÂ’ risk attitudes are with respect to changes in cognitive load. In a laboratory experiment using pairwise lottery choice and a within-subject design, we showthat putting subjects under load via a concurrent working-memory task significantly increases their risk aversion. Subjects made significantly faster choices under load. Regardless of load, they responded faster when choosing the less risky option in safeÂ–risky trials, but not in riskyÂ–risky trials. We discuss how these findings relate to both dual-system and unitarysystem theories of decision making.We observe that predictions of both recent dual-system and driftÂ–diffusion models of the decision-making process are confirmed by our data and argue for a convergence of these to-date separate strands of the literature. "
"767";767;"2016-012";"Neighborhood Effects in Wind Farm Performance: An Econometric Approach";"Matthias Ritter, Simone Pieralli and   Martin Odening";"C11+T1";"2016-03-07";"Q42; Q47";" Abstract:   The optimization of turbine density in wind farms entails a trade-off between the usage of scarce, expensive land and power losses through turbine wake effects. A quantification and prediction of the wake effect, however, is challenging because of the complex aerodynamic nature of the interdependencies of turbines. In this paper, we propose a parsimonious data driven econometric wake model that can be used to predict production losses of existing and potential wind parks. Motivated by simple engineering wake models, the predicting variables are wind speed, turbine alignment angle, and distance. By utilizing data from two wind parks in Germany, a significantly better prediction of wake effect losses is attained compared to the standard Jensen model. A scenario analysis reveals that a distance between turbines can be reduced up to three times the rotor size without entailing substantial production losses. In contrast, a suboptimal configuration of turbines with respect to the main wind direction can result in production losses that are five times higher."
"768";768;"2016-013";"The importance of time-varying parameters in new Keynesian models with zero lower bound";"Julien Albertini and   Hong Lan";"C7";"2016-03-07";"E3,   J6";" Abstract:   The optimization of turbine density in wind farms entails a trade-off between the usage of scarce, expensive land and power losses through turbine wake effects. A quantification and prediction of the wake effect, however, is challenging because of the complex aerodynamic nature of the interdependencies of turbines. In this paper, we propose a parsimonious data driven econometric wake model that can be used to predict production losses of existing and potential wind parks. Motivated by simple engineering wake models, the predicting variables are wind speed, turbine alignment angle, and distance. By utilizing data from two wind parks in Germany, a significantly better prediction of wake effect losses is attained compared to the standard Jensen model. A scenario analysis reveals that a distance between turbines can be reduced up to three times the rotor size without entailing substantial production losses. In contrast, a suboptimal configuration of turbines with respect to the main wind direction can result in production losses that are five times higher."
"769";769;"2016-014";"Aggregate Employment, Job Polarization and Inequalities: A Transatlantic Perspective";"Julien Albertini,  Francois Langot,  Thepthida Sopraseuth and   Jean Olivier Hairault";"C7";"2016-03-07";"E24,   J62,   J64,   O33";" Abstract:   This paper develops a multi-sectorial search and matching model with endogenous occupational choice in a context of structural change. Our objective is to shed light on the way labor market institutions aect aggregate employment, job polarization and inequalities observed in the US and in European countries. We consider the cases of the US, France and Germany that are representative of alternative institutional settings, having the potential to induce divergent time-paths in the evolution of labor market outcomes during the process of technological transition. In the US and in Germany, we nd employment gains from technological change and job polarization, whereas, in France, the technological change reduces aggregate employment in a context of job polarization. In the US, an half of these employment gains are due to the technological change, and the other half to the changes in the LMI, the contribution of the rise in share of skilled worker being negligible. In France, the change in LMI aects new job opportunities in manual jobs: the reallocation of routine workers towards manual jobs is obstructed for want of job creations of manual services. Hence, without technological change, the fall in French employment would have been cut by 70%. The model also predicts that, without the increase in skilled labor supply, the fall in French employment would have doubled. The improvement in educational attainment dampened the unfavorable consequences of technological change. we show that Germany transforms this structural change in employment gains, only after the labor reforms implemented after the middle of the 90s.   "
"770";770;"2016-015";"The Anchoring of In flation Expectations in the Short and in the Long Run";"Dieter Nautz,  Aleksei Netsunajev and   Till Strohsal";"C14 C15";"2016-03-18";"E31,   E52,   E58";" Abstract:   his paper introduces structural VAR analysis as a tool for investigating the anchoring of inflation expectations. We show that U.S. consumersÂ’ inflation expectations are anchored in the long run because macro-news shocks are long-run neutral for long-term inflation expectations. The identification of structural shocks helps to explain why inflation expectations deviate from the central bankÂ’s target in the short run. Our results indicate that the recent decline of long-term inflation expectations does not result from deanchoring macro-news but can be attributed to downward adjustments of consumersÂ’ expectations about the central bankÂ’s inflation target.   "
"771";771;"2016-016";"Irrational Exuberance and Herding in Financial Markets";"Christopher Boortz";"C14";"2016-03-21";"D81,   D82,   G12,   G14";" Abstract:  In the context of a two-state, two-trader financial market herd model introduced by Avery and Zemsky (1998) we             investigate how informational ambiguity in conjunction with waves of optimism and pessimism affect investor behavior,             social learning and price dynamics. Without ambiguity, neither herding nor contrarianism is possible. If there is ambiguity and agents have             invariant ambiguity preferences, only contrarianism is possible. If on the other hand ambiguity is high and traders become             overly exuberant (or desperate) as the asset price surges (or plummets), we establish that investor herding may drive             prices away from fundamentals with economically relevant probability.  "
"772";772;"2016-017";"Calculating Joint Confidence Bands for Impulse Response Functions using Highest Density Regions";"Helmut Lütkepohl,  Anna Staszewska-Bystrova and   Peter Winker";"C15";"2016-03-21";"C32";" Abstract:  This paper proposes a new non-parametric method of constructing joint con- dence bands for impulse response functions of vector autoregressive models. The estimation uncertainty is captured by means of bootstrapping and the highest density region (HDR) approach is used to construct the bands. A Monte Carlo comparison of the HDR bands with existing alternatives shows that the former are competitive with the bootstrap-based Bonferroni and Wald condence regions. The relative tightness of the HDR bands matched with their good coverage properties makes them attractive for applications. An application to corporate bond spreads for Germany highlights the potential for empirical work.  "
"773";773;"2016-018";"Factorisable Sparse Tail Event Curves with Expectiles";"Wolfgang K. Härdle,  Chen Huang and   Shih-Kang Chao";"B1";"2016-03-21";"C38,   C55,   C61,   C91,   D8";" Abstract:  Oberwolfach Report: New Developments in Functional and Highly Multivariate Statistical Methodology  "
"774";774;"2016-019";"International dynamics of inflation expectations";"Aleksei Netšunajev and   Lars Winkelmann";"C14  C15";"2016-05-03";"E31,   F42,   E52";" Abstract:  To what extent are US and Euro Area (EA) inflation expectations determined by foreign shocks? How do transmissions change during the great recession and European sovereign debt crisis? We address these questions with a flexible structural VAR model of weekly financial marketsÂ’ inflation expectations and an index of commodity futures. For the identification of the model, we exploit the heteroscedasticity of the data. We propose instrument-type regressions to uncover the economic nature and origin of identified shocks. In line with the discussion about global inflation, we find that inflation expectations can be labeled global over short expectations horizons but local at long horizons. While large US macro shocks explain the strong drop in US and EA inflation expectations during the great recession, expectations shocks are the important driver from 2009 on.  "
"775";775;"2016-020";"Academic Ranking Scales in Economics: Prediction and Imputation";"Alona Zharova,  Andrija Mihoci and   Wolfgang Karl Härdle";"B1 Z";"2016-05-09";"C14,   C53,   C81,   M10";" Abstract:  Publications are a vital element of any scientistÂ’s career. It is not only the number of media outlets but aslo the quality of published research that enters decisions on jobs, salary, tenure, etc. Academic ranking scales in economics and other disciplines are, therefore, widely used in classification, judgment and scientific depth of individual research. These ranking systems are competing, allow for different disciplinary gravity and sometimes give orthogonal results. Here a statistical analysis of the interconnection between Handelsblatt (HB), Research Papers in Economics (RePEc, here RP) and Google Scholar (GS) systems is presented. Quantile regression allows us to successfully predict missing ranking data and to obtain a so-called HB Common Score and to carry out a cross-rankings analysis. Based on the merged ranking data from different data providers, we discuss the ranking systems dependence, analyze the age effect and study the relationship between the research expertise areas and the ranking performance.  "
"776";776;"2016-021";"CRIX an Index for blockchain based Currencies";"Simon Trimborn and   Wolfgang Karl Härdle";"B1";"2016-05-24";"C51,   C52,   G10";" Abstract:  The S&P500 or DAX30 are important benchmarks for the financial industry. These and other indices describe different compositions of certain segments of the financial markets. For currency markets, the IMF offers the index SDR. Prior to the Euro, the ECU existed, which was an index representing the development of European currencies. It is surprising, though, to see that the common index providers have not mapped emerging e-coins into an index yet because with cryptos like Bitcoin, a new kind of asset of great public interest has arisen. Index providers decide on a fixed number of index constituents which will represent the market segment. It is a huge challenge to set this fixed number and develop the rules to find the constituents, especially since markets change and this has to be taken into account. A method relying on the AIC is proposed to quickly react to market changes and therefore enable us to create an index, referred to as CRIX, for the cryptocurrency market. The codes used to obtain the results in this paper are available via www.quantlet.de .  "
"777";777;"2016-022";"Towards a national indicator for urban green space provision and environmental inequalities in Germany: Method and findings";"Henry Wüstemann,  Dennis Kalisch and   Jens Kolbe";"B3";"2016-06-14";"Q56,   Q58,   R14,   R20,   R5";" Abstract:  Action 5 of the EU 2020 Biodiversity Strategy explicitly mentions that member states will map and assess the state of ecosystems and their services in their national territory by 2014 with the assistance of the Commission. Access to urban green is a key contributor to social and ecological functions in urban environments. However, in Germany - like in many other European countries - a national indicator measuring the provision of urban green on household and individual level is missing. This study develops a national indicator for urban green space provision and environmental inequalities in Germany on household and individual level. We investigate the provision of urban green by merging geo-coded household data from the German Socio- Economic Panel (GSOEP) and census population data with geo-coded data on land use from the European Urban Atlas (EUA) for German major cities with more than 100.000 inhabitants. Based on open green space standards applied in European urban city planning we dene two variables measuring access to green: First, we estimate the distance to urban green measured as the Euclidean distance between the household and the nearest green-site in meters. Secondly, we calculate the coverage of urban green space around the households in square meters. Results of the distance analysis based on GSOEP data show a mean and median distance to public green space of 229:1m and 190:5m, respectively. The results further indicate that 93% of the German households have access to green space within a 500m and 74.1% within a 300m buer around their location. The average green space provision in German major cities adds up to 8:1m2 per capita (median). Moreover, statistical analysis of the socio-economic background of the households shows dierences in urban green provision related to income, education, employment status, migration background and nationality. We also identify dierences in green space provision on the city level ranging from 10:6ha (city o"
"778";778;"2016-023";"A Mortality Model for Multi-populations: A Semi-Parametric Approach";"Lei Fang,  Wolfgang K. Härdle and   Juhyun Park";"B1";"2016-06-21";"C14,   C32,   C38,   J11,   J1";" Abstract:  Mortality is different across countries, states and regions. Several empirical research works however reveal that mortality trends exhibit a common pattern and show similar structures across populations. The key element in analyzing mortality rate is a time-varying indicator curve. Our main interest lies in validating the existence of the common trends among these curves, the similar gender differences and their variability in location among the curves at the national level. Motivated by the empirical findings, we make the study of estimating and forecasting mortality rates based on a semi-parametric approach, which is applied to multiple curves with the shape-related nonlinear variation. This approach allows us to capture the common features contained in the curve functions and meanwhile provides the possibility to characterize the nonlinear variation via a few deviation parameters. These parameters carry an instructive summary of the time-varying curve functions and can be further used to make a suggestive forecast analysis for countries with barren data sets. In this research the model is illustrated with mortality rates of Japan and China, and extended to incorporate more countries. All numerical procedures are transparent and reproduced on www.quantlet.de. "
"779";779;"2016-024";"Simultaneous Inference for the Partially Linear Model with a Multivariate Unknown Function when the Covariates are Measured with Errors";"Kun Ho Kim,  Shih-Kang Chao and   Wolfgang K. Härdle";"B1";"2016-08-04";"C12,   C13,   C14";" Abstract:  In this paper, we analyze the nonparametric part of a partially linear model when the covariates in parametric and non-parametric parts are subject to measurement errors. Based on a two-stage semi-parametric estimate, we construct a uniform condence surface of the multivariate function for simultaneous inference. The developed methodology is applied to perform inference for the U.S. gasoline demand where the income and price variables are measured with errors. The empirical results strongly suggest that the linearity of the U:S: gasoline demand is rejected. "
"780";780;"2016-025";"Forecasting Limit Order Book Liquidity Supply-Demand Curves with Functional AutoRegressive Dynamics";"Ying Chen,  Wee Song Chua and   Wolfgang K. Härdle";"B1";"2016-08-04";"C13,   C32,   C53";" Abstract:  Limit order book contains comprehensive information of liquidity on bid and ask sides. We propose a Vector Functional AutoRegressive (VFAR) model to describe the dynamics of the limit order book and demand curves and utilize the tted model to predict the joint evolution of the liquidity demand and supply curves. In the VFAR framework, we derive a closed-form maximum likelihood estimator under sieves and provide the asymptotic consistency of the estimator. In application to limit order book records of 12 stocks in NASDAQ traded from 2 Jan 2015 to 6 Mar 2015, it shows the VAR model presents a strong predictability in liquidity curves, with R2 values as high as 98.5 percent for insample estimation and 98.2 percent in out-of-sample forecast experiments. It produces accurate 5????; 25???? and 50????minute forecasts, with root mean squared error as low as 0.09 to 0.58 and mean absolute percentage error as low as 0.3 to 4.5 percent.  "
"781";781;"2016-026";"VAT multipliers and pass-through dynamics";"Simon Voigts";"C7";"2016-08-18";"E62";" Abstract:  To quantify scal multipliers in Eurozone countries, ECB, European Commission and IMF draw heavily on large-scale DSGE models. In these models, the value added tax (VAT) is implemented as consumption tax, implying essentially full contemporaneous pass-through of changes in the tax liability to consumers. However, empirical evidence suggests that VAT pass-through in Europe occurs only gradually. To investigate how realistic pass-through dynamics aect VAT multipliers, a DSGE model is augmented by a retail sector, which allows to replicate empirical pass- through estimates. The resulting short-run multipliers are dramatically smaller than those from a consumption tax, suggesting systematic over- estimation in institutional research.  "
"782";782;"2016-027";"Can a Bonus Overcome Moral Hazard? An Experiment on Voluntary Payments, Competition, and Reputation in Markets for Expert Services";"Vera Angelova and   Tobias Regner";"A6";"2016-08-29";"C91,   D03,   D82,   G20,   I1";" Abstract:  Interactions between players with private information and opposed interests are often prone to bad advice and inecient outcomes, e.g. markets for nancial or health care services. In a deception game we investigate experimentally which factors could improve advice quality. Besides advisor competition and identiability we add the possibility for clients to make a voluntary payment, a bonus, after observing advice quality. We observe a positive eect on the rate of truthful advice when the bonus creates multiple opportunities to reciprocate, that is, when the bonus is combined with identiability (leading to several client-advisor interactions over the course of the game) or competition (allowing one advisor to have several clients who may reciprocate within one period). Moreover, identiability signicantly increases truth-telling under competition.  "
"783";783;"2016-028";"Relative Performance of Liability Rules: Experimental Evidence";"Vera Angelova,  Giuseppe Attanasi and   Yolande Hiriart";"A6";"2016-08-29";"D82,   K13,   K32,   Q58";" Abstract:  We compare the performance of liability rules for managing environmental disasters when third parties are harmed and cannot always be compensated. A firm can invest in safety to reduce the likelihood of accidents. The firmÂ’s investment is unobservable to authorities. Externality and asymmetric information call for public intervention to define rules aimed at increasing prevention. We determine the investment in safety under No Liability, Strict Liability and Negligence, and compare it to the first best. Additionally, we investigate how the (dis)ability of the firm to fully cover potential damages affects the firmÂ’s behavior. An experiment tests the theoretical predictions. In line with theory, Strict Liability and Negligence are equally effective; both perform better than No Liability; investment in safety is not sensitive to the ability of the firm to compensate potential victims. In contrast with theory, prevention rates absent liability are much higher and liability is much less effective than predicted.  "
"784";784;"2016-029";"What renders financial advisors less treacherous? On commissions and reciprocity";"Vera Angelova";"A6";"2016-08-29";"C91,   D82,   D03,   L15,   M5";" Abstract:  An advisor is supposed to recommend a nancial product in the best interest of her client. However, the best product for the client may not always be the product yielding the highest commission to the advisor. Do advisors nevertheless provide truthful advice? If not, will a voluntary or obligatory upfront payment by clients induce more truthful advice? According to the results, both types of payment lead to more truthful advice. More generally, in a senderreceiver game with con ict of interest, an upfront payment to the sender by the receiver improves information transmission.  "
"785";785;"2016-030";"Do voluntary payments to advisors improve the quality of financial advice? An experimental sender-receiver game";"Vera Angelova and   Tobias Regner";"A6";"2016-08-29";"C91,   D03,   D82,   G20,   L1";" Abstract:  The market for retail nancial products (e.g. investment funds or insurances) is marred by information asymmetries. Clients are not well informed about the quality of these products. They have to rely on the recommendations of advisors. Incentives of advisors and clients may not be aligned, when fees are used by nancial institutions to steer advice. We experimentally investigate whether voluntary contract components can reduce the con ict of interest and increase truth telling of advisors. We compare a voluntary payment upfront, an obligatory payment upfront, a voluntary bonus afterwards, and a three-stage design with a voluntary payment upfront and a bonus after. Across treatments, there is signicantly more truthful advice when both clients and advisors have opportunities to reciprocate. Within treatments, the frequency of truthful advice is signicantly higher when the voluntary payment is large.  "
"786";786;"2016-031";"A first econometric analysis of the CRIX family";"Shi Chen,  Cathy Yi-Hsuan Chen,  Wolfgang Karl Härdle,  TM Lee and   Bobby Ong";"B1";"2016-08-30";"C51,   C52,   G10";" Abstract:  The CRIX (CRyptocurrency IndeX) has been constructed based on approximately 30 cryptos and captures high coverage of available market capitalisation. The CRIX index family covers a range of cryptos based on dierent liquidity rules and various model selection criteria. Details of ECRIX (Exact CRIX), EFCRIX (Exact Full CRIX) and also intraday CRIX movements may be found on the webpage of hu.berlin/crix.  "
"787";787;"2016-032";"Specification Testing in Nonparametric Instrumental Quantile Regression";"Christoph Breunig";"B1";"2016-08-31";"C12,   C14";" Abstract:  There are many environments in econometrics which require nonseparable modeling of a structural disturbance. In a nonseparable model, key conditions are validity of instrumental variables and monotonicity of the model in a scalar unobservable. Under these conditions the nonseparable model is equivalent to an instrumental quantile regression model. A failure of the key conditions, however, makes instrumental quantile regression potentially inconsistent. This paper develops a methodology for testing the hypothesis whether the instrumental quantile regression model is correctly specied. Our test statistic is asymptotically normally distributed under correct specication and consistent against any alternative model. In addition, test statistics to justify model simplication are established. Finite sample properties are examined in a Monte Carlo study and an empirical illustration.  "
"788";788;"2016-033";"Functional Principal Component Analysis for Derivatives of Multivariate Curves";"Maria Grith, Wolfgang K. Härdle, Alois Kneip and   Heiko Wagner";"B1";"2016-09-07";"C13,   C14,   G13";" Abstract:  We present two methods based on functional principal component analysis (FPCA) for the estimation of smooth derivatives of a sample of random functions, which are observed in a more than one-dimensional domain.We apply eigenvalue decomposition to a) the dual covariance matrix of the derivatives, and b) the dual covariance matrix of the observed curves. To handle noisy data from discrete observations, we rely on local polynomial regressions. If curves are contained in a finite-dimensional function space, the secondmethod performs better asymptotically. We apply our methodology in a simulation and empirical study, inwhichwe estimate state price density (SPD) surfaces from call option prices.We identify three main components, which can be interpreted as volatility, skewness and tail factors.We also find evidence for term structure variation.  "
"789";789;"2016-034";"Blooming Landscapes in the West? - German reunification and the price of land.";"Raphael Schoettler and   Nikolaus Wolf";"B3";"2016-09-14";"F15,   N14,   N94,   R12,   R3";" Abstract:  German reunication was a positive market access shock for both East and West Ger- many. Regions that for 45 years had experienced a decline in population due to their loss in market access following the division of Germany after WWII were most strongly aected by this positive shock. We use an entirely new data set to analyse the eects of German reunication on the value of land in West Germany. We nd that regions in the immediate border area experienced a relative rise in land prices compared to regions outside a 100km radius from the border. At the same time we conrm the absence of a population eect (Redding and Sturm, 2008) even including rural boroughs. We nd that land values have adjusted more quickly than population and in some cases even overshot predicted long-run levels within the rst decade of reunication. We attribute this nding to the information and expectation component of land prices. Land values incorporate expectations about long- run equilibrium adjustments following reunication more swiftly, but rms and households are slower to react due to the costs of relocating. The results are consistent with empirical work on the positive eects of infrastructure projects on land values (Yiu and Wong, 2005; Lai et al., 2007; Duncan, 2011). "
"790";790;"2016-035";"Time-Adaptive Probabilistic Forecasts of Electricity Spot Prices with Application to Risk Management.";"Brenda López Cabrera  and   Franziska Schulz";"C11";"2016-09-26";"C1,   Q41,   Q47";" Abstract:  The increasing exposure to renewable energy has amplied the need for risk management in electricity markets. Electricity price risk poses a major challenge to market participants. We propose an approach to model and fore- cast electricity prices taking into account information on renewable energy production. While most literature focuses on point forecasting, our method- ology forecasts the whole distribution of electricity prices and incorporates spike risk, which is of great value for risk management. It is based on func- tional principal component analysis and time-adaptive nonparametric density estimation techniques. The methodology is applied to electricity market data from Germany. We nd that renewable infeed eects both, the location and the shape of spot price densities. A comparison with benchmark methods and an application to risk management are provided. "
"791";791;"2016-036";"Protecting Unsophisticated Applicants in School Choice through Information Disclosure";"Christian Basteck and   Marco Mantovani";"C10";"2016-09-30";"C78,   C91,   D82,   I24";" Abstract:  Unsophisticated applicants can be at a disadvantage under manipulable and hence strategically demanding school choice mechanisms. Disclosing information on applications in previous admission periods makes it easier to asses the chances of being admitted at a particular school, and hence may level the playing field between applicants who differ in their cognitive ability. We test this conjecture experimentally for the widely used Boston mechanism. Results show that, absent this information, there exist a substantial gap between subjects of higher and lower cognitive ability, resulting in significant differences in payoffs, and ability segregation across schools. The treatment is effective in improving applicantsÂ’ strategic performance. However, because both lower and higher ability subjects improve when they have information about past demands, the gap between the two groups shrinks only marginally, and the instrument fails at levelling the playing field.  "
"792";792;"2016-037";"Cognitive Ability and Games of School Choice";"Christian Basteck and   Marco Mantovani";"C10";"2016-10-04";"C78,   C91,   D82,   I24";" Abstract:  We take school admission mechanisms to the lab to test whether the widely-used manipulable Boston-mechanism disadvantages students of lower cognitive ability and whether this leads to ability segregation across schools. Results show this is the case: lower ability participants receive lower payoffs and are over-represented at the worst school. Under the strategy-proof Deferred Acceptance mechanism, payoff differences are reduced, and ability distributions across schools harmonized. Hence, we find support for the argument that a strategy-proof mechanisms Â“levels the playing-fieldÂ”. Finally, we document a trade-off between equity and efficiency in that average payoffs are larger under Boston than under Deferred Acceptance.  "
"793";793;"2016-038";"The Cross-Section of Crypto-Currencies as Financial Assets: An Overview";"Hermann Elendner,  Simon Trimborn,  Bobby Ong and   Teik Ming Lee";"B1";"2016-10-06";"G11,   G15,   F31";" Abstract:  Crypto-currencies have developed a vibrant market since bitcoin, the rst crypto-currency, was created in 2009. We look at the properties of cryptocurrencies as nancial assets in a broad cross-section. We discuss approaches of altcoins to generate value and their trading and information platforms. Then we investigate crypto-currencies as alternative investment assets, studying their returns and the co-movements of altcoin prices with bitcoin and against each other. We evaluate their addition to investors' portfolios and document they are indeed able to enhance the diversication of portfolios due to their little co-movements with established assets, as well as with each other. Furthermore, we evaluate pure portfolios of crypto-currencies: an equallyweighted one, a value-weighted one, and one based on the CRypto-currency IndeX (CRIX). The CRIX portfolio displays lower risk than any individual of the liquid crypto-currencies. We also document the changing characteristics of the crypto-currency market. Deepening liquidity is accompanied by a rise in market value, and a growing number of altcoins is contributing larger amounts to aggregate crypto-currency market capitalization. "
"794";794;"2016-039";"Disinflation and the Phillips Curve: Israel 1986-2015";"Rafi Melnick and   Till Strohsal";"C14";"2016-10-12";"E31,   E52,   E58,   C22";" Abstract:  A Phillips Curve (PC) framework is utilized to study the challenging post-1985 disinflation process in Israel. The estimated PC is stable and has forecasting power. Based on endogenous structural break tests we find that actual and expected inflation are co-breaking. We argue that the step-like development of inflation is in line with shocks and monetary policy that changed inflationary expectations. The disinflation process was long, and a long-term commitment by both the Central Bank and the government was required. Credibility was achieved gradually and the transition from the last step of 10% to 2% inflation was accomplished by introducing an inflation targeting regime. "
"795";795;"2016-040";"Principal Component Analysis in an Asymmetric Norm";"Ngoc M. Tran,  Petra Burdejová,  Maria Osipenko and   Wolfgang K. Härdle";"B1";"2016-10-19";"C38,   C55,   C61,   C63,   D8";" Abstract:  Principal component analysis (PCA) is a widely used dimension reduction tool in the analysis of high-dimensional data. However, in many applications such as risk quantication in nance or climatology, one is interested in capturing the tail variations rather than variation around the mean. In this paper, we develop Principal Expectile Analysis (PEC), which generalizes PCA for expectiles. It can be seen as a dimension reduction tool for extreme value theory, where one approximates  uctuations in the -expectile level of the data by a low dimensional subspace. We provide algorithms based on iterative least squares, prove upper bounds on their convergence times, and compare their performances in a simulation study. We apply the algorithms to a Chinese weather dataset and fMRI data from an investment decision study."
"796";796;"2016-041";"Forward Guidance under Disagreement - Evidence from the Fed's Dot Projections";"Gunda-Alexandra Detmers";"C14";"2016-10-19";"E52,   E58";" Abstract:  This paper compares the effectiveness of date- and state-based forward guidance issued by the Federal Reserve since mid-2011 accounting for the influence of disagreement within the FOMC. Effectiveness is investigated through the lens of interest ratesÂ’ sensitivity to macroeconomic news and I find that the FedÂ’s forward guidance reduces the sensitivity and therefore crowds out other public information. The sensitivity shrinkage is stronger in the case of date-based forward guidance due to its unconditional nature. Yet, high levels of disagreement among monetary policy makers as published through the FOMCÂ’s dot projections since 2012 partially restore sensitivity to macroeconomic news. Thus, disagreement appears to lower the information content of forward guidance and to weaken the FedÂ’s commitment as perceived by financial markets. The dot projections are therefore able to reduce the focal point character of forward guidance.   "
"797";797;"2016-042";"The Impact of a Negative Labor Demand Shock on Fertility - Evidence from the Fall of the Berlin Wall";"Hannah Liepmann";"A9";"2016-10-21";"J13,   J23,   P36";" Abstract:  How does a negative labor demand shock impact individual-level fertility? I analyze this question in the context of the East German fertility decline after the fall of the Berlin Wall in 1989. Exploiting dierential pressure for restructuring across industries, I nd that throughout the 1990s, women more severely impacted by the demand shock had more children on average than their counterparts who were less severely impacted. I argue that in uncertain economic circumstances, women with relatively more favorable labor market outcomes postpone childbearing in order not to put their labor market situations at further risk. This mechanism is relevant for all qualication groups, including high-skilled women. There is some evidence for an impact on completed fertility. "
"798";798;"2016-043";"Implications of Shadow Bank Regulation for Monetary Policy at the Zero Lower Bound";"Falk Mazelis";"C7";"2016-10-26";"E32,   E44,   E52,   G11";" Abstract:  Counter to the credit channel of monetary transmission, monetary policy tightening induces a rise in lending by two dierent types of non-bank nancial institutions (NBFI): shadow banks and investment funds. A monetary DSGE model is able to replicate the empirical facts when augmented with interme- diaries that allow for regulatory arbitrage on the one hand, and household portfolio rebalancing on the other. Therefore NBFI reduce the eectiveness of the bank lending channel, which posits a decrease in bank lending following monetary tightening. Given the pending regulation of the nancial system, I study how regulation of the shadow banking sector may aect the monetary transmission mechanism, especially during a zero lower bound (ZLB) episode. I nd that bringing shadow banks back onto the balance sheets of commercial banks is benecial for consumption smoothing. Alternatively, regulating them like investment funds results in a milder recession during, and a quicker escape from, the ZLB. This is because a large demand shock that moves the economy to the ZLB acts in a similar way to a monetary tightening due to the inability to lower the policy rate to the unconstrained level. Consequently, the bank lending channel becomes operational and its eectiveness can be reduced via less reliance on deposit funding. "
"799";799;"2016-044";"Dynamic Contracting with Long-Term Consequences: Optimal CEO Compensation and Turnover";"Suvi Vasama";"A8";"2016-10-26";"C73,   D82,   D86";" Abstract:  We examine optimal managerial compensation and turnover policy in a principal-agent model in which the firm output is serially correlated over time. The model captures a learning-by-doing feature: higher effort by the manager increases the quality of the match between the firm and the manager in the future. The optimal incentive scheme entails an inefficiently high turnover rate in the early stages of the employment relationship. The optimal turnover probability depends on the past performance and the likelihood of turnover decreases gradually with superior performance. With good enough past performance, the turnover policy reaches efficiency; the manager is never retained if it is inefficient to do so. The managerÂ’s compensation depends on the firm value and the optimal performance-compensation relation increases with past performance.    "
"800";800;"2016-045";"Information Acquisition and Liquidity Dry-Ups";"Philipp Koenig and   David Pothier";"C10";"2016-10-26";"D82,   G01,   G12";" Abstract:  We analyze a novel feedback mechanism between market and funding liquidity that causes self-fullling liquidity dry-ups. Financial rms facing funding withdrawals have an incentive to acquire information about their assets. Those with good assets gain by resorting to outside liquidity sources and withhold assets from secondary markets. This leads to adverse selection and lowers market prices. If prices fall by enough, funding withdrawals are amplied and market and funding illiquidity become mutually reinforcing. We compare dierent policy measures that can mitigate the risk of inecient liquidity dry-ups. While outright debt purchases can implement the ecient allocation, liquidity injections may backre and exacerbate adverse selection.   "
"801";801;"2016-046";"Credit Rating Score Analysis";"Wolfgang Karl Härdle,  Phoon Kok Fai and   David Lee Kuo Chuen";"B1";"2016-11-02";"C01,   G00,   G17,   G24";" Abstract:  We analyse a sample of funds and other securities each assigned a total rating score by an unknown expert entity. The scores are based on a number of risk and complexity factors, each assigned a category (factor score) of Low, Medium, or High by the expert entity. A principal component analysis of the data reveals that based on the chosen risk factors alone we cannot identify a single underlying latent source of risk in the data. Conversely, the chosen complexity factors are clearly related to one or two underlying sources of complexity. For the sample we nd a clear positive relation between the rst principal component and the total expert score. An attempt to match the securities' expert score by linear projection of their individual factor scores yields a best case correlation between expert score and projection of 0.9952. However, the sum of squared dierences is, at 46.5552, still notable.  "
"802";802;"2016-047";"Time Varying Quantile Lasso";"Lenka Zbonakova,  Wolfgang Karl Hardle and   Weining Wang";"B1";"2016-11-07";"C21,   G01,   G20,   G32";" Abstract:  In the present paper we study the dynamics of penalization parameter ? of the least absolute shrinkage and selection operator (Lasso) method proposed by Tibshirani (1996) and extended into quantile regression context by Li and Zhu (2008). The dynamic behaviour of the parameter ? can be observed when the model is assumed to vary over time and therefore the fitting is performed with the use of moving windows. The proposal of investigating time series of ? and its dependency on model characteristics was brought into focus by HÂ¨ardle et al. (2016), which was a foundation of FinancialRiskMeter (http://frm.wiwi.hu-berlin.de). Following the ideas behind the two aforementioned projects, we use the derivation of the formula for the penalization parameter ? as a result of the optimization problem. This reveals three possible effects driving ?; variance of the error term, correlation structure of the covariates and number of nonzero coefficients of the model. Our aim is to disentangle these three effect and investigate their relationship with the tuning parameter ?, which is conducted by a simulation study. After dealing with the theoretical impact of the three model characteristics on ?, empirical application is performed and the idea of implementing the parameter ? into a systemic risk measure is presented. The codes used to obtain the results included in this work are available on http://quantlet.de/d3/ia/.  "
"803";803;"2016-048";"Unraveling of Cooperation in Dynamic Collaboration";"Suvi Vasama";"A8";"2016-11-07";"C73,   D83,   O31";" Abstract:  We examine collaboration in a one-arm bandit problem in which the players' actions affect the distribution over future payoffs. The players need to exert costly effort both to enhance the value of a risky technology and to learn about its current state. Both product value and learning are public goods, which gives the players incentives to free-ride on each others' actions. This leads to an inefficiently low aggregate level of effort. When the players' actions affect the distribution over future payoffs, they eventually get trapped in the low action, causing an inefficient unraveling of the game. Moreover, the players' incentives to exert effort depend on the state that in turn depends on the aggregate effort. If the players start restricting effort when the belief decreases in expectation, the two effects play in the same direction. Higher effort encourages higher effort and vice versa. Unraveling leads to multiple symmetric Markov perfect equilibria.   "
"804";804;"2016-049";"Q3-D3-LSA";"Lukas Borke and   Wolfgang K. Härdle";"B1";"2016-11-15";"C87,   C88,   G17";" Abstract:  QuantNet 1 is an integrated web-based environment consisting of different types of statistics-related documents and program codes. Its goal is creating reproducibility and offering a platform for sharing validated knowledge native to the social web. To increase the information retrieval (IR) efficiency there is a need for incorporating semantic information. Three text mining models will be examined: vector space model (VSM), generalized VSM (GVSM) and latent semantic analysis (LSA). The LSA has been successfully used for IR purposes as a technique for capturing semantic relations between terms and inserting them into the similarity measure between documents. Our results show that different model configurations allow adapted similarity-based document clustering and knowledge discovery. In particular, different LSA configurations together with hierarchical clustering reveal good results under M3 evaluation. QuantNet and the corresponding Data-Driven Documents (D3) based visualization can be found and applied under http://quantlet.de. The driving technology behind it is Q3-D3-LSA, which is the combination of Â“GitHub API based QuantNet Mining infrastructure in RÂ”, LSA and D3 implementation.  "
"805";805;"2016-050";"Network Quantile Autoregression";"Xuening Zhu,  Weining Wang,  Hangsheng Wang and   Wolfgang Karl Härdle";"B1";"2016-11-23";"C12,   C22";" Abstract:  It is a challenging task to understand the complex dependency structures in an ultra-high dimensional network, especially when one concentrates on the tail dependency. To tackle this problem, we consider a network quantile autoregres- sion model (NQAR) to characterize the dynamic quantile behavior in a complex system. In particular, we relate responses to its connected nodes and node spe- cic characteristics in a quantile autoregression process. A minimum contrast estimation approach for the NQAR model is introduced, and the asymptotic properties are studied. Finally, we demonstrate the usage of our model by in- vestigating the nancial contagions in the Chinese stock market accounting for shared ownership of companies. "
"806";806;"2016-051";"Dynamic Topic Modelling for Cryptocurrency Community Forums";"Marco Linton,  Ernie Gin Swee Teo,  Elisabeth Bommes,  Cathy Yi-Hsuan Chen and   Wolfgang Karl Härdle";"B1";"2016-11-23";"C19,   G09,   G10";" Abstract:  Cryptocurrencies are more and more used in ocial cash  ows and exchange of goods. Bitcoin and the underlying blockchain technology have been looked at by big companies that are adopting and investing in this technology. The CRIX Index of cryptocurrencies hu.berlin/CRIX indicates a wider acceptance of cryptos. One reason for its prosperity certainly being a security aspect, since the underlying network of cryptos is decentralized. It is also unregulated and highly volatile, making the risk assessment at any given moment dicult. In message boards one nds a huge source of information in the form of unstructured text written by e.g. Bitcoin developers and investors. We collect from a popular crypto currency message board texts, user information and associated time stamps. We then provide an indicator for fraudulent schemes. This indicator is constructed using dynamic topic modelling, text mining and unsupervised machine learning. We study how opinions and the evolution of topics are connected with big events in the cryptocurrency universe. Furthermore, the predictive power of these techniques are investigated, comparing the results to known events in the cryptocurrency space. We also test hypothesis of self-fulling prophecies and herding behaviour using the results. "
"807";807;"2016-052";"Beta-boosted ensemble for big credit scoring data";"Maciej Zieba and   Wolfgang Karl Härdle";"B1";"2016-11-23";"C53";" Abstract:  In this work we present a novel ensemble model for a credit scoring problem. The main idea of the approach is to incorporate separate beta binomial distributions for each of the classes to generate balanced datasets that are further used to construct base learners that constitute the final ensemble model. The sampling procedure is performed on two separate ranking lists, each for one class, where the ranking is based on prepotency of observing positive class. Two strategies are considered: one assumes mining easy examples and the second one forces good classification of hard cases. The proposed solutions are tested on two big datasets on credit scoring. "
"808";808;"2016-053";"Central Bank Reputation, Cheap Talk and Transparency as Substitutes for Commitment: Experimental Evidence";"John Duffy and   Frank Heinemann";"C10";"2016-12-07";"C92,   D83,   E52,   E58";" Abstract:  We implement a repeated version of the Barro-Gordon monetary policy game in the laboratory and ask whether reputation serves as a substitute for commitment, enabling the central bank to achieve the efficient Ramsey equilibrium and avoid the inefficient, time-inconsistent one-shot Nash equilibrium. We find that reputation is a poor substitute for commitment. We then explore whether central bank cheap talk, policy transparency, both cheap talk and policy transparency or economic transparency yield improvements in the direction of the Ramsey equilibrium under the discretionary policy regime. Our findings suggest that these mechanisms have only small or transitory effects on welfare. Surprisingly, the real effects of supply shocks are better mitigated by a commitment regime than by any discretionary policy. Thus, we find that there is no trade-off between flexibility and credibility. "
"809";809;"2016-054";"Labor Market Frictions and Monetary Policy Design";"Anna Almosova";"C7";"2016-12-12";"E52,   E24,   C11";" Abstract:  This paper estimates a New Keynesian DSGE model with search frictions and monetary rules augmented with dierent labor market indicators. In accordance with a theoretical literature I nd that a central bank reacts to a labor market tightness, employment or unemployment. Posterior odds tests speak in favor of models with augmented Taylor rules versus a model with a model with a standard rule. The augmented rules were also shown to be more ecient in terms of welfare.    "
